{'dataset': 'human_activity', 'algorithm': 'ptdqn', 'num_clf': 50, 'num_training': 400000, 'learning_rate': 0.1, 'discount_factor': 1.0, 'epsilon': 0.1, 'random_state': 2256, 'portion': 0.5, 'sequential': False}
(10299, 562)
reading data takes 1.571 sec
number of labels: 6

Running iteration 1 of 10 fold...
[23, 0, 24, 3, 34, 20, 8, 43, 39, 41, 7]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.965480, 0.965710, 0.965638, 0.965828

    accuracy, precision, recall, f_score
max3: 0.963315, 0.963460, 0.962724, 0.963098

    accuracy, precision, recall, f_score
max1: 0.975728, 0.975769, 0.975077, 0.975177


min loss: 0.012, episode: 248000
max accu: 0.976, episode: 380000

36.17 classifiers used
    accuracy, precision, recall, f_score
mv: 0.986408, 0.986419, 0.985873, 0.985876
wv: 0.986408, 0.986423, 0.985950, 0.985880
fs: 0.962136, 0.962191, 0.962039, 0.962020
rl: 0.975728, 0.975769, 0.975077, 0.975177

0.8647292340884575
0.85, 0.85, 0.84, 0.86, 0.88, 0.88, 0.87, 0.85, 0.87, 0.87, 0.85, 0.83, 0.82, 0.85, 0.87, 0.88, 0.85, 0.87, 0.86, 0.88, 0.86, 0.84, 0.87, 0.89, 0.88, 0.86, 0.86, 0.86, 0.87, 0.85, 0.84, 0.86, 0.87, 0.85, 0.88, 0.84, 0.88, 0.83, 0.83, 0.87, 0.86, 0.86, 0.85, 0.87, 0.87, 0.85, 0.87, 0.86, 0.85, 0.88

0.8633059991368148
0.85, 0.85, 0.84, 0.86, 0.88, 0.87, 0.87, 0.84, 0.87, 0.87, 0.86, 0.83, 0.82, 0.85, 0.87, 0.88, 0.86, 0.87, 0.87, 0.88, 0.85, 0.83, 0.87, 0.89, 0.87, 0.86, 0.85, 0.85, 0.86, 0.84, 0.84, 0.85, 0.85, 0.85, 0.88, 0.83, 0.88, 0.82, 0.83, 0.88, 0.87, 0.86, 0.85, 0.87, 0.87, 0.85, 0.87, 0.86, 0.84, 0.87

0.8703300970873786
0.87, 0.86, 0.86, 0.85, 0.90, 0.87, 0.87, 0.85, 0.89, 0.88, 0.86, 0.85, 0.83, 0.86, 0.89, 0.88, 0.86, 0.89, 0.88, 0.88, 0.83, 0.84, 0.86, 0.89, 0.88, 0.89, 0.85, 0.86, 0.88, 0.84, 0.85, 0.85, 0.89, 0.84, 0.87, 0.86, 0.89, 0.84, 0.84, 0.88, 0.89, 0.86, 0.86, 0.85, 0.87, 0.85, 0.89, 0.87, 0.83, 0.88

0.9001553398058251
0.9, 0.89, 0.91, 0.88, 0.89, 0.90, 0.88, 0.88, 0.89, 0.89, 0.91, 0.91, 0.90, 0.90, 0.89, 0.90, 0.90, 0.88, 0.91, 0.89, 0.88, 0.89, 0.90, 0.91, 0.90, 0.91, 0.87, 0.89, 0.88, 0.92, 0.92, 0.90, 0.89, 0.90, 0.90, 0.87, 0.91, 0.90, 0.91, 0.90, 0.90, 0.88, 0.87, 0.90, 0.90, 0.92, 0.90, 0.89, 0.86, 0.88


Running iteration 2 of 10 fold...
[23, 0, 43, 29, 13, 11, 15, 2, 1, 30, 18, 8, 14, 45, 3, 24, 16, 26, 34, 27]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.968069, 0.968385, 0.968044, 0.968119

    accuracy, precision, recall, f_score
max3: 0.967199, 0.967768, 0.966295, 0.966612

    accuracy, precision, recall, f_score
max1: 0.970874, 0.971538, 0.970084, 0.970642


min loss: 0.007, episode: 277000
max accu: 0.971, episode: 380000

49.71 classifiers used
    accuracy, precision, recall, f_score
mv: 0.978641, 0.978907, 0.977359, 0.978240
wv: 0.979612, 0.979830, 0.978269, 0.979122
fs: 0.965049, 0.965938, 0.963641, 0.964659
rl: 0.970874, 0.971538, 0.970084, 0.970642

0.8699288025889967
0.86, 0.86, 0.86, 0.87, 0.86, 0.86, 0.84, 0.86, 0.84, 0.86, 0.88, 0.86, 0.86, 0.86, 0.87, 0.87, 0.86, 0.84, 0.85, 0.86, 0.88, 0.85, 0.86, 0.89, 0.87, 0.88, 0.89, 0.85, 0.89, 0.88, 0.86, 0.87, 0.87, 0.87, 0.84, 0.89, 0.85, 0.88, 0.86, 0.89, 0.88, 0.88, 0.86, 0.86, 0.88, 0.84, 0.86, 0.84, 0.87, 0.88

0.8688131204143289
0.87, 0.86, 0.86, 0.88, 0.84, 0.85, 0.84, 0.86, 0.84, 0.86, 0.89, 0.86, 0.85, 0.86, 0.87, 0.87, 0.86, 0.84, 0.84, 0.87, 0.88, 0.85, 0.86, 0.89, 0.88, 0.88, 0.89, 0.84, 0.89, 0.89, 0.86, 0.86, 0.87, 0.86, 0.84, 0.89, 0.86, 0.87, 0.86, 0.89, 0.88, 0.88, 0.85, 0.85, 0.88, 0.85, 0.86, 0.84, 0.86, 0.87

0.873747572815534
0.87, 0.86, 0.86, 0.89, 0.86, 0.86, 0.85, 0.87, 0.86, 0.88, 0.89, 0.86, 0.85, 0.87, 0.87, 0.86, 0.86, 0.86, 0.85, 0.86, 0.88, 0.86, 0.85, 0.88, 0.87, 0.88, 0.89, 0.88, 0.90, 0.90, 0.84, 0.87, 0.88, 0.87, 0.85, 0.89, 0.86, 0.87, 0.85, 0.88, 0.89, 0.88, 0.86, 0.86, 0.88, 0.84, 0.87, 0.84, 0.85, 0.89

0.8988543689320391
0.88, 0.90, 0.90, 0.90, 0.91, 0.91, 0.87, 0.91, 0.88, 0.91, 0.90, 0.90, 0.90, 0.87, 0.86, 0.90, 0.90, 0.88, 0.89, 0.90, 0.86, 0.88, 0.92, 0.89, 0.88, 0.91, 0.90, 0.88, 0.87, 0.90, 0.89, 0.91, 0.87, 0.92, 0.91, 0.91, 0.91, 0.88, 0.92, 0.88, 0.88, 0.90, 0.90, 0.88, 0.90, 0.89, 0.89, 0.90, 0.91, 0.90


Running iteration 3 of 10 fold...
[20, 0, 32, 15, 17, 5, 22, 31, 26, 42, 37, 8, 25, 13, 23]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.968069, 0.968447, 0.967669, 0.967785

    accuracy, precision, recall, f_score
max3: 0.968494, 0.969057, 0.967259, 0.967307

    accuracy, precision, recall, f_score
max1: 0.972816, 0.972885, 0.972871, 0.973018


min loss: 0.008, episode: 264000
max accu: 0.973, episode: 400000

45.29 classifiers used
    accuracy, precision, recall, f_score
mv: 0.980583, 0.980870, 0.981056, 0.980777
wv: 0.980583, 0.980870, 0.981056, 0.980777
fs: 0.969903, 0.969925, 0.969370, 0.969683
rl: 0.972816, 0.972885, 0.972871, 0.973018

0.8685868392664509
0.87, 0.85, 0.87, 0.86, 0.88, 0.88, 0.88, 0.86, 0.87, 0.85, 0.85, 0.88, 0.88, 0.85, 0.86, 0.88, 0.86, 0.88, 0.86, 0.86, 0.89, 0.84, 0.87, 0.85, 0.85, 0.86, 0.86, 0.87, 0.85, 0.84, 0.88, 0.88, 0.89, 0.85, 0.84, 0.85, 0.87, 0.88, 0.87, 0.88, 0.84, 0.86, 0.86, 0.87, 0.87, 0.86, 0.83, 0.89, 0.86, 0.87

0.8667069486404835
0.86, 0.86, 0.85, 0.86, 0.87, 0.88, 0.87, 0.86, 0.87, 0.84, 0.85, 0.87, 0.88, 0.86, 0.86, 0.88, 0.86, 0.88, 0.86, 0.85, 0.89, 0.84, 0.87, 0.85, 0.85, 0.87, 0.87, 0.87, 0.85, 0.83, 0.87, 0.88, 0.89, 0.85, 0.84, 0.85, 0.87, 0.87, 0.86, 0.88, 0.84, 0.85, 0.86, 0.87, 0.86, 0.85, 0.83, 0.89, 0.86, 0.86

0.8655339805825244
0.87, 0.86, 0.86, 0.86, 0.87, 0.86, 0.88, 0.84, 0.86, 0.84, 0.85, 0.87, 0.88, 0.86, 0.86, 0.87, 0.87, 0.88, 0.87, 0.86, 0.87, 0.85, 0.87, 0.84, 0.83, 0.84, 0.87, 0.88, 0.85, 0.84, 0.88, 0.86, 0.89, 0.84, 0.84, 0.85, 0.88, 0.88, 0.86, 0.89, 0.84, 0.86, 0.85, 0.87, 0.86, 0.83, 0.83, 0.87, 0.87, 0.87

0.8942330097087379
0.89, 0.91, 0.91, 0.90, 0.88, 0.87, 0.90, 0.88, 0.87, 0.88, 0.88, 0.90, 0.88, 0.86, 0.89, 0.89, 0.89, 0.90, 0.90, 0.89, 0.88, 0.9, 0.88, 0.89, 0.90, 0.90, 0.89, 0.86, 0.90, 0.88, 0.90, 0.88, 0.89, 0.88, 0.88, 0.90, 0.89, 0.88, 0.9, 0.90, 0.89, 0.90, 0.90, 0.88, 0.88, 0.91, 0.90, 0.89, 0.87, 0.91


Running iteration 4 of 10 fold...
[7, 0, 46, 38, 18, 29, 36, 8, 1, 21, 23, 17, 4, 9, 5, 33, 39, 11]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.970011, 0.970301, 0.969461, 0.969676

    accuracy, precision, recall, f_score
max3: 0.968062, 0.968652, 0.966666, 0.966843

    accuracy, precision, recall, f_score
max1: 0.968932, 0.969220, 0.969090, 0.968783


min loss: 0.005, episode: 309000
max accu: 0.969, episode: 270000

45.12 classifiers used
    accuracy, precision, recall, f_score
mv: 0.973786, 0.973905, 0.974203, 0.974253
wv: 0.973786, 0.973905, 0.974203, 0.974253
fs: 0.969903, 0.969976, 0.970054, 0.969910
rl: 0.968932, 0.969220, 0.969090, 0.968783

0.8708694714131607
0.87, 0.88, 0.87, 0.87, 0.88, 0.89, 0.86, 0.89, 0.85, 0.86, 0.85, 0.86, 0.85, 0.86, 0.84, 0.88, 0.84, 0.85, 0.89, 0.85, 0.87, 0.86, 0.86, 0.89, 0.85, 0.86, 0.86, 0.88, 0.86, 0.87, 0.87, 0.86, 0.86, 0.86, 0.88, 0.86, 0.88, 0.86, 0.88, 0.86, 0.88, 0.87, 0.87, 0.86, 0.86, 0.86, 0.88, 0.88, 0.84, 0.85

0.8681053085886923
0.87, 0.87, 0.87, 0.88, 0.88, 0.88, 0.86, 0.89, 0.84, 0.86, 0.85, 0.87, 0.85, 0.86, 0.84, 0.87, 0.84, 0.85, 0.89, 0.84, 0.87, 0.86, 0.85, 0.88, 0.84, 0.86, 0.85, 0.87, 0.85, 0.87, 0.86, 0.86, 0.86, 0.86, 0.87, 0.85, 0.87, 0.85, 0.89, 0.87, 0.87, 0.87, 0.88, 0.87, 0.86, 0.86, 0.88, 0.89, 0.84, 0.85

0.8723106796116504
0.86, 0.89, 0.88, 0.88, 0.88, 0.89, 0.85, 0.88, 0.85, 0.85, 0.86, 0.86, 0.86, 0.86, 0.85, 0.88, 0.84, 0.85, 0.88, 0.85, 0.86, 0.87, 0.86, 0.89, 0.87, 0.87, 0.87, 0.88, 0.87, 0.86, 0.88, 0.86, 0.88, 0.85, 0.88, 0.85, 0.87, 0.86, 0.90, 0.87, 0.89, 0.88, 0.89, 0.86, 0.87, 0.85, 0.88, 0.88, 0.83, 0.85

0.8948932038834951
0.90, 0.89, 0.89, 0.90, 0.87, 0.90, 0.90, 0.89, 0.89, 0.86, 0.89, 0.86, 0.90, 0.90, 0.90, 0.89, 0.89, 0.88, 0.91, 0.89, 0.91, 0.87, 0.90, 0.89, 0.88, 0.89, 0.87, 0.90, 0.91, 0.89, 0.88, 0.89, 0.90, 0.88, 0.88, 0.91, 0.90, 0.86, 0.89, 0.88, 0.89, 0.88, 0.87, 0.89, 0.90, 0.90, 0.87, 0.87, 0.90, 0.91

    accuracy, precision, recall, f_score
mv: 0.979854, 0.980025, 0.979623, 0.979787
wv: 0.980097, 0.980257, 0.979870, 0.980008
fs: 0.966748, 0.967007, 0.966276, 0.966568
rl: 0.972087, 0.972353, 0.971780, 0.971905

fs avg size: 16.00000, rl avg size: 44.07403
full test avg accu: 0.89703, test avg accu: 0.87048

training takes 25988.215 sec
