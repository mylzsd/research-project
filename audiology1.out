{'dataset': 'audiology', 'algorithm': 'ptdqn', 'num_clf': 50, 'num_training': 400000, 'learning_rate': 0.1, 'discount_factor': 1.0, 'epsilon': 0.1, 'random_state': 4427, 'portion': 0.5, 'sequential': False}
(226, 95)
reading data takes 6.354 sec
number of labels: 24

Running iteration 1 of 10 fold...
[14, 0, 24, 3, 45, 12, 7, 40, 20, 13]
using cpu
using cpu
    accuracy, precision, recall, f_score
mdl1: 0.782609, 0.718841, 0.166667, 0.153704

    accuracy, precision, recall, f_score
max3: 0.647059, 0.573628, 0.227778, 0.187876

    accuracy, precision, recall, f_score
max1: 0.782609, 0.718841, 0.166667, 0.153704


min loss: 0.004, episode: 344000
max accu: 0.647, episode: 370000

48.22 classifiers used
    accuracy, precision, recall, f_score
mv: 0.826087, 0.760870, 0.244048, 0.231926
wv: 0.826087, 0.760870, 0.244048, 0.231926
fs: 0.782609, 0.760870, 0.197917, 0.190260
rl: 0.782609, 0.718841, 0.166667, 0.153704

0.4754455445544554
0.32, 0.46, 0.33, 0.46, 0.41, 0.47, 0.44, 0.55, 0.51, 0.40, 0.54, 0.55, 0.40, 0.47, 0.64, 0.40, 0.49, 0.43, 0.54, 0.45, 0.46, 0.55, 0.43, 0.42, 0.56, 0.41, 0.53, 0.44, 0.49, 0.37, 0.56, 0.48, 0.48, 0.38, 0.59, 0.54, 0.52, 0.47, 0.49, 0.46, 0.52, 0.51, 0.48, 0.41, 0.39, 0.45, 0.49, 0.42, 0.41, 0.54

0.47411764705882353
0.35, 0.49, 0.35, 0.50, 0.45, 0.47, 0.45, 0.58, 0.54, 0.45, 0.54, 0.50, 0.43, 0.50, 0.62, 0.37, 0.45, 0.43, 0.52, 0.39, 0.41, 0.56, 0.45, 0.43, 0.54, 0.39, 0.50, 0.45, 0.43, 0.39, 0.56, 0.54, 0.43, 0.41, 0.60, 0.58, 0.47, 0.47, 0.49, 0.47, 0.52, 0.47, 0.49, 0.45, 0.41, 0.47, 0.45, 0.37, 0.39, 0.54

0.5582608695652174
0.34, 0.52, 0.43, 0.52, 0.39, 0.60, 0.43, 0.73, 0.69, 0.60, 0.73, 0.65, 0.56, 0.60, 0.73, 0.47, 0.60, 0.56, 0.47, 0.43, 0.56, 0.73, 0.69, 0.43, 0.56, 0.34, 0.34, 0.56, 0.69, 0.56, 0.56, 0.60, 0.60, 0.34, 0.78, 0.78, 0.47, 0.56, 0.52, 0.47, 0.65, 0.56, 0.56, 0.43, 0.60, 0.56, 0.60, 0.43, 0.39, 0.65

0.62
0.47, 0.65, 0.56, 0.60, 0.69, 0.60, 0.56, 0.69, 0.69, 0.56, 0.69, 0.73, 0.73, 0.86, 0.60, 0.60, 0.52, 0.52, 0.65, 0.73, 0.65, 0.65, 0.65, 0.56, 0.60, 0.73, 0.52, 0.60, 0.65, 0.47, 0.78, 0.78, 0.56, 0.60, 0.56, 0.52, 0.69, 0.47, 0.65, 0.56, 0.56, 0.65, 0.65, 0.43, 0.56, 0.65, 0.43, 0.65, 0.60, 0.60


Running iteration 2 of 10 fold...
[31, 0, 14, 23, 27, 3, 5, 36, 6, 43, 2, 10, 7, 21, 9, 8, 28, 18]
using cpu
using cpu
    accuracy, precision, recall, f_score
mdl1: 0.565217, 0.563043, 0.214583, 0.181680

    accuracy, precision, recall, f_score
max3: 0.725490, 0.686601, 0.268763, 0.245868

    accuracy, precision, recall, f_score
max1: 0.565217, 0.564182, 0.212500, 0.178346


min loss: 0.005, episode: 321000
max accu: 0.725, episode: 190000

1.87 classifiers used
    accuracy, precision, recall, f_score
mv: 0.652174, 0.623188, 0.233333, 0.204861
wv: 0.652174, 0.623188, 0.233333, 0.204861
fs: 0.652174, 0.631988, 0.233333, 0.203220
rl: 0.565217, 0.564182, 0.212500, 0.178346

0.4952475247524752
0.43, 0.55, 0.58, 0.47, 0.51, 0.56, 0.44, 0.45, 0.43, 0.58, 0.52, 0.45, 0.45, 0.56, 0.57, 0.48, 0.51, 0.45, 0.50, 0.54, 0.58, 0.41, 0.36, 0.52, 0.31, 0.46, 0.51, 0.55, 0.53, 0.51, 0.32, 0.59, 0.41, 0.48, 0.54, 0.55, 0.49, 0.56, 0.50, 0.46, 0.40, 0.43, 0.55, 0.57, 0.40, 0.45, 0.51, 0.47, 0.57, 0.49

0.49490196078431375
0.39, 0.60, 0.56, 0.47, 0.52, 0.52, 0.47, 0.41, 0.49, 0.60, 0.54, 0.41, 0.45, 0.50, 0.58, 0.47, 0.52, 0.47, 0.56, 0.50, 0.56, 0.39, 0.37, 0.50, 0.35, 0.49, 0.50, 0.56, 0.45, 0.47, 0.31, 0.58, 0.49, 0.50, 0.54, 0.50, 0.47, 0.58, 0.52, 0.39, 0.43, 0.45, 0.54, 0.58, 0.41, 0.45, 0.52, 0.47, 0.60, 0.49

0.44608695652173913
0.30, 0.47, 0.56, 0.39, 0.47, 0.39, 0.34, 0.39, 0.34, 0.47, 0.52, 0.43, 0.39, 0.39, 0.60, 0.56, 0.34, 0.47, 0.52, 0.43, 0.56, 0.34, 0.26, 0.34, 0.30, 0.39, 0.47, 0.69, 0.47, 0.34, 0.39, 0.56, 0.39, 0.47, 0.47, 0.47, 0.56, 0.47, 0.60, 0.47, 0.34, 0.30, 0.52, 0.47, 0.34, 0.34, 0.60, 0.39, 0.39, 0.56

0.5295652173913044
0.43, 0.43, 0.52, 0.43, 0.43, 0.60, 0.47, 0.34, 0.30, 0.56, 0.52, 0.65, 0.56, 0.43, 0.60, 0.56, 0.56, 0.60, 0.52, 0.65, 0.34, 0.52, 0.65, 0.30, 0.47, 0.43, 0.65, 0.39, 0.69, 0.69, 0.43, 0.52, 0.52, 0.52, 0.65, 0.78, 0.52, 0.65, 0.43, 0.47, 0.47, 0.52, 0.60, 0.65, 0.47, 0.39, 0.47, 0.56, 0.69, 0.65


Running iteration 3 of 10 fold...
[24, 0, 14, 5, 23, 19, 8, 4, 22, 41, 1, 33, 18]
using cpu
using cpu
    accuracy, precision, recall, f_score
mdl1: 0.652174, 0.685300, 0.166667, 0.163141

    accuracy, precision, recall, f_score
max3: 0.725490, 0.745472, 0.305308, 0.254604

    accuracy, precision, recall, f_score
max1: 0.608696, 0.617754, 0.145833, 0.136813


min loss: 0.009, episode: 253000
max accu: 0.725, episode: 360000

11.09 classifiers used
    accuracy, precision, recall, f_score
mv: 0.826087, 0.847826, 0.263889, 0.264881
wv: 0.826087, 0.847826, 0.263889, 0.264881
fs: 0.652174, 0.660455, 0.159722, 0.154274
rl: 0.608696, 0.617754, 0.145833, 0.136813

0.497029702970297
0.48, 0.38, 0.51, 0.54, 0.57, 0.47, 0.48, 0.52, 0.58, 0.39, 0.54, 0.42, 0.40, 0.48, 0.54, 0.37, 0.49, 0.42, 0.59, 0.51, 0.47, 0.45, 0.41, 0.62, 0.65, 0.52, 0.58, 0.48, 0.48, 0.50, 0.60, 0.44, 0.47, 0.56, 0.33, 0.49, 0.36, 0.36, 0.63, 0.47, 0.44, 0.64, 0.52, 0.52, 0.43, 0.56, 0.49, 0.46, 0.51, 0.48

0.5258823529411765
0.50, 0.41, 0.52, 0.58, 0.62, 0.45, 0.54, 0.54, 0.66, 0.43, 0.62, 0.41, 0.45, 0.52, 0.58, 0.43, 0.56, 0.49, 0.60, 0.52, 0.50, 0.49, 0.39, 0.62, 0.66, 0.52, 0.60, 0.54, 0.50, 0.50, 0.66, 0.47, 0.52, 0.54, 0.37, 0.56, 0.35, 0.39, 0.62, 0.47, 0.49, 0.64, 0.52, 0.50, 0.52, 0.58, 0.47, 0.52, 0.54, 0.50

0.4773913043478261
0.43, 0.47, 0.60, 0.43, 0.60, 0.43, 0.43, 0.47, 0.47, 0.43, 0.34, 0.47, 0.43, 0.52, 0.60, 0.39, 0.30, 0.52, 0.56, 0.47, 0.39, 0.39, 0.34, 0.60, 0.65, 0.43, 0.65, 0.43, 0.43, 0.47, 0.52, 0.39, 0.65, 0.60, 0.26, 0.39, 0.30, 0.21, 0.69, 0.52, 0.52, 0.52, 0.65, 0.47, 0.52, 0.56, 0.47, 0.39, 0.43, 0.43

0.6095652173913044
0.78, 0.56, 0.52, 0.34, 0.82, 0.52, 0.60, 0.52, 0.52, 0.78, 0.56, 0.69, 0.69, 0.78, 0.65, 0.39, 0.52, 0.52, 0.65, 0.52, 0.73, 0.65, 0.56, 0.56, 0.86, 0.52, 0.73, 0.60, 0.47, 0.47, 0.69, 0.60, 0.52, 0.65, 0.52, 0.65, 0.73, 0.69, 0.47, 0.56, 0.56, 0.65, 0.43, 0.52, 0.52, 0.47, 0.73, 0.69, 0.73, 0.78


Running iteration 4 of 10 fold...
[15, 0, 2, 28, 40, 14, 25]
using cpu
using cpu
    accuracy, precision, recall, f_score
mdl1: 0.739130, 0.708696, 0.222222, 0.193633

    accuracy, precision, recall, f_score
max3: 0.705882, 0.741993, 0.172454, 0.178943

    accuracy, precision, recall, f_score
max1: 0.565217, 0.681159, 0.159722, 0.150000


min loss: 0.006, episode: 283000
max accu: 0.706, episode: 230000

6.96 classifiers used
    accuracy, precision, recall, f_score
mv: 0.782609, 0.768116, 0.243056, 0.229545
wv: 0.782609, 0.768116, 0.243056, 0.229545
fs: 0.826087, 0.746377, 0.250000, 0.219444
rl: 0.565217, 0.681159, 0.159722, 0.150000

0.5477227722772277
0.44, 0.52, 0.54, 0.62, 0.64, 0.61, 0.64, 0.50, 0.46, 0.49, 0.48, 0.49, 0.62, 0.57, 0.62, 0.70, 0.53, 0.51, 0.46, 0.37, 0.48, 0.56, 0.52, 0.56, 0.60, 0.51, 0.48, 0.60, 0.65, 0.39, 0.58, 0.58, 0.56, 0.54, 0.68, 0.56, 0.50, 0.57, 0.47, 0.42, 0.69, 0.57, 0.52, 0.59, 0.64, 0.55, 0.45, 0.45, 0.47, 0.61

0.5572549019607844
0.39, 0.47, 0.58, 0.66, 0.70, 0.58, 0.68, 0.45, 0.45, 0.49, 0.50, 0.50, 0.62, 0.62, 0.64, 0.70, 0.50, 0.52, 0.50, 0.39, 0.45, 0.56, 0.54, 0.54, 0.56, 0.41, 0.49, 0.58, 0.66, 0.43, 0.62, 0.60, 0.58, 0.52, 0.68, 0.62, 0.45, 0.66, 0.45, 0.45, 0.72, 0.60, 0.56, 0.62, 0.66, 0.52, 0.47, 0.47, 0.52, 0.64

0.5478260869565218
0.34, 0.56, 0.56, 0.69, 0.52, 0.56, 0.73, 0.52, 0.43, 0.65, 0.60, 0.47, 0.60, 0.43, 0.56, 0.60, 0.60, 0.56, 0.52, 0.26, 0.56, 0.60, 0.69, 0.47, 0.60, 0.60, 0.43, 0.65, 0.69, 0.43, 0.69, 0.43, 0.43, 0.52, 0.69, 0.56, 0.52, 0.52, 0.52, 0.43, 0.60, 0.34, 0.56, 0.60, 0.73, 0.56, 0.47, 0.52, 0.39, 0.56

0.5991304347826086
0.73, 0.65, 0.65, 0.52, 0.56, 0.60, 0.60, 0.60, 0.52, 0.65, 0.60, 0.60, 0.60, 0.69, 0.60, 0.65, 0.65, 0.65, 0.56, 0.69, 0.65, 0.82, 0.56, 0.52, 0.56, 0.60, 0.52, 0.52, 0.47, 0.43, 0.60, 0.78, 0.73, 0.43, 0.56, 0.69, 0.52, 0.65, 0.52, 0.65, 0.65, 0.56, 0.60, 0.43, 0.47, 0.47, 0.65, 0.60, 0.60, 0.52


Running iteration 5 of 10 fold...
[5, 0, 4, 2, 16, 13, 17, 1, 7, 44, 39]
using cpu
using cpu
    accuracy, precision, recall, f_score
mdl1: 0.782609, 0.695652, 0.200000, 0.191799

    accuracy, precision, recall, f_score
max3: 0.803922, 0.811934, 0.308780, 0.290543

    accuracy, precision, recall, f_score
max1: 0.695652, 0.610672, 0.181481, 0.163889


min loss: 0.005, episode: 285000
max accu: 0.804, episode: 240000

17.57 classifiers used
    accuracy, precision, recall, f_score
mv: 0.739130, 0.721014, 0.232407, 0.209325
wv: 0.782609, 0.729710, 0.237037, 0.212006
fs: 0.695652, 0.690580, 0.224074, 0.201833
rl: 0.695652, 0.610672, 0.181481, 0.163889

0.544950495049505
0.50, 0.57, 0.55, 0.51, 0.61, 0.74, 0.54, 0.46, 0.47, 0.68, 0.54, 0.47, 0.68, 0.56, 0.59, 0.68, 0.70, 0.45, 0.51, 0.60, 0.43, 0.50, 0.57, 0.43, 0.45, 0.52, 0.54, 0.62, 0.47, 0.38, 0.49, 0.54, 0.53, 0.45, 0.38, 0.58, 0.60, 0.52, 0.57, 0.54, 0.63, 0.55, 0.57, 0.49, 0.55, 0.49, 0.42, 0.56, 0.65, 0.59

0.5588235294117648
0.52, 0.58, 0.56, 0.54, 0.60, 0.74, 0.52, 0.50, 0.39, 0.64, 0.56, 0.54, 0.72, 0.60, 0.58, 0.76, 0.72, 0.37, 0.50, 0.66, 0.49, 0.49, 0.58, 0.41, 0.47, 0.54, 0.56, 0.54, 0.54, 0.41, 0.50, 0.54, 0.62, 0.45, 0.45, 0.58, 0.58, 0.56, 0.54, 0.58, 0.62, 0.58, 0.60, 0.47, 0.54, 0.52, 0.45, 0.60, 0.58, 0.62

0.6060869565217392
0.56, 0.39, 0.65, 0.60, 0.60, 0.69, 0.56, 0.60, 0.43, 0.73, 0.60, 0.65, 0.78, 0.56, 0.69, 0.52, 0.69, 0.47, 0.65, 0.60, 0.34, 0.47, 0.86, 0.56, 0.47, 0.47, 0.73, 0.47, 0.78, 0.56, 0.65, 0.73, 0.60, 0.60, 0.43, 0.73, 0.60, 0.60, 0.78, 0.65, 0.47, 0.56, 0.69, 0.56, 0.56, 0.52, 0.60, 0.65, 0.56, 0.73

0.6434782608695652
0.56, 0.65, 0.65, 0.47, 0.69, 0.73, 0.60, 0.69, 0.69, 0.73, 0.60, 0.56, 0.69, 0.60, 0.56, 0.60, 0.65, 0.69, 0.65, 0.60, 0.56, 0.69, 0.69, 0.69, 0.56, 0.65, 0.60, 0.82, 0.56, 0.56, 0.65, 0.65, 0.56, 0.60, 0.73, 0.69, 0.60, 0.73, 0.69, 0.65, 0.56, 0.60, 0.65, 0.69, 0.69, 0.69, 0.52, 0.65, 0.60, 0.65


Running iteration 6 of 10 fold...
[37, 0, 11, 3, 4, 7, 13, 20, 9, 14, 17, 46, 48, 1, 29, 6, 38, 41, 8, 28, 23, 30]
using cpu
using cpu
    accuracy, precision, recall, f_score
mdl1: 0.826087, 0.773551, 0.239583, 0.232937

    accuracy, precision, recall, f_score
max3: 0.823529, 0.766151, 0.326389, 0.322932

    accuracy, precision, recall, f_score
max1: 0.782609, 0.690217, 0.229167, 0.206548


min loss: 0.004, episode: 293000
max accu: 0.824, episode: 370000

21.96 classifiers used
    accuracy, precision, recall, f_score
mv: 0.782609, 0.740942, 0.229167, 0.219048
wv: 0.782609, 0.740942, 0.229167, 0.219048
fs: 0.782609, 0.755435, 0.229167, 0.224901
rl: 0.782609, 0.690217, 0.229167, 0.206548

0.5578217821782178
0.58, 0.43, 0.54, 0.60, 0.62, 0.50, 0.45, 0.47, 0.75, 0.50, 0.52, 0.68, 0.74, 0.52, 0.56, 0.55, 0.46, 0.54, 0.54, 0.38, 0.63, 0.58, 0.55, 0.51, 0.67, 0.52, 0.61, 0.47, 0.54, 0.73, 0.49, 0.54, 0.62, 0.64, 0.55, 0.57, 0.60, 0.78, 0.50, 0.50, 0.49, 0.44, 0.51, 0.49, 0.58, 0.49, 0.57, 0.53, 0.49, 0.55

0.5807843137254902
0.66, 0.47, 0.58, 0.56, 0.62, 0.56, 0.35, 0.54, 0.78, 0.47, 0.60, 0.70, 0.76, 0.49, 0.58, 0.62, 0.52, 0.54, 0.54, 0.43, 0.64, 0.66, 0.60, 0.49, 0.68, 0.49, 0.64, 0.43, 0.54, 0.78, 0.54, 0.52, 0.72, 0.70, 0.60, 0.54, 0.62, 0.86, 0.47, 0.56, 0.50, 0.41, 0.52, 0.50, 0.62, 0.47, 0.64, 0.52, 0.54, 0.56

0.5704347826086957
0.73, 0.43, 0.52, 0.56, 0.56, 0.52, 0.30, 0.39, 0.73, 0.56, 0.39, 0.56, 0.69, 0.47, 0.56, 0.52, 0.69, 0.47, 0.60, 0.47, 0.56, 0.69, 0.52, 0.43, 0.65, 0.60, 0.69, 0.52, 0.60, 0.73, 0.73, 0.39, 0.65, 0.60, 0.52, 0.56, 0.65, 0.65, 0.69, 0.56, 0.73, 0.47, 0.43, 0.52, 0.65, 0.43, 0.52, 0.73, 0.47, 0.60

0.6573913043478262
0.65, 0.65, 0.69, 0.65, 0.69, 0.56, 0.69, 0.65, 0.65, 0.60, 0.60, 0.60, 0.73, 0.52, 0.65, 0.69, 0.69, 0.78, 0.78, 0.60, 0.69, 0.65, 0.60, 0.69, 0.69, 0.69, 0.73, 0.65, 0.73, 0.60, 0.65, 0.65, 0.73, 0.78, 0.60, 0.60, 0.60, 0.82, 0.73, 0.65, 0.69, 0.65, 0.60, 0.34, 0.60, 0.65, 0.65, 0.47, 0.69, 0.60


Running iteration 7 of 10 fold...
[41, 0, 32, 10, 35, 18, 42, 14, 1, 29, 17, 43, 5, 11, 2, 13, 16, 33]
using cpu
using cpu
    accuracy, precision, recall, f_score
mdl1: 0.409091, 0.378788, 0.126984, 0.106151

    accuracy, precision, recall, f_score
max3: 0.843137, 0.782797, 0.391204, 0.374272

    accuracy, precision, recall, f_score
max1: 0.500000, 0.461364, 0.153770, 0.138763


min loss: 0.005, episode: 242000
max accu: 0.843, episode: 340000

42.05 classifiers used
    accuracy, precision, recall, f_score
mv: 0.545455, 0.583333, 0.210317, 0.201496
wv: 0.545455, 0.583333, 0.210317, 0.201496
fs: 0.545455, 0.522727, 0.160714, 0.146825
rl: 0.500000, 0.461364, 0.153770, 0.138763

0.5413725490196079
0.45, 0.64, 0.41, 0.51, 0.51, 0.45, 0.54, 0.56, 0.49, 0.65, 0.53, 0.57, 0.56, 0.52, 0.62, 0.49, 0.57, 0.65, 0.62, 0.62, 0.62, 0.57, 0.52, 0.51, 0.44, 0.45, 0.55, 0.52, 0.34, 0.64, 0.5, 0.54, 0.57, 0.56, 0.52, 0.54, 0.51, 0.58, 0.44, 0.59, 0.53, 0.73, 0.49, 0.61, 0.38, 0.39, 0.45, 0.61, 0.54, 0.55

0.5658823529411765
0.43, 0.62, 0.45, 0.52, 0.54, 0.47, 0.60, 0.56, 0.50, 0.66, 0.52, 0.62, 0.54, 0.56, 0.72, 0.50, 0.58, 0.64, 0.72, 0.62, 0.66, 0.62, 0.56, 0.54, 0.37, 0.54, 0.64, 0.49, 0.39, 0.64, 0.52, 0.52, 0.60, 0.56, 0.54, 0.50, 0.56, 0.62, 0.43, 0.66, 0.60, 0.76, 0.45, 0.66, 0.41, 0.35, 0.50, 0.66, 0.64, 0.60

0.3527272727272727
0.27, 0.5, 0.18, 0.31, 0.45, 0.45, 0.31, 0.36, 0.31, 0.40, 0.45, 0.27, 0.31, 0.27, 0.36, 0.31, 0.36, 0.31, 0.36, 0.5, 0.36, 0.5, 0.36, 0.36, 0.31, 0.27, 0.40, 0.22, 0.09, 0.45, 0.45, 0.40, 0.45, 0.40, 0.5, 0.36, 0.31, 0.40, 0.22, 0.45, 0.27, 0.63, 0.13, 0.36, 0.18, 0.18, 0.22, 0.36, 0.27, 0.5

0.4545454545454546
0.5, 0.40, 0.36, 0.45, 0.54, 0.59, 0.5, 0.36, 0.36, 0.45, 0.54, 0.45, 0.40, 0.40, 0.5, 0.54, 0.27, 0.40, 0.45, 0.36, 0.54, 0.45, 0.45, 0.40, 0.5, 0.45, 0.68, 0.40, 0.36, 0.40, 0.5, 0.45, 0.27, 0.5, 0.45, 0.54, 0.36, 0.68, 0.59, 0.31, 0.40, 0.40, 0.5, 0.5, 0.36, 0.40, 0.5, 0.45, 0.54, 0.36


Running iteration 8 of 10 fold...
[9, 0, 47, 32, 23, 40, 6, 33, 14, 1, 19, 44, 7, 41, 2, 15, 36, 18, 4, 5, 31]
using cpu
using cpu
    accuracy, precision, recall, f_score
mdl1: 0.772727, 0.777489, 0.255556, 0.233796

    accuracy, precision, recall, f_score
max3: 0.843137, 0.816682, 0.405556, 0.394147

    accuracy, precision, recall, f_score
max1: 0.863636, 0.834848, 0.277778, 0.274916


min loss: 0.007, episode: 324000
max accu: 0.843, episode: 370000

24.73 classifiers used
    accuracy, precision, recall, f_score
mv: 0.818182, 0.765152, 0.236111, 0.229545
wv: 0.818182, 0.765152, 0.236111, 0.229545
fs: 0.772727, 0.765152, 0.227778, 0.224916
rl: 0.863636, 0.834848, 0.277778, 0.274916

0.5390196078431373
0.40, 0.66, 0.55, 0.46, 0.53, 0.49, 0.58, 0.57, 0.58, 0.67, 0.59, 0.58, 0.40, 0.57, 0.67, 0.56, 0.5, 0.44, 0.49, 0.55, 0.48, 0.56, 0.43, 0.54, 0.60, 0.54, 0.50, 0.43, 0.59, 0.61, 0.52, 0.58, 0.60, 0.56, 0.50, 0.60, 0.60, 0.40, 0.42, 0.47, 0.64, 0.57, 0.5, 0.41, 0.47, 0.55, 0.49, 0.62, 0.52, 0.52

0.563921568627451
0.45, 0.72, 0.54, 0.45, 0.47, 0.52, 0.62, 0.62, 0.64, 0.70, 0.60, 0.58, 0.41, 0.64, 0.72, 0.60, 0.56, 0.49, 0.47, 0.56, 0.50, 0.56, 0.43, 0.62, 0.66, 0.58, 0.54, 0.47, 0.64, 0.64, 0.52, 0.60, 0.60, 0.50, 0.50, 0.72, 0.70, 0.43, 0.41, 0.43, 0.70, 0.52, 0.47, 0.43, 0.54, 0.62, 0.49, 0.68, 0.50, 0.54

0.5354545454545455
0.36, 0.59, 0.5, 0.40, 0.54, 0.5, 0.36, 0.63, 0.5, 0.54, 0.68, 0.63, 0.54, 0.54, 0.68, 0.59, 0.40, 0.59, 0.40, 0.5, 0.36, 0.5, 0.45, 0.5, 0.68, 0.36, 0.54, 0.36, 0.63, 0.77, 0.36, 0.59, 0.68, 0.63, 0.5, 0.59, 0.5, 0.5, 0.63, 0.36, 0.72, 0.54, 0.45, 0.36, 0.63, 0.63, 0.59, 0.63, 0.40, 0.68

0.6372727272727272
0.54, 0.63, 0.72, 0.59, 0.5, 0.63, 0.72, 0.77, 0.5, 0.72, 0.72, 0.59, 0.5, 0.68, 0.59, 0.54, 0.63, 0.54, 0.68, 0.63, 0.63, 0.5, 0.72, 0.77, 0.77, 0.63, 0.68, 0.5, 0.59, 0.68, 0.63, 0.63, 0.72, 0.77, 0.5, 0.54, 0.72, 0.81, 0.59, 0.63, 0.68, 0.68, 0.68, 0.54, 0.63, 0.72, 0.59, 0.68, 0.5, 0.54


Running iteration 9 of 10 fold...
[8, 0, 4, 5, 41, 39]
using cpu
using cpu
    accuracy, precision, recall, f_score
mdl1: 0.727273, 0.628788, 0.241667, 0.214815

    accuracy, precision, recall, f_score
max3: 0.843137, 0.855627, 0.356827, 0.358338

    accuracy, precision, recall, f_score
max1: 0.727273, 0.628139, 0.241667, 0.221044


min loss: 0.005, episode: 298000
max accu: 0.843, episode: 380000

25.77 classifiers used
    accuracy, precision, recall, f_score
mv: 0.727273, 0.636364, 0.208333, 0.179167
wv: 0.727273, 0.656818, 0.208333, 0.178704
fs: 0.636364, 0.558442, 0.191667, 0.157386
rl: 0.727273, 0.628139, 0.241667, 0.221044

0.5566666666666666
0.47, 0.57, 0.53, 0.55, 0.62, 0.55, 0.63, 0.55, 0.66, 0.50, 0.64, 0.56, 0.57, 0.58, 0.56, 0.56, 0.56, 0.51, 0.59, 0.63, 0.47, 0.57, 0.48, 0.58, 0.49, 0.51, 0.62, 0.61, 0.47, 0.53, 0.44, 0.40, 0.46, 0.59, 0.55, 0.60, 0.57, 0.47, 0.48, 0.43, 0.62, 0.60, 0.61, 0.61, 0.53, 0.53, 0.50, 0.60, 0.56, 0.63

0.5933333333333334
0.58, 0.62, 0.54, 0.56, 0.60, 0.64, 0.64, 0.66, 0.68, 0.56, 0.68, 0.62, 0.64, 0.72, 0.56, 0.60, 0.58, 0.56, 0.58, 0.70, 0.52, 0.62, 0.45, 0.56, 0.54, 0.50, 0.64, 0.66, 0.52, 0.47, 0.49, 0.43, 0.49, 0.58, 0.62, 0.64, 0.60, 0.50, 0.54, 0.43, 0.74, 0.66, 0.60, 0.70, 0.52, 0.58, 0.56, 0.62, 0.66, 0.56

0.48454545454545456
0.40, 0.63, 0.45, 0.31, 0.5, 0.54, 0.27, 0.54, 0.5, 0.5, 0.5, 0.40, 0.59, 0.63, 0.31, 0.5, 0.59, 0.54, 0.72, 0.54, 0.40, 0.68, 0.45, 0.63, 0.5, 0.54, 0.5, 0.54, 0.36, 0.36, 0.40, 0.31, 0.31, 0.5, 0.36, 0.5, 0.36, 0.40, 0.36, 0.40, 0.5, 0.63, 0.68, 0.54, 0.40, 0.40, 0.63, 0.54, 0.31, 0.54

0.5963636363636363
0.59, 0.5, 0.63, 0.63, 0.5, 0.54, 0.5, 0.72, 0.5, 0.5, 0.5, 0.5, 0.68, 0.68, 0.36, 0.68, 0.59, 0.54, 0.59, 0.63, 0.68, 0.68, 0.77, 0.54, 0.68, 0.72, 0.59, 0.5, 0.45, 0.68, 0.54, 0.54, 0.72, 0.68, 0.68, 0.63, 0.63, 0.63, 0.72, 0.54, 0.45, 0.5, 0.40, 0.68, 0.40, 0.59, 0.59, 0.68, 0.72, 0.68


Running iteration 10 of 10 fold...
[33, 0, 29, 19, 2, 16, 4, 38, 23, 13, 39, 14, 20, 26, 5, 40, 15, 6, 1, 7, 22, 3, 36, 43, 30]
using cpu
using cpu
    accuracy, precision, recall, f_score
mdl1: 0.681818, 0.645455, 0.215278, 0.200595

    accuracy, precision, recall, f_score
max3: 0.901961, 0.882956, 0.427083, 0.420417

    accuracy, precision, recall, f_score
max1: 0.636364, 0.602273, 0.208333, 0.189649


min loss: 0.007, episode: 288000
max accu: 0.902, episode: 300000

43.36 classifiers used
    accuracy, precision, recall, f_score
mv: 0.772727, 0.761364, 0.256944, 0.244048
wv: 0.772727, 0.761364, 0.256944, 0.244048
fs: 0.636364, 0.602273, 0.208333, 0.189649
rl: 0.636364, 0.602273, 0.208333, 0.189649

0.5350980392156863
0.54, 0.44, 0.62, 0.52, 0.50, 0.49, 0.61, 0.52, 0.47, 0.66, 0.66, 0.43, 0.55, 0.5, 0.46, 0.44, 0.52, 0.63, 0.55, 0.52, 0.50, 0.56, 0.48, 0.51, 0.61, 0.47, 0.35, 0.48, 0.45, 0.63, 0.54, 0.51, 0.5, 0.67, 0.50, 0.56, 0.44, 0.51, 0.60, 0.63, 0.53, 0.57, 0.43, 0.54, 0.41, 0.65, 0.50, 0.63, 0.52, 0.54

0.5745098039215687
0.56, 0.47, 0.64, 0.58, 0.56, 0.52, 0.68, 0.60, 0.47, 0.66, 0.70, 0.49, 0.62, 0.50, 0.45, 0.39, 0.50, 0.74, 0.58, 0.62, 0.52, 0.60, 0.50, 0.58, 0.74, 0.41, 0.37, 0.50, 0.49, 0.64, 0.64, 0.50, 0.54, 0.72, 0.58, 0.62, 0.47, 0.58, 0.76, 0.60, 0.60, 0.56, 0.43, 0.58, 0.56, 0.68, 0.52, 0.76, 0.50, 0.52

0.439090909090909
0.59, 0.22, 0.5, 0.5, 0.31, 0.40, 0.5, 0.59, 0.59, 0.5, 0.45, 0.36, 0.45, 0.45, 0.36, 0.5, 0.27, 0.40, 0.31, 0.54, 0.27, 0.45, 0.36, 0.40, 0.31, 0.5, 0.40, 0.40, 0.45, 0.40, 0.36, 0.36, 0.36, 0.54, 0.40, 0.63, 0.54, 0.31, 0.59, 0.63, 0.27, 0.59, 0.40, 0.40, 0.31, 0.63, 0.40, 0.45, 0.31, 0.5

0.5790909090909091
0.63, 0.59, 0.54, 0.54, 0.54, 0.59, 0.31, 0.45, 0.59, 0.63, 0.54, 0.77, 0.54, 0.5, 0.54, 0.45, 0.54, 0.54, 0.54, 0.5, 0.68, 0.77, 0.59, 0.54, 0.54, 0.54, 0.63, 0.59, 0.54, 0.54, 0.5, 0.77, 0.54, 0.40, 0.45, 0.63, 0.5, 0.68, 0.63, 0.63, 0.77, 0.59, 0.5, 0.59, 0.63, 0.68, 0.68, 0.54, 0.59, 0.63


training takes 34436.418 sec
