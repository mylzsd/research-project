{'dataset': 'avila', 'algorithm': 'ptdqn', 'num_clf': 50, 'num_training': 450000, 'learning_rate': 0.1, 'discount_factor': 1.0, 'epsilon': 0.1, 'random_state': 382, 'portion': 0.5, 'sequential': False}
(20867, 11)
reading data takes 0.093 sec
number of labels: 12

Running iteration 1 of 10 fold...
[8, 0, 16, 28, 24, 40, 4, 36, 12, 20, 32, 47, 48]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.920980, 0.921743, 0.876342, 0.901267

    accuracy, precision, recall, f_score
max3: 0.921193, 0.921483, 0.871486, 0.896182

    accuracy, precision, recall, f_score
max1: 0.925731, 0.926910, 0.888841, 0.917151


min loss: 0.007, episode: 239000
max accu: 0.926, episode: 400000

4.22 classifiers used
    accuracy, precision, recall, f_score
mv: 0.859607, 0.861363, 0.813179, 0.845624
wv: 0.919502, 0.919943, 0.878833, 0.901952
fs: 0.969813, 0.970067, 0.963521, 0.972655
rl: 0.925731, 0.926910, 0.888841, 0.917151

0.6325537806176783
0.85, 0.64, 0.69, 0.33, 0.82, 0.62, 0.69, 0.31, 0.88, 0.65, 0.68, 0.29, 0.84, 0.65, 0.68, 0.29, 0.87, 0.67, 0.69, 0.32, 0.83, 0.67, 0.68, 0.32, 0.87, 0.66, 0.68, 0.31, 0.84, 0.63, 0.69, 0.33, 0.82, 0.66, 0.68, 0.33, 0.82, 0.63, 0.69, 0.30, 0.86, 0.66, 0.69, 0.34, 0.80, 0.66, 0.69, 0.30, 0.80, 0.65

0.6316592119275826
0.85, 0.64, 0.69, 0.32, 0.82, 0.63, 0.68, 0.30, 0.88, 0.66, 0.68, 0.28, 0.83, 0.64, 0.68, 0.29, 0.88, 0.69, 0.69, 0.32, 0.84, 0.68, 0.68, 0.31, 0.87, 0.66, 0.68, 0.30, 0.85, 0.63, 0.69, 0.32, 0.83, 0.67, 0.69, 0.32, 0.82, 0.63, 0.68, 0.29, 0.86, 0.66, 0.68, 0.33, 0.80, 0.67, 0.69, 0.29, 0.80, 0.66

0.6322855773838044
0.84, 0.64, 0.68, 0.33, 0.84, 0.63, 0.68, 0.32, 0.88, 0.65, 0.67, 0.29, 0.85, 0.64, 0.68, 0.29, 0.89, 0.66, 0.68, 0.31, 0.82, 0.66, 0.68, 0.32, 0.86, 0.66, 0.69, 0.33, 0.85, 0.63, 0.68, 0.32, 0.83, 0.66, 0.68, 0.33, 0.82, 0.62, 0.69, 0.31, 0.87, 0.66, 0.68, 0.35, 0.80, 0.66, 0.69, 0.31, 0.80, 0.64

0.6582367034020126
0.87, 0.64, 0.73, 0.31, 0.92, 0.66, 0.73, 0.30, 0.94, 0.66, 0.73, 0.32, 0.90, 0.65, 0.74, 0.30, 0.87, 0.67, 0.73, 0.30, 0.89, 0.67, 0.73, 0.29, 0.89, 0.68, 0.73, 0.29, 0.91, 0.66, 0.75, 0.33, 0.91, 0.66, 0.73, 0.30, 0.87, 0.66, 0.73, 0.30, 0.90, 0.65, 0.74, 0.28, 0.90, 0.65, 0.74, 0.32, 0.91, 0.64


Running iteration 2 of 10 fold...
[32, 0, 24, 20, 36, 12, 8, 40, 4, 28, 3]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.920447, 0.920945, 0.899110, 0.912537

    accuracy, precision, recall, f_score
max3: 0.922258, 0.922432, 0.896243, 0.911010

    accuracy, precision, recall, f_score
max1: 0.931960, 0.932455, 0.911023, 0.919163


min loss: 0.007, episode: 238000
max accu: 0.932, episode: 200000

4.66 classifiers used
    accuracy, precision, recall, f_score
mv: 0.859607, 0.863398, 0.785219, 0.823211
wv: 0.917585, 0.919001, 0.859559, 0.886671
fs: 0.968855, 0.968947, 0.959132, 0.968641
rl: 0.931960, 0.932455, 0.911023, 0.919163

0.6264558040468584
0.81, 0.68, 0.68, 0.33, 0.82, 0.66, 0.68, 0.29, 0.86, 0.63, 0.68, 0.27, 0.84, 0.65, 0.68, 0.30, 0.82, 0.67, 0.68, 0.31, 0.86, 0.62, 0.69, 0.30, 0.87, 0.66, 0.68, 0.30, 0.83, 0.66, 0.69, 0.29, 0.88, 0.64, 0.69, 0.33, 0.85, 0.66, 0.69, 0.3, 0.79, 0.64, 0.69, 0.31, 0.82, 0.64, 0.69, 0.28, 0.75, 0.64

0.6273865814696484
0.81, 0.69, 0.69, 0.33, 0.83, 0.67, 0.68, 0.29, 0.86, 0.64, 0.68, 0.26, 0.84, 0.65, 0.69, 0.30, 0.82, 0.67, 0.68, 0.30, 0.87, 0.63, 0.69, 0.29, 0.87, 0.67, 0.68, 0.29, 0.83, 0.67, 0.68, 0.28, 0.89, 0.65, 0.68, 0.32, 0.85, 0.66, 0.69, 0.28, 0.80, 0.64, 0.69, 0.30, 0.82, 0.66, 0.69, 0.27, 0.75, 0.65

0.6259511260182079
0.81, 0.66, 0.68, 0.34, 0.82, 0.65, 0.68, 0.30, 0.85, 0.63, 0.67, 0.27, 0.85, 0.63, 0.68, 0.32, 0.82, 0.66, 0.68, 0.32, 0.87, 0.61, 0.68, 0.30, 0.88, 0.65, 0.68, 0.30, 0.83, 0.66, 0.68, 0.30, 0.90, 0.63, 0.67, 0.34, 0.87, 0.65, 0.67, 0.31, 0.80, 0.63, 0.69, 0.32, 0.82, 0.63, 0.67, 0.29, 0.75, 0.64

0.6564159080019166
0.86, 0.67, 0.73, 0.32, 0.92, 0.67, 0.73, 0.29, 0.89, 0.66, 0.72, 0.28, 0.84, 0.66, 0.73, 0.29, 0.89, 0.64, 0.74, 0.28, 0.90, 0.66, 0.74, 0.26, 0.92, 0.64, 0.72, 0.29, 0.92, 0.66, 0.73, 0.28, 0.92, 0.64, 0.72, 0.34, 0.91, 0.65, 0.73, 0.32, 0.93, 0.64, 0.73, 0.33, 0.93, 0.66, 0.73, 0.30, 0.92, 0.65


Running iteration 3 of 10 fold...
[28, 0, 12, 36, 20, 24, 8, 48, 40, 44]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.929712, 0.930216, 0.868743, 0.894321

    accuracy, precision, recall, f_score
max3: 0.929073, 0.929407, 0.842777, 0.872987

    accuracy, precision, recall, f_score
max1: 0.934835, 0.935041, 0.887983, 0.909258


min loss: 0.009, episode: 267000
max accu: 0.935, episode: 410000

4.01 classifiers used
    accuracy, precision, recall, f_score
mv: 0.865836, 0.868276, 0.742295, 0.781984
wv: 0.919023, 0.919716, 0.835212, 0.869623
fs: 0.979396, 0.979634, 0.973290, 0.971983
rl: 0.934835, 0.935041, 0.887983, 0.909258

0.6332353567625133
0.84, 0.63, 0.68, 0.33, 0.79, 0.66, 0.69, 0.31, 0.86, 0.66, 0.68, 0.31, 0.89, 0.64, 0.68, 0.30, 0.80, 0.66, 0.69, 0.30, 0.88, 0.64, 0.68, 0.27, 0.89, 0.66, 0.68, 0.29, 0.92, 0.67, 0.68, 0.32, 0.83, 0.66, 0.69, 0.32, 0.87, 0.67, 0.69, 0.31, 0.83, 0.64, 0.69, 0.29, 0.82, 0.65, 0.69, 0.30, 0.86, 0.66

0.6341725239616615
0.84, 0.64, 0.69, 0.33, 0.78, 0.67, 0.69, 0.30, 0.86, 0.67, 0.68, 0.31, 0.89, 0.65, 0.68, 0.29, 0.80, 0.67, 0.69, 0.29, 0.87, 0.64, 0.68, 0.27, 0.88, 0.68, 0.68, 0.28, 0.92, 0.67, 0.68, 0.31, 0.83, 0.67, 0.68, 0.32, 0.87, 0.67, 0.69, 0.30, 0.83, 0.65, 0.69, 0.29, 0.82, 0.66, 0.68, 0.30, 0.87, 0.66

0.6315764254911356
0.84, 0.61, 0.68, 0.33, 0.79, 0.65, 0.68, 0.31, 0.86, 0.66, 0.68, 0.31, 0.89, 0.65, 0.68, 0.30, 0.79, 0.65, 0.69, 0.31, 0.87, 0.62, 0.67, 0.27, 0.89, 0.67, 0.68, 0.30, 0.92, 0.67, 0.68, 0.32, 0.83, 0.66, 0.69, 0.31, 0.87, 0.64, 0.68, 0.31, 0.82, 0.63, 0.68, 0.29, 0.82, 0.66, 0.68, 0.30, 0.87, 0.66

0.6619070436032582
0.88, 0.64, 0.74, 0.32, 0.87, 0.65, 0.73, 0.31, 0.89, 0.64, 0.74, 0.34, 0.91, 0.65, 0.74, 0.33, 0.91, 0.66, 0.73, 0.29, 0.91, 0.66, 0.74, 0.33, 0.91, 0.64, 0.74, 0.30, 0.89, 0.65, 0.74, 0.36, 0.92, 0.67, 0.73, 0.32, 0.90, 0.66, 0.74, 0.32, 0.95, 0.65, 0.74, 0.30, 0.91, 0.64, 0.73, 0.29, 0.91, 0.64

    accuracy, precision, recall, f_score
mv: 0.861683, 0.864346, 0.780231, 0.816940
wv: 0.918703, 0.919553, 0.857868, 0.886082
fs: 0.972688, 0.972883, 0.965314, 0.971093
rl: 0.930842, 0.931468, 0.895949, 0.915191

fs avg size: 11.33333, rl avg size: 4.29500
full test avg accu: 0.65885, test avg accu: 0.62994

training takes 26988.523 sec
