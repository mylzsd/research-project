{'dataset': 'iris', 'algorithm': 'ptdqn', 'num_clf': 100, 'num_training': 350000, 'learning_rate': 0.1, 'discount_factor': 1.0, 'epsilon': 0.1, 'random_state': 183, 'portion': 0.5, 'sequential': False}
(300, 5)
reading data takes 0.019 sec
number of labels: 3

Running iteration 1 of 10 fold...
[26, 0, 59, 1, 34, 2, 47, 3, 57, 4, 61, 5]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.962963, 0.966762, 0.965812, 0.963964

    accuracy, precision, recall, f_score
max3: 0.962963, 0.966667, 0.966667, 0.964912

    accuracy, precision, recall, f_score
max1: 1.000000, 1.000000, 1.000000, 1.000000


min loss: 0.010, episode: 280000
max accu: 0.963, episode: 250000

6.50 classifiers used
    accuracy, precision, recall, f_score
mv: 1.000000, 1.000000, 1.000000, 1.000000
wv: 1.000000, 1.000000, 1.000000, 1.000000
fs: 1.000000, 1.000000, 1.000000, 1.000000
rl: 1.000000, 1.000000, 1.000000, 1.000000

0.956203703703704
0.96, 0.94, 0.92, 0.96, 0.96, 0.96, 0.95, 0.95, 0.95, 0.97, 0.95, 0.98, 0.92, 0.95, 0.96, 0.95, 0.91, 0.96, 0.92, 0.97, 0.96, 0.92, 0.94, 0.94, 0.96, 0.95, 0.99, 0.96, 0.97, 0.93, 0.94, 0.95, 0.96, 0.95, 0.94, 0.94, 0.96, 0.95, 0.93, 0.96, 0.96, 0.96, 0.95, 0.96, 0.96, 0.96, 0.95, 0.96, 0.96, 0.95, 0.96, 0.94, 0.90, 0.96, 0.96, 0.94, 0.95, 0.97, 0.95, 0.94, 0.97, 0.99, 0.96, 0.96, 0.95, 0.89, 0.96, 0.94, 0.97, 0.95, 0.96, 0.95, 0.95, 0.95, 0.95, 0.96, 0.95, 0.94, 0.95, 0.96, 0.96, 0.98, 0.96, 0.95, 0.97, 0.96, 0.97, 0.95, 0.96, 0.96, 0.95, 0.95, 0.96, 0.96, 0.95, 0.95, 0.96, 0.97, 0.97, 0.94

0.9525925925925927
0.96, 0.94, 0.94, 0.96, 0.96, 0.96, 0.96, 0.98, 0.94, 0.98, 0.96, 0.96, 0.92, 0.92, 0.96, 0.94, 0.90, 0.96, 0.94, 0.96, 0.96, 0.92, 0.92, 0.94, 0.96, 0.94, 0.98, 0.96, 0.96, 0.94, 0.96, 0.96, 0.96, 0.94, 0.94, 0.94, 0.96, 0.94, 0.96, 0.96, 0.96, 0.92, 0.94, 0.96, 0.96, 0.96, 0.94, 0.96, 0.96, 0.94, 0.92, 0.94, 0.88, 0.96, 0.96, 0.98, 0.96, 0.96, 0.94, 0.96, 0.96, 0.98, 0.96, 0.96, 0.94, 0.90, 0.96, 0.94, 0.94, 0.94, 0.92, 0.94, 0.94, 0.94, 0.94, 0.96, 0.94, 0.94, 0.94, 0.96, 0.96, 0.96, 0.96, 0.96, 0.94, 0.96, 0.94, 0.94, 0.96, 0.96, 0.94, 0.96, 0.96, 0.94, 0.96, 0.94, 0.96, 0.94, 0.94, 0.94

0.9790000000000001
1.0, 0.93, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 0.96, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 0.96, 0.96, 1.0, 1.0, 1.0, 1.0, 0.96, 0.93, 0.93, 1.0, 0.96, 1.0, 1.0, 0.96, 1.0, 0.96, 0.96, 1.0, 0.96, 0.93, 0.93, 1.0, 0.96, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 0.96, 0.96, 1.0, 0.96, 0.96, 0.93, 0.96, 1.0, 1.0, 0.96, 1.0, 0.93, 0.96, 0.9, 0.96, 1.0, 1.0, 1.0, 0.96, 0.9, 1.0, 0.96, 1.0, 1.0, 0.93, 0.96, 0.96, 1.0, 0.96, 1.0, 0.96, 0.96, 0.96, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 0.96, 0.96, 0.96, 1.0, 0.96, 1.0, 1.0, 0.96, 0.96, 0.96, 1.0, 0.96, 0.93, 0.96

0.9853333333333333
1.0, 0.96, 0.93, 1.0, 0.96, 1.0, 1.0, 0.96, 1.0, 1.0, 0.96, 0.96, 1.0, 0.93, 1.0, 0.93, 1.0, 1.0, 1.0, 0.93, 0.96, 1.0, 0.96, 1.0, 0.96, 1.0, 1.0, 0.96, 0.96, 0.96, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 0.96, 1.0, 0.96, 0.96, 1.0, 0.96, 1.0, 0.96, 1.0, 0.96, 1.0, 0.96, 1.0, 0.96, 1.0, 0.96, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 0.96, 0.96, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 0.96, 1.0, 1.0, 0.96, 0.93, 1.0, 0.96, 1.0, 1.0, 0.96, 0.96, 1.0, 1.0, 1.0, 0.96, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0


Running iteration 2 of 10 fold...
[27, 0, 8, 1, 6, 2, 19, 3, 77, 4, 84, 5, 86, 7]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.972222, 0.974415, 0.972973, 0.972217

    accuracy, precision, recall, f_score
max3: 0.962963, 0.966490, 0.966667, 0.965789

    accuracy, precision, recall, f_score
max1: 0.966667, 0.970000, 0.966667, 0.964912


min loss: 0.008, episode: 264000
max accu: 0.972, episode: 220000

2.83 classifiers used
    accuracy, precision, recall, f_score
mv: 1.000000, 1.000000, 1.000000, 1.000000
wv: 1.000000, 1.000000, 1.000000, 1.000000
fs: 1.000000, 1.000000, 1.000000, 1.000000
rl: 0.966667, 0.970000, 0.966667, 0.964912

0.9561111111111115
0.96, 0.95, 0.94, 0.96, 0.96, 0.96, 0.95, 0.96, 0.95, 0.96, 0.96, 0.95, 0.92, 0.95, 0.96, 0.96, 0.96, 0.96, 0.95, 0.97, 0.95, 0.96, 0.97, 0.97, 0.92, 0.95, 0.95, 0.99, 0.95, 0.95, 0.94, 0.95, 0.94, 0.95, 0.95, 0.96, 0.96, 0.92, 0.96, 0.95, 0.96, 0.95, 0.95, 0.94, 0.96, 0.98, 0.96, 0.96, 0.95, 0.93, 0.96, 0.91, 0.96, 0.93, 0.91, 0.95, 0.95, 0.96, 0.95, 0.96, 0.97, 0.97, 0.94, 0.94, 0.95, 0.91, 0.92, 0.96, 0.96, 0.95, 0.95, 0.95, 0.95, 0.97, 0.96, 0.93, 0.96, 0.98, 0.96, 0.95, 0.95, 0.97, 0.95, 0.93, 0.97, 0.95, 0.99, 0.96, 0.98, 0.96, 0.96, 0.97, 0.96, 0.94, 0.96, 0.94, 0.93, 0.97, 0.93, 0.95

0.9562962962962963
0.96, 0.94, 0.94, 0.96, 0.96, 0.96, 0.96, 0.96, 0.94, 0.94, 0.96, 0.96, 0.92, 0.96, 0.96, 0.96, 0.96, 0.94, 0.96, 0.98, 0.96, 0.96, 0.94, 0.94, 0.92, 1.0, 0.94, 1.0, 0.94, 0.94, 0.96, 0.96, 0.94, 0.96, 1.0, 0.96, 0.94, 0.92, 0.96, 0.94, 0.96, 0.96, 0.92, 0.94, 0.94, 0.96, 0.96, 0.96, 0.94, 0.96, 0.96, 0.92, 0.96, 0.92, 0.92, 0.96, 0.94, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.94, 0.96, 0.92, 0.90, 0.94, 0.96, 0.94, 0.96, 0.96, 0.96, 0.94, 0.96, 0.94, 0.96, 0.98, 0.96, 0.96, 0.96, 0.96, 0.96, 0.92, 0.98, 0.96, 0.98, 0.96, 0.96, 0.96, 0.96, 0.94, 0.96, 0.96, 0.96, 0.94, 0.94, 0.94, 0.94, 1.0

0.9666666666666667
0.93, 1.0, 0.93, 1.0, 1.0, 1.0, 0.93, 1.0, 1.0, 0.93, 0.96, 0.96, 1.0, 0.86, 0.96, 1.0, 0.96, 0.96, 0.93, 1.0, 0.93, 1.0, 0.96, 0.96, 1.0, 0.96, 0.96, 0.9, 0.96, 0.93, 0.96, 0.96, 0.96, 0.96, 1.0, 0.93, 0.93, 0.96, 0.93, 1.0, 0.96, 0.93, 1.0, 1.0, 0.96, 1.0, 1.0, 0.93, 1.0, 0.8, 0.96, 0.93, 0.96, 0.96, 0.9, 0.9, 1.0, 1.0, 0.93, 1.0, 0.96, 0.93, 0.96, 0.96, 0.96, 0.9, 0.96, 0.93, 0.96, 1.0, 0.96, 0.96, 0.96, 1.0, 1.0, 0.96, 1.0, 0.96, 0.96, 0.93, 0.96, 0.96, 1.0, 1.0, 0.9, 0.93, 1.0, 0.93, 0.96, 0.96, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0

0.9633333333333334
0.96, 0.96, 1.0, 0.9, 1.0, 1.0, 0.96, 0.96, 1.0, 0.96, 1.0, 0.96, 0.96, 0.96, 0.93, 0.93, 0.96, 0.93, 0.96, 1.0, 0.93, 0.96, 0.96, 1.0, 0.96, 0.96, 0.96, 0.96, 0.96, 1.0, 1.0, 0.93, 0.96, 1.0, 0.86, 0.93, 0.96, 0.96, 1.0, 1.0, 0.86, 0.96, 1.0, 0.9, 0.96, 0.93, 0.96, 0.9, 1.0, 0.96, 0.9, 0.96, 0.96, 0.9, 0.93, 1.0, 0.96, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 0.93, 1.0, 0.93, 1.0, 1.0, 0.96, 0.96, 0.93, 0.86, 0.93, 0.96, 0.93, 0.96, 0.9, 1.0, 0.93, 0.9, 1.0, 0.96, 1.0, 0.93, 0.96, 0.93, 0.93, 0.96, 0.96, 1.0, 0.93, 0.96, 0.96, 1.0, 1.0, 1.0, 0.93, 1.0, 0.96, 1.0


Running iteration 3 of 10 fold...
[41, 0, 10, 1, 8, 2, 17, 3, 51, 4, 16, 5, 23, 6, 43, 7, 61, 9, 78, 11]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.962963, 0.966490, 0.963964, 0.964286

    accuracy, precision, recall, f_score
max3: 0.962963, 0.966049, 0.960784, 0.964674

    accuracy, precision, recall, f_score
max1: 1.000000, 1.000000, 1.000000, 1.000000


min loss: 0.011, episode: 280000
max accu: 0.963, episode: 10000

1.00 classifiers used
    accuracy, precision, recall, f_score
mv: 1.000000, 1.000000, 1.000000, 1.000000
wv: 1.000000, 1.000000, 1.000000, 1.000000
fs: 1.000000, 1.000000, 1.000000, 1.000000
rl: 1.000000, 1.000000, 1.000000, 1.000000

0.9504629629629631
0.96, 0.91, 0.94, 0.97, 0.94, 0.94, 0.93, 0.96, 0.97, 0.95, 0.97, 0.96, 0.96, 0.92, 0.96, 0.96, 0.95, 0.96, 0.96, 0.95, 0.92, 0.96, 0.96, 0.96, 0.96, 0.93, 0.97, 0.93, 0.95, 0.91, 0.94, 0.93, 0.91, 0.92, 0.94, 0.94, 0.96, 0.92, 0.95, 0.96, 0.92, 0.99, 0.93, 0.95, 0.96, 0.98, 0.96, 0.97, 0.93, 0.95, 0.94, 0.98, 0.93, 0.91, 0.90, 0.93, 0.96, 0.96, 0.95, 0.94, 0.96, 0.93, 0.95, 0.91, 0.96, 0.96, 0.92, 0.96, 0.95, 0.93, 0.94, 0.89, 0.96, 0.97, 0.96, 0.97, 0.96, 0.95, 0.96, 0.94, 0.96, 0.95, 0.97, 0.92, 0.98, 0.95, 0.95, 0.94, 0.94, 0.94, 0.93, 0.91, 0.96, 0.95, 0.94, 0.97, 0.93, 0.93, 0.96, 0.96

0.9524074074074075
0.96, 0.94, 0.92, 0.94, 0.96, 0.96, 0.94, 0.96, 0.96, 0.94, 0.96, 0.92, 0.96, 0.92, 0.96, 0.96, 0.94, 0.96, 0.96, 0.96, 0.92, 0.96, 0.96, 0.98, 0.96, 0.94, 0.94, 0.94, 0.96, 0.92, 0.92, 0.94, 0.92, 0.94, 0.96, 0.96, 0.96, 0.94, 0.96, 0.96, 0.92, 0.98, 0.92, 0.92, 0.96, 0.96, 0.96, 0.94, 0.96, 0.96, 0.96, 0.98, 0.94, 0.92, 0.88, 0.96, 0.96, 0.96, 0.94, 0.96, 0.96, 0.98, 0.96, 0.94, 0.96, 0.96, 0.94, 0.96, 0.96, 0.94, 0.96, 0.90, 0.96, 0.94, 0.96, 0.94, 0.96, 0.96, 0.94, 0.96, 0.96, 0.94, 0.94, 0.94, 0.96, 0.96, 0.94, 0.94, 0.96, 0.96, 0.92, 0.94, 0.96, 0.94, 0.96, 0.94, 0.92, 0.96, 0.96, 0.96

0.9943333333333334
1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 0.96, 0.96, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 0.96, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 0.93, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 0.96, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0

0.9933333333333334
1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 0.96, 0.96, 0.96, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 0.96, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 0.96, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 0.96, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 1.0, 0.96, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0

    accuracy, precision, recall, f_score
mv: 1.000000, 1.000000, 1.000000, 1.000000
wv: 1.000000, 1.000000, 1.000000, 1.000000
fs: 1.000000, 1.000000, 1.000000, 1.000000
rl: 0.988889, 0.990000, 0.988889, 0.988304

fs avg size: 15.33333, rl avg size: 3.44444
full test avg accu: 0.98067, test avg accu: 0.98000

training takes 20397.677 sec
