{'dataset': 'heart', 'algorithm': 'ptdqn', 'num_clf': 100, 'num_training': 350000, 'learning_rate': 0.1, 'discount_factor': 1.0, 'epsilon': 0.1, 'random_state': 2597, 'portion': 0.5, 'sequential': False}
(270, 21)
reading data takes 0.847 sec
number of labels: 2

Running iteration 1 of 10 fold...
[63, 0, 22, 5, 75, 2, 55, 47, 9, 10]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.824742, 0.826214, 0.816537, 0.819840

    accuracy, precision, recall, f_score
max3: 0.918367, 0.918367, 0.915517, 0.915517

    accuracy, precision, recall, f_score
max1: 0.777778, 0.844444, 0.769231, 0.761765


min loss: 0.006, episode: 273000
max accu: 0.825, episode: 260000

38.74 classifiers used
    accuracy, precision, recall, f_score
mv: 0.814815, 0.863548, 0.807692, 0.805195
wv: 0.814815, 0.863548, 0.807692, 0.805195
fs: 0.740741, 0.776072, 0.733516, 0.727273
rl: 0.777778, 0.844444, 0.769231, 0.761765

0.7217525773195876
0.65, 0.65, 0.74, 0.73, 0.68, 0.74, 0.67, 0.71, 0.72, 0.78, 0.69, 0.69, 0.65, 0.73, 0.65, 0.74, 0.69, 0.75, 0.80, 0.81, 0.60, 0.70, 0.73, 0.73, 0.72, 0.72, 0.73, 0.79, 0.68, 0.78, 0.71, 0.73, 0.65, 0.69, 0.73, 0.71, 0.68, 0.77, 0.68, 0.73, 0.76, 0.76, 0.72, 0.67, 0.77, 0.77, 0.71, 0.78, 0.67, 0.76, 0.74, 0.72, 0.68, 0.73, 0.74, 0.79, 0.72, 0.75, 0.69, 0.70, 0.67, 0.73, 0.72, 0.82, 0.73, 0.68, 0.71, 0.61, 0.74, 0.68, 0.65, 0.76, 0.70, 0.68, 0.74, 0.73, 0.69, 0.76, 0.80, 0.71, 0.69, 0.69, 0.77, 0.64, 0.77, 0.68, 0.73, 0.65, 0.73, 0.69, 0.74, 0.72, 0.73, 0.75, 0.69, 0.79, 0.76, 0.72, 0.73, 0.77

0.7714285714285712
0.61, 0.73, 0.77, 0.79, 0.75, 0.71, 0.73, 0.75, 0.79, 0.79, 0.71, 0.73, 0.73, 0.71, 0.67, 0.77, 0.73, 0.81, 0.83, 0.87, 0.67, 0.75, 0.75, 0.73, 0.73, 0.71, 0.83, 0.83, 0.73, 0.83, 0.79, 0.79, 0.75, 0.75, 0.79, 0.77, 0.75, 0.75, 0.69, 0.77, 0.79, 0.83, 0.75, 0.77, 0.81, 0.79, 0.69, 0.79, 0.75, 0.73, 0.79, 0.71, 0.73, 0.79, 0.81, 0.81, 0.81, 0.71, 0.79, 0.83, 0.79, 0.75, 0.85, 0.91, 0.79, 0.67, 0.71, 0.69, 0.79, 0.73, 0.75, 0.85, 0.77, 0.69, 0.77, 0.77, 0.73, 0.83, 0.81, 0.75, 0.69, 0.67, 0.83, 0.75, 0.83, 0.69, 0.79, 0.71, 0.79, 0.77, 0.81, 0.79, 0.77, 0.87, 0.75, 0.85, 0.85, 0.77, 0.77, 0.83

0.7292592592592594
0.77, 0.66, 0.62, 0.74, 0.59, 0.70, 0.74, 0.77, 0.70, 0.77, 0.74, 0.74, 0.81, 0.70, 0.81, 0.55, 0.70, 0.62, 0.74, 0.66, 0.70, 0.74, 0.70, 0.66, 0.88, 0.66, 0.81, 0.77, 0.88, 0.74, 0.70, 0.74, 0.88, 0.77, 0.77, 0.70, 0.81, 0.66, 0.66, 0.81, 0.88, 0.88, 0.85, 0.85, 0.85, 0.81, 0.77, 0.70, 0.62, 0.66, 0.74, 0.59, 0.66, 0.81, 0.66, 0.66, 0.74, 0.66, 0.66, 0.77, 0.55, 0.70, 0.77, 0.74, 0.74, 0.62, 0.85, 0.51, 0.74, 0.70, 0.70, 0.77, 0.70, 0.77, 0.70, 0.74, 0.74, 0.74, 0.70, 0.85, 0.77, 0.74, 0.74, 0.77, 0.77, 0.66, 0.66, 0.62, 0.74, 0.62, 0.77, 0.85, 0.74, 0.81, 0.59, 0.70, 0.62, 0.59, 0.74, 0.62

0.7385185185185185
0.81, 0.70, 0.62, 0.74, 0.70, 0.62, 0.85, 0.77, 0.81, 0.74, 0.77, 0.77, 0.66, 0.62, 0.66, 0.66, 0.81, 0.81, 0.74, 0.55, 0.59, 0.59, 0.74, 0.74, 0.66, 0.62, 0.92, 0.74, 0.77, 0.66, 0.66, 0.85, 0.55, 0.81, 0.81, 0.74, 0.77, 0.77, 0.85, 0.77, 0.70, 0.70, 0.77, 0.59, 0.62, 0.70, 0.74, 0.77, 0.62, 0.70, 0.74, 0.66, 0.70, 0.81, 0.74, 0.85, 0.62, 0.81, 0.74, 0.77, 0.62, 0.81, 0.74, 0.85, 0.85, 0.74, 0.77, 0.70, 0.74, 0.81, 0.88, 0.77, 0.74, 0.77, 0.74, 0.70, 0.81, 0.81, 0.51, 0.59, 0.70, 0.74, 0.88, 0.81, 0.92, 0.77, 0.70, 0.74, 0.62, 0.77, 0.74, 0.74, 0.85, 0.81, 0.62, 0.62, 0.70, 0.85, 0.81, 0.66


Running iteration 2 of 10 fold...
[95, 0, 93, 5, 41, 14, 19, 27, 23, 36]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.824742, 0.827324, 0.821611, 0.822860

    accuracy, precision, recall, f_score
max3: 0.918367, 0.918367, 0.915517, 0.915517

    accuracy, precision, recall, f_score
max1: 0.888889, 0.893308, 0.891176, 0.883117


min loss: 0.006, episode: 263000
max accu: 0.825, episode: 350000

98.07 classifiers used
    accuracy, precision, recall, f_score
mv: 0.814815, 0.841066, 0.832353, 0.810659
wv: 0.814815, 0.841066, 0.832353, 0.810659
fs: 0.814815, 0.841066, 0.832353, 0.810659
rl: 0.888889, 0.893308, 0.891176, 0.883117

0.7139175257731959
0.74, 0.76, 0.75, 0.77, 0.69, 0.73, 0.74, 0.72, 0.73, 0.73, 0.71, 0.64, 0.72, 0.77, 0.72, 0.75, 0.71, 0.79, 0.74, 0.72, 0.74, 0.71, 0.72, 0.72, 0.67, 0.74, 0.78, 0.75, 0.67, 0.69, 0.67, 0.71, 0.73, 0.73, 0.65, 0.71, 0.67, 0.60, 0.74, 0.74, 0.74, 0.67, 0.65, 0.75, 0.71, 0.72, 0.73, 0.74, 0.63, 0.68, 0.67, 0.63, 0.75, 0.74, 0.72, 0.65, 0.69, 0.71, 0.72, 0.67, 0.71, 0.71, 0.70, 0.74, 0.73, 0.70, 0.76, 0.70, 0.79, 0.73, 0.69, 0.72, 0.75, 0.68, 0.70, 0.71, 0.71, 0.65, 0.70, 0.71, 0.77, 0.72, 0.73, 0.74, 0.69, 0.70, 0.69, 0.67, 0.68, 0.67, 0.63, 0.72, 0.68, 0.75, 0.67, 0.84, 0.68, 0.69, 0.67, 0.74

0.763877551020408
0.79, 0.83, 0.77, 0.83, 0.75, 0.75, 0.77, 0.75, 0.79, 0.71, 0.83, 0.67, 0.77, 0.81, 0.79, 0.81, 0.71, 0.79, 0.83, 0.77, 0.81, 0.73, 0.79, 0.73, 0.71, 0.75, 0.73, 0.87, 0.69, 0.73, 0.73, 0.75, 0.81, 0.73, 0.69, 0.73, 0.77, 0.61, 0.75, 0.83, 0.85, 0.77, 0.73, 0.79, 0.71, 0.73, 0.75, 0.77, 0.65, 0.75, 0.77, 0.63, 0.77, 0.79, 0.77, 0.79, 0.71, 0.75, 0.75, 0.71, 0.79, 0.77, 0.75, 0.79, 0.71, 0.77, 0.81, 0.83, 0.75, 0.77, 0.81, 0.73, 0.81, 0.73, 0.81, 0.71, 0.77, 0.65, 0.71, 0.75, 0.79, 0.79, 0.83, 0.79, 0.71, 0.77, 0.71, 0.79, 0.73, 0.73, 0.75, 0.79, 0.75, 0.81, 0.65, 0.89, 0.75, 0.71, 0.67, 0.83

0.7014814814814815
0.55, 0.77, 0.70, 0.70, 0.85, 0.66, 0.70, 0.62, 0.70, 0.74, 0.74, 0.74, 0.62, 0.85, 0.66, 0.85, 0.66, 0.85, 0.74, 0.81, 0.70, 0.66, 0.77, 0.77, 0.51, 0.55, 0.70, 0.66, 0.55, 0.81, 0.70, 0.70, 0.66, 0.70, 0.62, 0.70, 0.66, 0.85, 0.74, 0.81, 0.81, 0.81, 0.66, 0.59, 0.70, 0.55, 0.70, 0.70, 0.70, 0.70, 0.59, 0.59, 0.59, 0.77, 0.66, 0.81, 0.74, 0.77, 0.70, 0.70, 0.70, 0.77, 0.59, 0.59, 0.74, 0.74, 0.62, 0.74, 0.59, 0.81, 0.74, 0.66, 0.62, 0.51, 0.81, 0.74, 0.59, 0.59, 0.74, 0.59, 0.70, 0.59, 0.70, 0.70, 0.62, 0.74, 0.62, 0.70, 0.66, 0.62, 0.77, 0.77, 0.77, 0.77, 0.74, 0.81, 0.70, 0.55, 0.77, 0.74

0.7025925925925927
0.66, 0.51, 0.70, 0.70, 0.74, 0.77, 0.59, 0.70, 0.74, 0.77, 0.62, 0.66, 0.70, 0.62, 0.70, 0.81, 0.70, 0.62, 0.70, 0.66, 0.74, 0.59, 0.81, 0.70, 0.74, 0.66, 0.66, 0.70, 0.74, 0.66, 0.51, 0.74, 0.55, 0.70, 0.59, 0.62, 0.81, 0.70, 0.74, 0.74, 0.66, 0.59, 0.74, 0.66, 0.66, 0.70, 0.74, 0.62, 0.70, 0.70, 0.74, 0.70, 0.81, 0.70, 0.88, 0.74, 0.70, 0.66, 0.66, 0.70, 0.66, 0.77, 0.62, 0.77, 0.66, 0.62, 0.66, 0.62, 0.62, 0.74, 0.74, 0.74, 0.74, 0.59, 0.85, 0.70, 0.88, 0.74, 0.66, 0.74, 0.62, 0.55, 0.48, 0.74, 0.77, 0.81, 0.77, 0.96, 0.81, 0.70, 0.66, 0.66, 0.77, 0.59, 0.62, 0.92, 0.77, 0.59, 0.77, 0.66


Running iteration 3 of 10 fold...
[77, 0, 92, 1]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.876289, 0.880841, 0.869425, 0.873258

    accuracy, precision, recall, f_score
max3: 0.938776, 0.939163, 0.934524, 0.937099

    accuracy, precision, recall, f_score
max1: 0.851852, 0.851852, 0.851648, 0.851648


min loss: 0.009, episode: 242000
max accu: 0.876, episode: 320000

94.74 classifiers used
    accuracy, precision, recall, f_score
mv: 0.851852, 0.851852, 0.851648, 0.851648
wv: 0.851852, 0.851852, 0.851648, 0.851648
fs: 0.666667, 0.666667, 0.664835, 0.664828
rl: 0.851852, 0.851852, 0.851648, 0.851648

0.7208247422680414
0.65, 0.74, 0.62, 0.82, 0.71, 0.70, 0.74, 0.75, 0.69, 0.74, 0.70, 0.70, 0.72, 0.71, 0.73, 0.74, 0.77, 0.72, 0.75, 0.72, 0.76, 0.62, 0.74, 0.68, 0.73, 0.73, 0.73, 0.69, 0.78, 0.72, 0.76, 0.69, 0.74, 0.67, 0.64, 0.75, 0.71, 0.74, 0.73, 0.73, 0.71, 0.64, 0.73, 0.76, 0.76, 0.64, 0.70, 0.81, 0.67, 0.71, 0.71, 0.74, 0.81, 0.76, 0.72, 0.76, 0.79, 0.68, 0.76, 0.81, 0.69, 0.71, 0.74, 0.77, 0.72, 0.70, 0.68, 0.67, 0.75, 0.80, 0.67, 0.64, 0.78, 0.61, 0.72, 0.74, 0.74, 0.83, 0.73, 0.70, 0.55, 0.71, 0.63, 0.71, 0.73, 0.74, 0.68, 0.68, 0.73, 0.72, 0.74, 0.69, 0.78, 0.62, 0.65, 0.74, 0.71, 0.75, 0.70, 0.72

0.7516326530612244
0.67, 0.75, 0.59, 0.85, 0.75, 0.73, 0.79, 0.73, 0.71, 0.73, 0.69, 0.69, 0.73, 0.73, 0.79, 0.77, 0.75, 0.71, 0.81, 0.79, 0.79, 0.65, 0.73, 0.73, 0.79, 0.73, 0.69, 0.73, 0.79, 0.75, 0.73, 0.77, 0.77, 0.73, 0.69, 0.79, 0.73, 0.81, 0.81, 0.75, 0.71, 0.69, 0.71, 0.81, 0.71, 0.65, 0.77, 0.85, 0.69, 0.73, 0.79, 0.71, 0.83, 0.81, 0.81, 0.85, 0.79, 0.73, 0.73, 0.79, 0.69, 0.75, 0.77, 0.81, 0.75, 0.75, 0.71, 0.67, 0.83, 0.81, 0.65, 0.69, 0.81, 0.63, 0.77, 0.77, 0.79, 0.85, 0.85, 0.77, 0.63, 0.71, 0.65, 0.75, 0.79, 0.75, 0.69, 0.65, 0.75, 0.77, 0.75, 0.77, 0.83, 0.63, 0.69, 0.75, 0.73, 0.79, 0.81, 0.77

0.7525925925925928
0.66, 0.74, 0.70, 0.70, 0.85, 0.81, 0.85, 0.70, 0.74, 0.74, 0.77, 0.74, 0.70, 0.74, 0.74, 0.85, 0.81, 0.74, 0.81, 0.77, 0.70, 0.66, 0.85, 0.81, 0.66, 0.74, 0.85, 0.85, 0.74, 0.77, 0.74, 0.66, 0.81, 0.62, 0.74, 0.70, 0.74, 0.74, 0.81, 0.70, 0.59, 0.70, 0.88, 0.81, 0.88, 0.74, 0.66, 0.70, 0.62, 0.70, 0.77, 0.77, 0.81, 0.59, 0.77, 0.85, 0.85, 0.77, 0.77, 0.70, 0.77, 0.74, 0.88, 0.81, 0.70, 0.77, 0.74, 0.81, 0.74, 0.85, 0.81, 0.81, 0.85, 0.70, 0.77, 0.70, 0.74, 0.70, 0.70, 0.70, 0.51, 0.74, 0.66, 0.85, 0.70, 0.70, 0.77, 0.77, 0.74, 0.66, 0.70, 0.74, 0.74, 0.74, 0.77, 0.88, 0.77, 0.74, 0.66, 0.74

0.7525925925925928
0.66, 0.88, 0.74, 0.88, 0.81, 0.74, 0.85, 0.66, 0.74, 0.74, 0.77, 0.70, 0.77, 0.74, 0.85, 0.66, 0.85, 0.70, 0.77, 0.59, 0.70, 0.77, 0.77, 0.88, 0.88, 0.81, 0.77, 0.88, 0.62, 0.74, 0.66, 0.70, 0.66, 0.62, 0.81, 0.81, 0.77, 0.81, 0.85, 0.77, 0.81, 0.92, 0.77, 0.70, 0.70, 0.92, 0.66, 0.66, 0.74, 0.74, 0.62, 0.70, 0.81, 0.85, 0.66, 0.70, 0.62, 0.62, 0.70, 0.70, 0.81, 0.66, 0.66, 0.66, 0.74, 0.74, 0.74, 0.77, 0.74, 0.70, 0.70, 0.74, 0.70, 0.74, 0.74, 0.74, 0.81, 0.81, 0.81, 0.81, 0.74, 0.74, 0.77, 0.70, 0.70, 0.77, 0.85, 0.92, 0.85, 0.77, 0.62, 0.70, 0.70, 0.70, 0.77, 0.70, 0.85, 0.74, 0.66, 0.74

    accuracy, precision, recall, f_score
mv: 0.827160, 0.852155, 0.830565, 0.822501
wv: 0.827160, 0.852155, 0.830565, 0.822501
fs: 0.740741, 0.761268, 0.743568, 0.734253
rl: 0.839506, 0.863201, 0.837352, 0.832177

fs avg size: 8.00000, rl avg size: 77.18519
full test avg accu: 0.73123, test avg accu: 0.72778

training takes 21098.563 sec
