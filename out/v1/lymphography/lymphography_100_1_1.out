{'dataset': 'lymphography', 'algorithm': 'ptdqn', 'num_clf': 100, 'num_training': 350000, 'learning_rate': 0.1, 'discount_factor': 1.0, 'epsilon': 0.1, 'random_state': 524, 'portion': 0.5, 'sequential': False}
(148, 39)
reading data takes 0.438 sec
number of labels: 4

Running iteration 1 of 10 fold...
[34, 0, 18, 24, 45, 1, 3, 2, 4, 6, 10, 33, 11, 5, 21, 9, 54, 28, 13, 25, 31, 49, 17, 41, 38, 14, 58, 32, 48, 55, 69, 7, 26]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.811321, 0.764040, 0.439338, 0.414530

    accuracy, precision, recall, f_score
max3: 0.888889, 0.895062, 0.884868, 0.871224

    accuracy, precision, recall, f_score
max1: 0.733333, 0.746667, 0.723214, 0.722222


min loss: 0.004, episode: 280000
max accu: 0.811, episode: 350000

63.87 classifiers used
    accuracy, precision, recall, f_score
mv: 0.733333, 0.746667, 0.723214, 0.722222
wv: 0.733333, 0.746667, 0.723214, 0.722222
fs: 0.800000, 0.854545, 0.785714, 0.784689
rl: 0.733333, 0.746667, 0.723214, 0.722222

0.6937735849056602
0.62, 0.75, 0.58, 0.75, 0.77, 0.67, 0.62, 0.67, 0.71, 0.69, 0.75, 0.64, 0.64, 0.58, 0.50, 0.64, 0.67, 0.73, 0.79, 0.64, 0.67, 0.58, 0.60, 0.71, 0.73, 0.75, 0.54, 0.66, 0.75, 0.75, 0.66, 0.66, 0.71, 0.66, 0.83, 0.64, 0.60, 0.81, 0.79, 0.66, 0.73, 0.77, 0.58, 0.73, 0.69, 0.69, 0.73, 0.64, 0.73, 0.73, 0.75, 0.62, 0.75, 0.67, 0.77, 0.75, 0.71, 0.66, 0.71, 0.58, 0.71, 0.77, 0.49, 0.69, 0.67, 0.67, 0.58, 0.67, 0.67, 0.79, 0.58, 0.81, 0.66, 0.79, 0.66, 0.58, 0.66, 0.71, 0.69, 0.73, 0.64, 0.79, 0.69, 0.73, 0.75, 0.66, 0.71, 0.71, 0.73, 0.71, 0.71, 0.66, 0.69, 0.77, 0.77, 0.56, 0.73, 0.75, 0.71, 0.69

0.7185185185185183
0.62, 0.77, 0.59, 0.77, 0.85, 0.70, 0.62, 0.70, 0.77, 0.74, 0.81, 0.70, 0.74, 0.59, 0.51, 0.74, 0.81, 0.85, 0.88, 0.70, 0.70, 0.62, 0.51, 0.70, 0.70, 0.77, 0.59, 0.74, 0.77, 0.70, 0.62, 0.62, 0.77, 0.62, 0.81, 0.55, 0.66, 0.88, 0.85, 0.70, 0.77, 0.77, 0.62, 0.66, 0.74, 0.81, 0.70, 0.70, 0.66, 0.70, 0.77, 0.59, 0.88, 0.59, 0.85, 0.81, 0.74, 0.74, 0.77, 0.48, 0.81, 0.77, 0.48, 0.74, 0.74, 0.66, 0.62, 0.62, 0.70, 0.81, 0.55, 0.81, 0.66, 0.88, 0.74, 0.55, 0.66, 0.74, 0.77, 0.74, 0.70, 0.77, 0.74, 0.74, 0.77, 0.62, 0.70, 0.74, 0.77, 0.70, 0.70, 0.66, 0.70, 0.81, 0.74, 0.62, 0.85, 0.81, 0.77, 0.70

0.69
0.66, 0.6, 0.73, 0.8, 0.6, 0.66, 0.66, 0.6, 0.66, 0.73, 0.8, 0.6, 0.6, 0.66, 0.73, 0.66, 0.66, 0.6, 0.73, 0.8, 0.73, 0.46, 0.73, 0.73, 0.66, 0.6, 0.8, 0.46, 0.8, 0.66, 0.6, 0.73, 0.66, 0.6, 0.8, 0.53, 0.8, 0.73, 0.66, 0.66, 0.6, 0.66, 0.8, 0.46, 0.86, 0.53, 0.66, 0.66, 0.8, 0.66, 0.8, 0.66, 0.73, 0.66, 0.73, 0.66, 0.66, 0.86, 0.66, 0.86, 0.8, 0.8, 0.46, 0.6, 0.66, 0.73, 0.66, 0.66, 0.66, 0.8, 0.86, 0.86, 0.6, 0.53, 0.6, 0.93, 0.66, 0.73, 0.66, 0.6, 0.8, 0.8, 0.73, 0.66, 0.6, 0.6, 0.66, 0.73, 0.73, 0.8, 0.6, 0.66, 0.73, 0.66, 0.86, 0.6, 0.6, 0.66, 0.73, 0.66

0.7093333333333334
0.73, 0.73, 0.6, 0.73, 0.66, 0.73, 0.93, 0.8, 0.8, 0.8, 0.8, 0.53, 0.66, 0.6, 0.66, 0.73, 0.73, 0.66, 0.66, 0.73, 0.6, 0.86, 0.8, 0.6, 0.73, 0.8, 0.8, 0.6, 0.86, 0.66, 0.66, 0.73, 0.86, 0.8, 0.66, 0.73, 0.66, 0.86, 0.66, 0.66, 0.6, 0.8, 0.66, 0.86, 0.66, 0.86, 0.53, 0.73, 0.66, 0.8, 0.66, 0.66, 0.66, 0.73, 0.73, 0.53, 0.73, 0.73, 0.46, 0.8, 0.73, 0.8, 0.46, 0.93, 0.6, 0.73, 0.8, 0.66, 0.8, 0.6, 0.66, 0.6, 0.66, 0.6, 0.6, 0.86, 0.86, 0.8, 0.66, 0.53, 0.86, 0.73, 0.66, 0.73, 0.66, 0.73, 0.73, 0.73, 0.66, 0.66, 0.6, 0.73, 0.73, 0.66, 0.6, 0.66, 0.93, 0.66, 0.66, 0.53


Running iteration 2 of 10 fold...
[74, 0, 6, 11, 30, 3, 9, 4, 13, 5, 15, 7, 46, 8, 1, 10, 34, 12, 43, 17, 64, 22, 2, 23, 61, 14, 48, 24, 93, 25, 29, 16, 71, 18, 39, 28, 50, 32, 98, 35, 77, 19, 81, 40, 52, 58, 47, 59, 37, 60]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.867925, 0.835479, 0.441176, 0.437473

    accuracy, precision, recall, f_score
max3: 0.888889, 0.896199, 0.878571, 0.861538

    accuracy, precision, recall, f_score
max1: 0.733333, 0.630303, 0.600000, 0.555556


min loss: 0.008, episode: 245000
max accu: 0.868, episode: 230000

35.93 classifiers used
    accuracy, precision, recall, f_score
mv: 0.733333, 0.630303, 0.600000, 0.555556
wv: 0.733333, 0.630303, 0.600000, 0.555556
fs: 0.733333, 0.630303, 0.600000, 0.555556
rl: 0.733333, 0.630303, 0.600000, 0.555556

0.7264150943396228
0.67, 0.75, 0.54, 0.71, 0.73, 0.69, 0.77, 0.73, 0.67, 0.81, 0.64, 0.69, 0.71, 0.79, 0.73, 0.75, 0.83, 0.79, 0.73, 0.81, 0.73, 0.66, 0.81, 0.62, 0.73, 0.62, 0.64, 0.69, 0.77, 0.71, 0.69, 0.64, 0.67, 0.64, 0.84, 0.77, 0.69, 0.79, 0.73, 0.73, 0.64, 0.66, 0.73, 0.81, 0.73, 0.75, 0.81, 0.79, 0.69, 0.75, 0.84, 0.66, 0.69, 0.83, 0.75, 0.73, 0.67, 0.71, 0.67, 0.64, 0.66, 0.60, 0.69, 0.60, 0.79, 0.62, 0.79, 0.81, 0.81, 0.75, 0.58, 0.73, 0.81, 0.71, 0.86, 0.75, 0.67, 0.71, 0.67, 0.69, 0.77, 0.79, 0.67, 0.77, 0.75, 0.66, 0.67, 0.69, 0.77, 0.64, 0.62, 0.84, 0.73, 0.75, 0.73, 0.73, 0.79, 0.71, 0.81, 0.71

0.7233333333333333
0.66, 0.66, 0.66, 0.66, 0.74, 0.59, 0.74, 0.74, 0.62, 0.85, 0.70, 0.74, 0.74, 0.74, 0.62, 0.81, 0.81, 0.81, 0.81, 0.81, 0.74, 0.66, 0.85, 0.62, 0.77, 0.59, 0.55, 0.74, 0.77, 0.74, 0.70, 0.48, 0.66, 0.55, 0.88, 0.74, 0.74, 0.70, 0.81, 0.74, 0.62, 0.62, 0.74, 0.81, 0.85, 0.70, 0.81, 0.81, 0.70, 0.74, 0.81, 0.70, 0.70, 0.85, 0.70, 0.74, 0.62, 0.70, 0.70, 0.66, 0.70, 0.51, 0.66, 0.62, 0.81, 0.66, 0.85, 0.77, 0.77, 0.81, 0.48, 0.74, 0.88, 0.59, 0.88, 0.74, 0.62, 0.77, 0.62, 0.70, 0.81, 0.77, 0.66, 0.81, 0.70, 0.62, 0.66, 0.70, 0.70, 0.59, 0.62, 0.85, 0.66, 0.66, 0.81, 0.77, 0.88, 0.74, 0.77, 0.77

0.6626666666666666
0.73, 0.6, 0.53, 0.73, 0.66, 0.73, 0.6, 0.66, 0.73, 0.66, 0.73, 0.6, 0.6, 0.66, 0.8, 0.8, 0.66, 0.6, 0.66, 0.73, 0.66, 0.66, 0.66, 0.73, 0.73, 0.73, 0.66, 0.6, 0.66, 0.66, 0.66, 0.66, 0.66, 0.73, 0.53, 0.73, 0.73, 0.53, 0.6, 0.66, 0.8, 0.66, 0.66, 0.6, 0.8, 0.53, 0.66, 0.66, 0.66, 0.6, 0.6, 0.53, 0.66, 0.66, 0.73, 0.73, 0.73, 0.6, 0.73, 0.73, 0.6, 0.6, 0.73, 0.6, 0.66, 0.53, 0.73, 0.66, 0.6, 0.73, 0.73, 0.53, 0.66, 0.46, 0.73, 0.6, 0.6, 0.6, 0.73, 0.66, 0.66, 0.73, 0.53, 0.8, 0.6, 0.73, 0.66, 0.73, 0.73, 0.6, 0.6, 0.73, 0.6, 0.66, 0.66, 0.6, 0.6, 0.66, 0.53, 0.73

0.6366666666666666
0.66, 0.53, 0.53, 0.46, 0.73, 0.6, 0.53, 0.6, 0.73, 0.66, 0.6, 0.6, 0.4, 0.66, 0.53, 0.6, 0.66, 0.66, 0.73, 0.53, 0.8, 0.6, 0.66, 0.6, 0.73, 0.53, 0.46, 0.66, 0.73, 0.66, 0.46, 0.8, 0.73, 0.6, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.53, 0.6, 0.66, 0.66, 0.66, 0.66, 0.66, 0.53, 0.66, 0.73, 0.73, 0.46, 0.73, 0.73, 0.53, 0.73, 0.6, 0.53, 0.46, 0.6, 0.6, 0.53, 0.8, 0.66, 0.73, 0.66, 0.73, 0.66, 0.53, 0.6, 0.66, 0.53, 0.6, 0.66, 0.66, 0.6, 0.66, 0.4, 0.73, 0.66, 0.53, 0.66, 0.73, 0.73, 0.66, 0.66, 0.66, 0.8, 0.66, 0.6, 0.73, 0.73, 0.6, 0.6, 0.6, 0.73, 0.66, 0.6, 0.6


Running iteration 3 of 10 fold...
[14, 0, 85, 1, 9, 4, 7, 10, 2, 26, 8, 22, 12, 36, 47, 11, 5, 13, 3, 18, 19, 40, 16, 21, 33, 20, 27, 29, 32, 23, 44, 42, 54, 34, 31, 55, 52, 60, 6, 65, 15]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.811321, 0.793563, 0.657118, 0.655411

    accuracy, precision, recall, f_score
max3: 0.814815, 0.814815, 0.546296, 0.546296

    accuracy, precision, recall, f_score
max1: 0.800000, 0.795455, 0.750000, 0.761905


min loss: 0.006, episode: 290000
max accu: 0.811, episode: 220000

2.47 classifiers used
    accuracy, precision, recall, f_score
mv: 0.933333, 0.939394, 0.900000, 0.920635
wv: 0.933333, 0.939394, 0.900000, 0.920635
fs: 0.933333, 0.939394, 0.900000, 0.920635
rl: 0.800000, 0.795455, 0.750000, 0.761905

0.7105660377358491
0.66, 0.77, 0.71, 0.71, 0.69, 0.81, 0.64, 0.64, 0.79, 0.77, 0.73, 0.79, 0.73, 0.62, 0.88, 0.64, 0.86, 0.66, 0.62, 0.75, 0.64, 0.71, 0.69, 0.50, 0.66, 0.69, 0.62, 0.81, 0.66, 0.88, 0.67, 0.56, 0.84, 0.77, 0.71, 0.66, 0.66, 0.69, 0.75, 0.79, 0.73, 0.66, 0.66, 0.75, 0.81, 0.75, 0.71, 0.66, 0.71, 0.71, 0.79, 0.67, 0.66, 0.67, 0.64, 0.67, 0.50, 0.73, 0.73, 0.71, 0.79, 0.67, 0.64, 0.77, 0.62, 0.77, 0.66, 0.75, 0.58, 0.62, 0.69, 0.64, 0.77, 0.64, 0.81, 0.60, 0.69, 0.67, 0.58, 0.79, 0.73, 0.66, 0.71, 0.66, 0.69, 0.86, 0.64, 0.73, 0.79, 0.79, 0.62, 0.77, 0.60, 0.69, 0.73, 0.73, 0.77, 0.83, 0.81, 0.71

0.7092592592592593
0.70, 0.81, 0.70, 0.62, 0.70, 0.81, 0.70, 0.62, 0.77, 0.77, 0.74, 0.81, 0.70, 0.62, 0.88, 0.59, 0.88, 0.66, 0.66, 0.66, 0.51, 0.66, 0.77, 0.44, 0.66, 0.59, 0.70, 0.81, 0.59, 0.92, 0.62, 0.59, 0.85, 0.74, 0.70, 0.59, 0.66, 0.62, 0.77, 0.81, 0.74, 0.66, 0.62, 0.74, 0.74, 0.74, 0.81, 0.66, 0.70, 0.74, 0.77, 0.66, 0.74, 0.74, 0.74, 0.66, 0.44, 0.81, 0.74, 0.70, 0.85, 0.66, 0.66, 0.81, 0.55, 0.70, 0.74, 0.77, 0.59, 0.70, 0.66, 0.55, 0.77, 0.66, 0.81, 0.66, 0.66, 0.70, 0.55, 0.77, 0.66, 0.74, 0.85, 0.66, 0.70, 0.85, 0.62, 0.74, 0.77, 0.77, 0.59, 0.66, 0.51, 0.74, 0.81, 0.70, 0.74, 0.81, 0.85, 0.70

0.7473333333333333
0.66, 0.86, 0.73, 0.73, 0.73, 0.8, 0.6, 0.6, 0.66, 0.86, 0.8, 0.8, 0.53, 0.73, 0.86, 0.8, 0.93, 0.86, 0.8, 0.73, 0.8, 0.8, 0.86, 0.66, 0.66, 0.73, 0.73, 0.8, 0.66, 0.86, 0.6, 0.8, 0.86, 0.86, 0.8, 0.6, 0.73, 0.8, 0.73, 0.8, 0.86, 0.66, 0.6, 0.86, 0.73, 0.73, 0.93, 0.73, 0.73, 0.73, 0.8, 0.66, 0.66, 0.8, 0.66, 0.73, 0.6, 0.73, 0.73, 0.8, 0.73, 0.8, 0.73, 0.8, 0.8, 0.8, 0.66, 0.93, 0.73, 0.66, 0.73, 0.46, 0.66, 0.53, 0.6, 0.6, 0.73, 0.66, 0.73, 0.8, 0.8, 0.73, 0.6, 0.6, 0.66, 0.86, 0.73, 0.66, 0.73, 0.86, 0.8, 0.86, 0.66, 0.86, 0.73, 0.8, 0.86, 0.93, 0.73, 0.86

0.7186666666666666
0.6, 0.86, 0.6, 0.8, 0.8, 0.66, 0.73, 0.86, 0.8, 0.66, 0.86, 0.66, 0.66, 0.66, 0.73, 0.66, 0.8, 0.8, 0.86, 0.73, 0.73, 0.73, 0.73, 0.93, 0.66, 0.8, 0.73, 0.53, 0.8, 0.8, 0.8, 0.66, 0.6, 0.66, 0.66, 0.8, 0.93, 0.8, 0.33, 0.86, 0.8, 0.53, 0.93, 0.6, 0.73, 0.66, 0.6, 0.8, 0.73, 0.86, 0.6, 0.86, 0.73, 0.66, 0.73, 0.66, 0.66, 0.73, 0.53, 0.8, 0.8, 0.66, 0.73, 0.53, 0.53, 0.73, 0.93, 0.6, 0.73, 0.73, 0.8, 0.73, 0.66, 0.86, 0.73, 0.6, 0.73, 0.73, 0.8, 0.73, 0.53, 0.86, 0.86, 0.73, 0.66, 0.6, 0.6, 0.6, 0.73, 0.86, 0.46, 0.66, 0.73, 0.73, 0.66, 0.66, 0.66, 0.73, 0.86, 0.46

    accuracy, precision, recall, f_score
mv: 0.800000, 0.772121, 0.741071, 0.732804
wv: 0.800000, 0.772121, 0.741071, 0.732804
fs: 0.822222, 0.808081, 0.761905, 0.753626
rl: 0.755556, 0.724141, 0.691071, 0.679894

fs avg size: 41.33333, rl avg size: 34.08889
full test avg accu: 0.68822, test avg accu: 0.70000

training takes 22419.151 sec
