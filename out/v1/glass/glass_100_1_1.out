{'dataset': 'glass', 'algorithm': 'ptdqn', 'num_clf': 100, 'num_training': 350000, 'learning_rate': 0.1, 'discount_factor': 1.0, 'epsilon': 0.1, 'random_state': 7968, 'portion': 0.5, 'sequential': False}
(214, 10)
reading data takes 0.036 sec
number of labels: 6

Running iteration 1 of 10 fold...
[0, 1, 97, 10, 37, 20, 8]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.805195, 0.809524, 0.753456, 0.746699

    accuracy, precision, recall, f_score
max3: 0.846154, 0.815268, 0.727315, 0.726389

    accuracy, precision, recall, f_score
max1: 0.727273, 0.704296, 0.600000, 0.577965


min loss: 0.008, episode: 248000
max accu: 0.805, episode: 230000

34.82 classifiers used
    accuracy, precision, recall, f_score
mv: 0.727273, 0.704296, 0.600000, 0.577965
wv: 0.727273, 0.704296, 0.600000, 0.577965
fs: 0.681818, 0.727273, 0.729167, 0.720833
rl: 0.727273, 0.704296, 0.600000, 0.577965

0.5914285714285714
0.68, 0.49, 0.63, 0.51, 0.64, 0.59, 0.58, 0.67, 0.59, 0.57, 0.64, 0.55, 0.55, 0.63, 0.59, 0.58, 0.48, 0.58, 0.66, 0.49, 0.57, 0.62, 0.63, 0.55, 0.53, 0.61, 0.53, 0.55, 0.61, 0.62, 0.54, 0.67, 0.54, 0.62, 0.64, 0.66, 0.62, 0.68, 0.54, 0.62, 0.68, 0.62, 0.63, 0.59, 0.57, 0.50, 0.41, 0.58, 0.64, 0.68, 0.55, 0.55, 0.62, 0.58, 0.63, 0.50, 0.59, 0.58, 0.62, 0.58, 0.62, 0.64, 0.53, 0.64, 0.62, 0.58, 0.58, 0.57, 0.54, 0.58, 0.62, 0.51, 0.57, 0.58, 0.58, 0.61, 0.57, 0.51, 0.57, 0.66, 0.50, 0.62, 0.63, 0.62, 0.54, 0.59, 0.67, 0.57, 0.50, 0.62, 0.63, 0.67, 0.55, 0.42, 0.54, 0.63, 0.68, 0.59, 0.58, 0.44

0.6435897435897436
0.71, 0.46, 0.69, 0.56, 0.74, 0.56, 0.66, 0.74, 0.58, 0.58, 0.76, 0.51, 0.64, 0.69, 0.66, 0.71, 0.53, 0.64, 0.69, 0.51, 0.64, 0.71, 0.76, 0.64, 0.51, 0.69, 0.61, 0.53, 0.66, 0.61, 0.66, 0.66, 0.56, 0.69, 0.71, 0.69, 0.71, 0.69, 0.53, 0.64, 0.74, 0.69, 0.64, 0.64, 0.61, 0.53, 0.43, 0.61, 0.69, 0.79, 0.53, 0.64, 0.69, 0.64, 0.69, 0.53, 0.64, 0.61, 0.69, 0.69, 0.69, 0.71, 0.56, 0.64, 0.66, 0.58, 0.58, 0.76, 0.66, 0.69, 0.64, 0.51, 0.56, 0.61, 0.64, 0.64, 0.66, 0.51, 0.69, 0.74, 0.46, 0.76, 0.64, 0.71, 0.66, 0.66, 0.79, 0.66, 0.53, 0.66, 0.64, 0.69, 0.61, 0.56, 0.61, 0.66, 0.71, 0.69, 0.69, 0.53

0.6027272727272727
0.54, 0.68, 0.68, 0.63, 0.63, 0.63, 0.5, 0.54, 0.45, 0.54, 0.63, 0.45, 0.72, 0.54, 0.63, 0.5, 0.72, 0.63, 0.68, 0.59, 0.59, 0.72, 0.59, 0.59, 0.68, 0.72, 0.59, 0.63, 0.5, 0.63, 0.72, 0.5, 0.59, 0.5, 0.54, 0.54, 0.59, 0.59, 0.54, 0.54, 0.40, 0.63, 0.68, 0.5, 0.54, 0.68, 0.5, 0.72, 0.68, 0.68, 0.77, 0.68, 0.68, 0.54, 0.59, 0.54, 0.63, 0.63, 0.68, 0.54, 0.54, 0.54, 0.59, 0.54, 0.45, 0.5, 0.59, 0.68, 0.59, 0.59, 0.54, 0.63, 0.59, 0.45, 0.72, 0.54, 0.59, 0.63, 0.54, 0.68, 0.72, 0.5, 0.59, 0.77, 0.45, 0.72, 0.68, 0.59, 0.68, 0.45, 0.72, 0.81, 0.63, 0.45, 0.77, 0.5, 0.63, 0.68, 0.54, 0.40

0.585
0.59, 0.68, 0.54, 0.63, 0.59, 0.5, 0.54, 0.54, 0.59, 0.77, 0.54, 0.63, 0.5, 0.54, 0.68, 0.45, 0.5, 0.72, 0.54, 0.54, 0.54, 0.68, 0.54, 0.68, 0.54, 0.54, 0.63, 0.54, 0.63, 0.45, 0.63, 0.59, 0.54, 0.54, 0.45, 0.59, 0.36, 0.63, 0.5, 0.68, 0.68, 0.72, 0.68, 0.63, 0.59, 0.40, 0.68, 0.45, 0.5, 0.68, 0.40, 0.54, 0.54, 0.5, 0.72, 0.59, 0.45, 0.54, 0.45, 0.40, 0.5, 0.54, 0.63, 0.72, 0.54, 0.63, 0.72, 0.36, 0.68, 0.5, 0.68, 0.54, 0.59, 0.45, 0.63, 0.54, 0.63, 0.77, 0.68, 0.81, 0.59, 0.54, 0.59, 0.63, 0.54, 0.63, 0.68, 0.59, 0.5, 0.63, 0.68, 0.45, 0.72, 0.59, 0.54, 0.63, 0.63, 0.59, 0.45, 0.72


Running iteration 2 of 10 fold...
[61, 0, 94, 14, 41, 2, 1, 11, 3, 8, 29, 17, 52, 26, 5]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.792208, 0.783998, 0.698876, 0.686463

    accuracy, precision, recall, f_score
max3: 0.846154, 0.858745, 0.692857, 0.702041

    accuracy, precision, recall, f_score
max1: 0.863636, 0.753968, 0.833333, 0.799679


min loss: 0.012, episode: 240000
max accu: 0.792, episode: 350000

68.45 classifiers used
    accuracy, precision, recall, f_score
mv: 0.772727, 0.670455, 0.642857, 0.609524
wv: 0.772727, 0.670455, 0.642857, 0.609524
fs: 0.772727, 0.663518, 0.583333, 0.561254
rl: 0.863636, 0.753968, 0.833333, 0.799679

0.5868831168831169
0.64, 0.59, 0.59, 0.59, 0.49, 0.61, 0.64, 0.57, 0.61, 0.57, 0.55, 0.61, 0.51, 0.54, 0.64, 0.53, 0.51, 0.62, 0.54, 0.51, 0.66, 0.59, 0.48, 0.59, 0.57, 0.68, 0.63, 0.61, 0.64, 0.46, 0.58, 0.58, 0.62, 0.57, 0.64, 0.67, 0.57, 0.64, 0.57, 0.61, 0.38, 0.49, 0.64, 0.59, 0.58, 0.61, 0.66, 0.62, 0.62, 0.57, 0.59, 0.61, 0.59, 0.55, 0.57, 0.49, 0.48, 0.50, 0.54, 0.51, 0.51, 0.70, 0.61, 0.59, 0.54, 0.55, 0.63, 0.55, 0.59, 0.67, 0.61, 0.63, 0.55, 0.58, 0.51, 0.67, 0.61, 0.67, 0.55, 0.66, 0.57, 0.54, 0.61, 0.59, 0.50, 0.54, 0.64, 0.50, 0.57, 0.57, 0.58, 0.54, 0.63, 0.66, 0.58, 0.50, 0.70, 0.49, 0.63, 0.66

0.6223076923076922
0.71, 0.61, 0.53, 0.61, 0.61, 0.64, 0.61, 0.56, 0.64, 0.64, 0.56, 0.64, 0.53, 0.61, 0.61, 0.53, 0.53, 0.66, 0.64, 0.48, 0.74, 0.64, 0.46, 0.64, 0.56, 0.71, 0.66, 0.74, 0.66, 0.41, 0.71, 0.66, 0.64, 0.56, 0.71, 0.66, 0.56, 0.61, 0.58, 0.64, 0.38, 0.51, 0.66, 0.66, 0.64, 0.69, 0.69, 0.74, 0.69, 0.61, 0.71, 0.64, 0.71, 0.56, 0.56, 0.51, 0.48, 0.61, 0.48, 0.56, 0.58, 0.76, 0.61, 0.58, 0.48, 0.48, 0.64, 0.64, 0.61, 0.76, 0.58, 0.74, 0.64, 0.69, 0.46, 0.66, 0.66, 0.79, 0.61, 0.69, 0.64, 0.64, 0.56, 0.66, 0.46, 0.66, 0.69, 0.53, 0.64, 0.58, 0.58, 0.51, 0.82, 0.74, 0.58, 0.53, 0.79, 0.53, 0.69, 0.66

0.6345454545454545
0.54, 0.72, 0.63, 0.63, 0.63, 0.54, 0.68, 0.54, 0.68, 0.72, 0.5, 0.68, 0.68, 0.5, 0.54, 0.40, 0.72, 0.5, 0.68, 0.45, 0.59, 0.72, 0.59, 0.5, 0.72, 0.5, 0.59, 0.68, 0.68, 0.59, 0.68, 0.68, 0.54, 0.54, 0.68, 0.68, 0.63, 0.72, 0.59, 0.63, 0.36, 0.59, 0.72, 0.63, 0.72, 0.54, 0.72, 0.86, 0.63, 0.72, 0.59, 0.63, 0.72, 0.59, 0.63, 0.72, 0.59, 0.72, 0.5, 0.63, 0.59, 0.59, 0.59, 0.72, 0.68, 0.5, 0.72, 0.68, 0.63, 0.54, 0.68, 0.77, 0.63, 0.68, 0.59, 0.68, 0.59, 0.59, 0.63, 0.59, 0.68, 0.63, 0.68, 0.59, 0.5, 0.72, 0.77, 0.54, 0.77, 0.63, 0.54, 0.63, 0.68, 0.72, 0.59, 0.54, 0.86, 0.59, 0.77, 0.68

0.6522727272727273
0.68, 0.63, 0.54, 0.72, 0.5, 0.59, 0.63, 0.68, 0.54, 0.63, 0.68, 0.68, 0.63, 0.63, 0.68, 0.72, 0.77, 0.77, 0.77, 0.63, 0.59, 0.77, 0.68, 0.68, 0.5, 0.5, 0.68, 0.54, 0.54, 0.68, 0.77, 0.54, 0.63, 0.54, 0.54, 0.72, 0.59, 0.72, 0.54, 0.68, 0.81, 0.59, 0.63, 0.68, 0.63, 0.63, 0.59, 0.59, 0.72, 0.63, 0.68, 0.72, 0.59, 0.72, 0.72, 0.72, 0.77, 0.68, 0.63, 0.68, 0.59, 0.68, 0.59, 0.77, 0.54, 0.59, 0.68, 0.59, 0.72, 0.59, 0.63, 0.54, 0.63, 0.5, 0.68, 0.68, 0.63, 0.72, 0.72, 0.59, 0.86, 0.68, 0.54, 0.63, 0.77, 0.72, 0.72, 0.59, 0.63, 0.54, 0.63, 0.63, 0.68, 0.68, 0.68, 0.72, 0.63, 0.63, 0.54, 0.72


Running iteration 3 of 10 fold...
[23, 0, 74, 46, 47, 11, 3, 79, 14, 64, 53, 25, 1, 30]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.779221, 0.766201, 0.728725, 0.750292

    accuracy, precision, recall, f_score
max3: 0.846154, 0.837771, 0.741148, 0.752746

    accuracy, precision, recall, f_score
max1: 0.772727, 0.785714, 0.666667, 0.593939


min loss: 0.010, episode: 257000
max accu: 0.779, episode: 220000

34.18 classifiers used
    accuracy, precision, recall, f_score
mv: 0.818182, 0.818182, 0.766667, 0.584848
wv: 0.818182, 0.818182, 0.766667, 0.584848
fs: 0.818182, 0.818182, 0.650000, 0.513889
rl: 0.772727, 0.785714, 0.666667, 0.593939

0.5898701298701299
0.58, 0.55, 0.54, 0.50, 0.62, 0.45, 0.54, 0.54, 0.53, 0.50, 0.46, 0.67, 0.57, 0.57, 0.53, 0.57, 0.49, 0.66, 0.58, 0.50, 0.58, 0.50, 0.61, 0.71, 0.54, 0.58, 0.62, 0.51, 0.61, 0.63, 0.54, 0.57, 0.59, 0.54, 0.59, 0.53, 0.63, 0.61, 0.57, 0.50, 0.64, 0.66, 0.58, 0.66, 0.64, 0.59, 0.64, 0.62, 0.54, 0.63, 0.64, 0.59, 0.59, 0.67, 0.68, 0.57, 0.55, 0.61, 0.70, 0.67, 0.63, 0.54, 0.50, 0.63, 0.66, 0.51, 0.53, 0.58, 0.54, 0.54, 0.54, 0.53, 0.67, 0.64, 0.70, 0.61, 0.71, 0.51, 0.55, 0.64, 0.59, 0.66, 0.59, 0.54, 0.50, 0.62, 0.54, 0.68, 0.53, 0.55, 0.61, 0.62, 0.61, 0.64, 0.51, 0.64, 0.63, 0.59, 0.55, 0.53

0.6333333333333333
0.66, 0.66, 0.58, 0.58, 0.66, 0.43, 0.58, 0.56, 0.64, 0.58, 0.53, 0.66, 0.58, 0.61, 0.66, 0.64, 0.53, 0.69, 0.64, 0.46, 0.51, 0.51, 0.58, 0.69, 0.64, 0.56, 0.71, 0.56, 0.61, 0.69, 0.61, 0.66, 0.61, 0.64, 0.64, 0.61, 0.58, 0.76, 0.61, 0.61, 0.64, 0.74, 0.61, 0.61, 0.69, 0.61, 0.82, 0.64, 0.51, 0.66, 0.64, 0.56, 0.69, 0.74, 0.71, 0.61, 0.61, 0.61, 0.79, 0.76, 0.69, 0.58, 0.53, 0.69, 0.69, 0.64, 0.56, 0.69, 0.58, 0.48, 0.61, 0.58, 0.66, 0.71, 0.64, 0.64, 0.74, 0.58, 0.61, 0.79, 0.74, 0.71, 0.69, 0.58, 0.56, 0.58, 0.58, 0.74, 0.53, 0.56, 0.69, 0.64, 0.56, 0.66, 0.56, 0.69, 0.71, 0.64, 0.61, 0.53

0.5968181818181818
0.63, 0.63, 0.63, 0.68, 0.54, 0.5, 0.54, 0.54, 0.5, 0.40, 0.68, 0.59, 0.59, 0.59, 0.68, 0.40, 0.54, 0.81, 0.63, 0.40, 0.5, 0.68, 0.45, 0.72, 0.72, 0.63, 0.45, 0.45, 0.72, 0.68, 0.54, 0.63, 0.59, 0.63, 0.68, 0.77, 0.63, 0.68, 0.59, 0.63, 0.72, 0.63, 0.40, 0.59, 0.54, 0.63, 0.77, 0.68, 0.5, 0.59, 0.63, 0.5, 0.68, 0.59, 0.63, 0.59, 0.59, 0.5, 0.68, 0.72, 0.68, 0.5, 0.77, 0.72, 0.54, 0.54, 0.40, 0.45, 0.45, 0.59, 0.63, 0.54, 0.63, 0.63, 0.59, 0.36, 0.68, 0.63, 0.63, 0.81, 0.59, 0.54, 0.59, 0.59, 0.45, 0.54, 0.59, 0.63, 0.31, 0.77, 0.5, 0.63, 0.5, 0.68, 0.68, 0.68, 0.5, 0.63, 0.59, 0.5

0.5927272727272725
0.68, 0.54, 0.5, 0.63, 0.54, 0.40, 0.63, 0.5, 0.63, 0.68, 0.77, 0.59, 0.59, 0.63, 0.59, 0.68, 0.5, 0.59, 0.54, 0.72, 0.59, 0.63, 0.54, 0.68, 0.45, 0.59, 0.68, 0.68, 0.68, 0.54, 0.68, 0.77, 0.59, 0.54, 0.63, 0.63, 0.63, 0.5, 0.68, 0.59, 0.59, 0.59, 0.5, 0.59, 0.31, 0.59, 0.5, 0.45, 0.68, 0.77, 0.59, 0.77, 0.54, 0.54, 0.86, 0.54, 0.59, 0.59, 0.54, 0.54, 0.59, 0.54, 0.45, 0.54, 0.72, 0.5, 0.63, 0.54, 0.81, 0.63, 0.77, 0.54, 0.59, 0.5, 0.54, 0.59, 0.5, 0.54, 0.59, 0.59, 0.77, 0.63, 0.45, 0.77, 0.68, 0.36, 0.54, 0.68, 0.45, 0.72, 0.36, 0.90, 0.63, 0.5, 0.45, 0.31, 0.40, 0.63, 0.59, 0.5

    accuracy, precision, recall, f_score
mv: 0.772727, 0.730977, 0.669841, 0.590779
wv: 0.772727, 0.730977, 0.669841, 0.590779
fs: 0.757576, 0.736324, 0.654167, 0.598659
rl: 0.787879, 0.747993, 0.700000, 0.657194

fs avg size: 12.00000, rl avg size: 45.81818
full test avg accu: 0.61000, test avg accu: 0.61136

training takes 27816.869 sec
