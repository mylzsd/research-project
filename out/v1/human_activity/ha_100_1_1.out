{'dataset': 'human_activity', 'algorithm': 'ptdqn', 'num_clf': 100, 'num_training': 350000, 'learning_rate': 0.1, 'discount_factor': 1.0, 'epsilon': 0.1, 'random_state': 2441, 'portion': 0.5, 'sequential': False}
(10299, 562)
reading data takes 1.686 sec
number of labels: 6

Running iteration 1 of 10 fold...
[6, 0, 57, 60, 39, 87, 42, 32, 78, 99, 1]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.975189, 0.975246, 0.974897, 0.974913

    accuracy, precision, recall, f_score
max3: 0.970874, 0.970942, 0.970560, 0.970466

    accuracy, precision, recall, f_score
max1: 0.976699, 0.976943, 0.976860, 0.976639


min loss: 0.009, episode: 273000
max accu: 0.975, episode: 260000

99.98 classifiers used
    accuracy, precision, recall, f_score
mv: 0.976699, 0.976871, 0.977373, 0.977214
wv: 0.976699, 0.976871, 0.977373, 0.977214
fs: 0.968932, 0.969002, 0.967916, 0.967857
rl: 0.976699, 0.976943, 0.976860, 0.976639

0.8794417475728155
0.87, 0.86, 0.89, 0.88, 0.87, 0.86, 0.90, 0.87, 0.86, 0.89, 0.88, 0.86, 0.86, 0.89, 0.88, 0.89, 0.88, 0.87, 0.88, 0.89, 0.88, 0.86, 0.88, 0.87, 0.88, 0.88, 0.89, 0.88, 0.87, 0.85, 0.85, 0.86, 0.89, 0.90, 0.85, 0.88, 0.87, 0.89, 0.85, 0.88, 0.87, 0.90, 0.89, 0.89, 0.85, 0.86, 0.87, 0.87, 0.88, 0.89, 0.88, 0.87, 0.86, 0.88, 0.86, 0.86, 0.87, 0.88, 0.88, 0.84, 0.89, 0.87, 0.89, 0.88, 0.85, 0.90, 0.87, 0.88, 0.87, 0.86, 0.88, 0.87, 0.87, 0.87, 0.85, 0.89, 0.90, 0.86, 0.89, 0.86, 0.85, 0.88, 0.88, 0.88, 0.90, 0.88, 0.88, 0.90, 0.87, 0.87, 0.86, 0.88, 0.87, 0.88, 0.89, 0.87, 0.85, 0.88, 0.87, 0.86

0.8751510248112191
0.87, 0.86, 0.88, 0.87, 0.87, 0.85, 0.90, 0.87, 0.86, 0.88, 0.87, 0.85, 0.86, 0.89, 0.88, 0.89, 0.87, 0.86, 0.87, 0.88, 0.87, 0.86, 0.87, 0.86, 0.88, 0.87, 0.90, 0.87, 0.87, 0.84, 0.84, 0.86, 0.89, 0.89, 0.85, 0.87, 0.87, 0.89, 0.85, 0.88, 0.87, 0.90, 0.89, 0.90, 0.84, 0.87, 0.87, 0.86, 0.87, 0.89, 0.88, 0.86, 0.86, 0.87, 0.86, 0.86, 0.86, 0.88, 0.88, 0.84, 0.89, 0.87, 0.88, 0.87, 0.84, 0.89, 0.87, 0.89, 0.87, 0.85, 0.87, 0.86, 0.87, 0.88, 0.85, 0.88, 0.90, 0.85, 0.89, 0.84, 0.85, 0.87, 0.87, 0.88, 0.89, 0.88, 0.88, 0.89, 0.86, 0.87, 0.86, 0.87, 0.86, 0.88, 0.88, 0.87, 0.85, 0.88, 0.86, 0.86

0.8744368932038834
0.86, 0.85, 0.88, 0.89, 0.86, 0.85, 0.90, 0.86, 0.86, 0.89, 0.87, 0.87, 0.87, 0.89, 0.88, 0.88, 0.87, 0.85, 0.87, 0.89, 0.86, 0.87, 0.87, 0.87, 0.89, 0.85, 0.89, 0.88, 0.88, 0.85, 0.84, 0.84, 0.89, 0.90, 0.85, 0.87, 0.87, 0.89, 0.84, 0.88, 0.87, 0.88, 0.90, 0.90, 0.85, 0.86, 0.85, 0.88, 0.86, 0.87, 0.89, 0.88, 0.85, 0.87, 0.84, 0.85, 0.87, 0.88, 0.89, 0.82, 0.89, 0.85, 0.88, 0.88, 0.85, 0.88, 0.88, 0.87, 0.87, 0.87, 0.87, 0.87, 0.88, 0.87, 0.86, 0.88, 0.89, 0.87, 0.88, 0.86, 0.83, 0.87, 0.88, 0.87, 0.88, 0.88, 0.89, 0.88, 0.86, 0.87, 0.86, 0.87, 0.86, 0.88, 0.89, 0.86, 0.85, 0.86, 0.86, 0.84

0.8734660194174758
0.87, 0.84, 0.87, 0.87, 0.86, 0.85, 0.88, 0.86, 0.89, 0.87, 0.85, 0.81, 0.90, 0.88, 0.87, 0.89, 0.85, 0.90, 0.87, 0.83, 0.86, 0.87, 0.89, 0.86, 0.85, 0.87, 0.87, 0.87, 0.86, 0.89, 0.87, 0.86, 0.86, 0.88, 0.88, 0.89, 0.86, 0.85, 0.89, 0.86, 0.85, 0.82, 0.87, 0.90, 0.87, 0.88, 0.85, 0.85, 0.89, 0.86, 0.87, 0.86, 0.86, 0.89, 0.86, 0.87, 0.90, 0.84, 0.90, 0.88, 0.89, 0.86, 0.87, 0.83, 0.87, 0.87, 0.87, 0.87, 0.86, 0.86, 0.87, 0.87, 0.88, 0.86, 0.89, 0.84, 0.86, 0.88, 0.86, 0.86, 0.86, 0.88, 0.88, 0.86, 0.84, 0.88, 0.85, 0.86, 0.89, 0.86, 0.88, 0.88, 0.88, 0.85, 0.88, 0.87, 0.92, 0.87, 0.88, 0.88


Running iteration 2 of 10 fold...
[90, 0, 13, 22, 81, 11, 70, 75, 35, 9, 95]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.974380, 0.974412, 0.974205, 0.974204

    accuracy, precision, recall, f_score
max3: 0.969795, 0.969839, 0.969679, 0.969593

    accuracy, precision, recall, f_score
max1: 0.979612, 0.979633, 0.979604, 0.979632


min loss: 0.008, episode: 260000
max accu: 0.974, episode: 340000

99.68 classifiers used
    accuracy, precision, recall, f_score
mv: 0.981553, 0.981589, 0.981086, 0.981410
wv: 0.981553, 0.981589, 0.981086, 0.981410
fs: 0.969903, 0.970038, 0.969969, 0.969745
rl: 0.979612, 0.979633, 0.979604, 0.979632

0.8795846817691477
0.88, 0.87, 0.89, 0.86, 0.87, 0.90, 0.88, 0.86, 0.87, 0.87, 0.85, 0.89, 0.90, 0.89, 0.87, 0.88, 0.86, 0.89, 0.86, 0.89, 0.86, 0.87, 0.88, 0.87, 0.89, 0.88, 0.88, 0.84, 0.88, 0.84, 0.87, 0.88, 0.87, 0.87, 0.86, 0.85, 0.86, 0.88, 0.86, 0.90, 0.88, 0.88, 0.87, 0.89, 0.87, 0.88, 0.88, 0.87, 0.88, 0.87, 0.86, 0.87, 0.88, 0.86, 0.87, 0.88, 0.87, 0.86, 0.89, 0.88, 0.89, 0.87, 0.89, 0.88, 0.88, 0.87, 0.87, 0.89, 0.87, 0.87, 0.86, 0.88, 0.86, 0.88, 0.86, 0.86, 0.85, 0.87, 0.87, 0.88, 0.87, 0.86, 0.88, 0.87, 0.89, 0.89, 0.88, 0.88, 0.90, 0.88, 0.90, 0.85, 0.88, 0.87, 0.88, 0.88, 0.86, 0.88, 0.88, 0.88

0.8739428263214671
0.88, 0.86, 0.88, 0.85, 0.85, 0.90, 0.88, 0.86, 0.87, 0.87, 0.85, 0.88, 0.89, 0.88, 0.88, 0.88, 0.85, 0.88, 0.86, 0.88, 0.85, 0.87, 0.88, 0.87, 0.88, 0.88, 0.86, 0.84, 0.87, 0.84, 0.86, 0.88, 0.87, 0.85, 0.85, 0.85, 0.86, 0.87, 0.85, 0.89, 0.88, 0.88, 0.86, 0.89, 0.86, 0.88, 0.88, 0.87, 0.88, 0.86, 0.86, 0.86, 0.88, 0.86, 0.86, 0.88, 0.86, 0.85, 0.89, 0.88, 0.88, 0.86, 0.88, 0.87, 0.87, 0.85, 0.85, 0.88, 0.88, 0.87, 0.87, 0.87, 0.86, 0.88, 0.85, 0.84, 0.84, 0.86, 0.86, 0.88, 0.87, 0.86, 0.88, 0.87, 0.89, 0.88, 0.88, 0.87, 0.89, 0.87, 0.89, 0.85, 0.87, 0.86, 0.87, 0.87, 0.86, 0.88, 0.87, 0.88

0.8843203883495145
0.88, 0.87, 0.88, 0.86, 0.87, 0.90, 0.89, 0.87, 0.88, 0.89, 0.88, 0.89, 0.91, 0.89, 0.87, 0.89, 0.87, 0.88, 0.89, 0.89, 0.87, 0.87, 0.89, 0.89, 0.89, 0.89, 0.89, 0.83, 0.89, 0.85, 0.86, 0.90, 0.87, 0.87, 0.87, 0.85, 0.87, 0.88, 0.88, 0.90, 0.89, 0.89, 0.89, 0.90, 0.87, 0.88, 0.88, 0.9, 0.91, 0.88, 0.87, 0.86, 0.87, 0.88, 0.88, 0.89, 0.86, 0.87, 0.91, 0.87, 0.90, 0.88, 0.89, 0.87, 0.88, 0.87, 0.88, 0.89, 0.90, 0.88, 0.84, 0.88, 0.86, 0.89, 0.87, 0.84, 0.83, 0.88, 0.86, 0.88, 0.88, 0.85, 0.88, 0.88, 0.89, 0.90, 0.87, 0.88, 0.90, 0.85, 0.89, 0.87, 0.88, 0.89, 0.89, 0.90, 0.88, 0.89, 0.92, 0.88

0.8799126213592232
0.87, 0.87, 0.87, 0.89, 0.88, 0.85, 0.87, 0.87, 0.88, 0.87, 0.88, 0.89, 0.87, 0.88, 0.86, 0.86, 0.87, 0.87, 0.90, 0.84, 0.91, 0.88, 0.88, 0.88, 0.88, 0.85, 0.87, 0.87, 0.90, 0.88, 0.91, 0.88, 0.87, 0.86, 0.87, 0.89, 0.87, 0.89, 0.88, 0.90, 0.89, 0.88, 0.88, 0.88, 0.88, 0.90, 0.87, 0.89, 0.87, 0.86, 0.88, 0.85, 0.89, 0.85, 0.86, 0.88, 0.86, 0.86, 0.87, 0.89, 0.86, 0.86, 0.88, 0.90, 0.87, 0.88, 0.88, 0.87, 0.89, 0.90, 0.87, 0.89, 0.88, 0.87, 0.89, 0.85, 0.85, 0.89, 0.88, 0.86, 0.85, 0.85, 0.87, 0.86, 0.87, 0.85, 0.87, 0.88, 0.91, 0.86, 0.88, 0.86, 0.89, 0.89, 0.86, 0.88, 0.88, 0.89, 0.89, 0.85


Running iteration 3 of 10 fold...
[20, 0, 31, 43, 15, 7, 38, 23, 44, 2, 86, 30, 34, 47, 6, 63, 65, 9, 36, 87, 49, 37, 22, 13, 25, 52, 4, 96]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.975998, 0.976029, 0.975513, 0.975507

    accuracy, precision, recall, f_score
max3: 0.973031, 0.973047, 0.971972, 0.971999

    accuracy, precision, recall, f_score
max1: 0.972816, 0.972842, 0.972412, 0.972242


min loss: 0.008, episode: 271000
max accu: 0.976, episode: 260000

99.94 classifiers used
    accuracy, precision, recall, f_score
mv: 0.968932, 0.968935, 0.968405, 0.968455
wv: 0.968932, 0.968935, 0.968405, 0.968455
fs: 0.968932, 0.969255, 0.969307, 0.968643
rl: 0.972816, 0.972842, 0.972412, 0.972242

0.8774379719525349
0.88, 0.88, 0.88, 0.86, 0.84, 0.90, 0.88, 0.87, 0.88, 0.85, 0.85, 0.87, 0.88, 0.86, 0.89, 0.89, 0.87, 0.85, 0.86, 0.88, 0.90, 0.86, 0.88, 0.89, 0.87, 0.88, 0.86, 0.83, 0.83, 0.88, 0.86, 0.88, 0.88, 0.88, 0.88, 0.87, 0.89, 0.89, 0.85, 0.83, 0.85, 0.88, 0.87, 0.85, 0.88, 0.87, 0.87, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88, 0.87, 0.88, 0.85, 0.88, 0.88, 0.87, 0.90, 0.86, 0.87, 0.87, 0.86, 0.88, 0.88, 0.88, 0.86, 0.87, 0.87, 0.88, 0.88, 0.88, 0.88, 0.86, 0.87, 0.85, 0.87, 0.89, 0.88, 0.85, 0.87, 0.85, 0.88, 0.87, 0.88, 0.86, 0.87, 0.88, 0.88, 0.89, 0.88, 0.88, 0.88, 0.90, 0.87, 0.86, 0.89, 0.87, 0.85

0.8745253505933117
0.88, 0.88, 0.88, 0.86, 0.85, 0.90, 0.88, 0.87, 0.87, 0.83, 0.85, 0.87, 0.88, 0.86, 0.89, 0.89, 0.87, 0.85, 0.86, 0.88, 0.89, 0.85, 0.88, 0.88, 0.87, 0.87, 0.86, 0.83, 0.83, 0.88, 0.86, 0.87, 0.87, 0.88, 0.87, 0.87, 0.88, 0.89, 0.86, 0.83, 0.84, 0.88, 0.86, 0.85, 0.89, 0.86, 0.87, 0.88, 0.88, 0.89, 0.88, 0.88, 0.88, 0.87, 0.88, 0.85, 0.87, 0.88, 0.86, 0.89, 0.86, 0.87, 0.86, 0.85, 0.87, 0.88, 0.88, 0.85, 0.86, 0.87, 0.88, 0.88, 0.89, 0.88, 0.86, 0.87, 0.84, 0.87, 0.88, 0.88, 0.85, 0.86, 0.86, 0.88, 0.88, 0.87, 0.85, 0.86, 0.88, 0.88, 0.89, 0.88, 0.88, 0.88, 0.89, 0.88, 0.86, 0.89, 0.86, 0.85

0.8754563106796115
0.88, 0.89, 0.89, 0.86, 0.84, 0.88, 0.88, 0.88, 0.88, 0.84, 0.85, 0.87, 0.89, 0.88, 0.90, 0.90, 0.86, 0.85, 0.85, 0.88, 0.89, 0.87, 0.89, 0.86, 0.87, 0.88, 0.86, 0.85, 0.86, 0.88, 0.84, 0.87, 0.85, 0.88, 0.87, 0.86, 0.88, 0.87, 0.86, 0.86, 0.87, 0.87, 0.85, 0.85, 0.88, 0.86, 0.86, 0.86, 0.87, 0.88, 0.88, 0.88, 0.87, 0.87, 0.87, 0.84, 0.85, 0.88, 0.87, 0.89, 0.85, 0.87, 0.88, 0.85, 0.87, 0.88, 0.87, 0.87, 0.87, 0.87, 0.88, 0.87, 0.88, 0.87, 0.86, 0.86, 0.85, 0.89, 0.88, 0.88, 0.86, 0.88, 0.86, 0.87, 0.87, 0.88, 0.87, 0.88, 0.89, 0.89, 0.87, 0.88, 0.88, 0.89, 0.89, 0.87, 0.86, 0.89, 0.89, 0.83

0.873893203883495
0.88, 0.88, 0.88, 0.87, 0.87, 0.87, 0.87, 0.88, 0.86, 0.84, 0.88, 0.86, 0.88, 0.86, 0.89, 0.85, 0.86, 0.88, 0.85, 0.87, 0.88, 0.86, 0.86, 0.88, 0.85, 0.87, 0.89, 0.84, 0.83, 0.90, 0.87, 0.87, 0.88, 0.90, 0.85, 0.86, 0.86, 0.87, 0.87, 0.88, 0.87, 0.87, 0.87, 0.84, 0.88, 0.87, 0.85, 0.87, 0.88, 0.87, 0.87, 0.86, 0.88, 0.86, 0.87, 0.89, 0.88, 0.88, 0.85, 0.88, 0.85, 0.85, 0.89, 0.87, 0.86, 0.88, 0.85, 0.88, 0.88, 0.85, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.90, 0.84, 0.87, 0.86, 0.88, 0.88, 0.87, 0.88, 0.89, 0.89, 0.84, 0.88, 0.86, 0.87, 0.88, 0.89, 0.85, 0.86, 0.87, 0.90, 0.83, 0.86, 0.87, 0.88

    accuracy, precision, recall, f_score
mv: 0.975728, 0.975798, 0.975621, 0.975693
wv: 0.975728, 0.975798, 0.975621, 0.975693
fs: 0.969256, 0.969432, 0.969064, 0.968748
rl: 0.976375, 0.976473, 0.976292, 0.976171

fs avg size: 16.66667, rl avg size: 99.86699
full test avg accu: 0.87576, test avg accu: 0.87807

training takes 37681.323 sec
