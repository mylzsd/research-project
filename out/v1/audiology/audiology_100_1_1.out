{'dataset': 'audiology', 'algorithm': 'ptdqn', 'num_clf': 100, 'num_training': 350000, 'learning_rate': 0.1, 'discount_factor': 1.0, 'epsilon': 0.1, 'random_state': 4759, 'portion': 0.5, 'sequential': False}
(226, 95)
reading data takes 11.123 sec
number of labels: 24

Running iteration 1 of 10 fold...
[60, 0, 6, 61, 11, 28, 27, 16]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.777778, 0.755761, 0.561235, 0.513729

    accuracy, precision, recall, f_score
max3: 0.756098, 0.693148, 0.664352, 0.579302

    accuracy, precision, recall, f_score
max1: 0.652174, 0.573913, 0.384127, 0.300535


min loss: 0.013, episode: 246000
max accu: 0.778, episode: 340000

54.91 classifiers used
    accuracy, precision, recall, f_score
mv: 0.652174, 0.555383, 0.369312, 0.307381
wv: 0.652174, 0.555383, 0.369312, 0.307381
fs: 0.608696, 0.525362, 0.353439, 0.281319
rl: 0.652174, 0.573913, 0.384127, 0.300535

0.5603703703703703
0.49, 0.67, 0.56, 0.64, 0.58, 0.48, 0.59, 0.46, 0.46, 0.53, 0.56, 0.48, 0.56, 0.59, 0.43, 0.62, 0.59, 0.44, 0.55, 0.51, 0.61, 0.58, 0.53, 0.54, 0.62, 0.48, 0.58, 0.61, 0.59, 0.60, 0.59, 0.58, 0.55, 0.55, 0.60, 0.54, 0.50, 0.61, 0.60, 0.61, 0.58, 0.50, 0.49, 0.62, 0.50, 0.44, 0.50, 0.54, 0.50, 0.44, 0.53, 0.54, 0.56, 0.55, 0.58, 0.54, 0.64, 0.51, 0.64, 0.56, 0.71, 0.60, 0.59, 0.58, 0.65, 0.45, 0.59, 0.53, 0.59, 0.54, 0.56, 0.65, 0.59, 0.59, 0.51, 0.61, 0.44, 0.49, 0.55, 0.61, 0.64, 0.49, 0.65, 0.70, 0.59, 0.59, 0.43, 0.49, 0.61, 0.58, 0.55, 0.65, 0.48, 0.56, 0.53, 0.51, 0.56, 0.44, 0.56, 0.53

0.5190243902439025
0.43, 0.60, 0.48, 0.58, 0.58, 0.43, 0.53, 0.41, 0.41, 0.53, 0.43, 0.41, 0.56, 0.48, 0.39, 0.58, 0.58, 0.46, 0.46, 0.41, 0.56, 0.58, 0.43, 0.51, 0.58, 0.39, 0.56, 0.60, 0.58, 0.56, 0.53, 0.46, 0.53, 0.53, 0.53, 0.56, 0.46, 0.58, 0.51, 0.60, 0.53, 0.53, 0.41, 0.56, 0.36, 0.36, 0.51, 0.53, 0.43, 0.34, 0.46, 0.53, 0.51, 0.43, 0.48, 0.51, 0.63, 0.46, 0.70, 0.51, 0.70, 0.63, 0.58, 0.56, 0.58, 0.48, 0.53, 0.51, 0.60, 0.43, 0.53, 0.68, 0.56, 0.51, 0.43, 0.60, 0.39, 0.43, 0.58, 0.58, 0.63, 0.48, 0.58, 0.65, 0.58, 0.58, 0.39, 0.39, 0.51, 0.53, 0.51, 0.58, 0.41, 0.51, 0.51, 0.48, 0.56, 0.48, 0.51, 0.48

0.4856521739130436
0.47, 0.69, 0.34, 0.56, 0.43, 0.47, 0.65, 0.26, 0.43, 0.39, 0.47, 0.43, 0.47, 0.47, 0.30, 0.65, 0.56, 0.43, 0.69, 0.39, 0.60, 0.47, 0.56, 0.47, 0.52, 0.52, 0.43, 0.52, 0.43, 0.34, 0.34, 0.60, 0.47, 0.47, 0.56, 0.60, 0.39, 0.56, 0.65, 0.47, 0.60, 0.52, 0.34, 0.56, 0.47, 0.39, 0.34, 0.47, 0.52, 0.43, 0.43, 0.39, 0.52, 0.47, 0.65, 0.34, 0.60, 0.52, 0.52, 0.34, 0.65, 0.43, 0.52, 0.52, 0.69, 0.43, 0.43, 0.47, 0.43, 0.60, 0.52, 0.56, 0.39, 0.47, 0.43, 0.39, 0.39, 0.39, 0.52, 0.56, 0.52, 0.47, 0.65, 0.34, 0.52, 0.43, 0.30, 0.52, 0.56, 0.43, 0.47, 0.56, 0.47, 0.43, 0.34, 0.52, 0.56, 0.34, 0.56, 0.34

0.48521739130434777
0.47, 0.56, 0.43, 0.47, 0.52, 0.34, 0.43, 0.52, 0.56, 0.69, 0.39, 0.39, 0.47, 0.34, 0.52, 0.60, 0.52, 0.39, 0.52, 0.65, 0.60, 0.52, 0.39, 0.47, 0.47, 0.47, 0.34, 0.43, 0.52, 0.47, 0.43, 0.60, 0.43, 0.52, 0.47, 0.52, 0.60, 0.56, 0.56, 0.43, 0.39, 0.43, 0.52, 0.43, 0.52, 0.47, 0.60, 0.39, 0.65, 0.47, 0.30, 0.52, 0.52, 0.34, 0.52, 0.39, 0.60, 0.47, 0.34, 0.52, 0.69, 0.43, 0.52, 0.39, 0.52, 0.43, 0.52, 0.52, 0.47, 0.39, 0.52, 0.39, 0.56, 0.52, 0.47, 0.65, 0.39, 0.39, 0.60, 0.34, 0.47, 0.69, 0.47, 0.52, 0.47, 0.56, 0.52, 0.43, 0.65, 0.43, 0.47, 0.43, 0.43, 0.52, 0.52, 0.34, 0.43, 0.26, 0.34, 0.43


Running iteration 2 of 10 fold...
[94, 0, 9, 18, 36, 19, 1, 6, 20, 33, 44, 34, 2, 4, 32, 45, 12, 47, 23, 25, 3, 5, 72, 27, 16, 52, 29, 65, 11, 77, 21, 42, 7, 22]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.728395, 0.684156, 0.430798, 0.422254

    accuracy, precision, recall, f_score
max3: 0.707317, 0.625203, 0.452381, 0.422619

    accuracy, precision, recall, f_score
max1: 0.739130, 0.682609, 0.550000, 0.421296


min loss: 0.009, episode: 271000
max accu: 0.728, episode: 240000

22.17 classifiers used
    accuracy, precision, recall, f_score
mv: 0.782609, 0.709110, 0.566667, 0.471795
wv: 0.782609, 0.709110, 0.566667, 0.471795
fs: 0.739130, 0.640683, 0.550000, 0.471084
rl: 0.739130, 0.682609, 0.550000, 0.421296

0.5414814814814815
0.46, 0.62, 0.64, 0.62, 0.38, 0.56, 0.56, 0.60, 0.62, 0.58, 0.46, 0.43, 0.53, 0.55, 0.56, 0.51, 0.54, 0.62, 0.53, 0.58, 0.58, 0.51, 0.56, 0.44, 0.65, 0.45, 0.58, 0.60, 0.56, 0.45, 0.45, 0.55, 0.58, 0.59, 0.58, 0.69, 0.58, 0.43, 0.50, 0.44, 0.56, 0.59, 0.50, 0.53, 0.54, 0.43, 0.56, 0.49, 0.58, 0.58, 0.54, 0.48, 0.50, 0.59, 0.59, 0.59, 0.59, 0.54, 0.55, 0.54, 0.54, 0.51, 0.56, 0.49, 0.67, 0.53, 0.61, 0.64, 0.56, 0.53, 0.35, 0.65, 0.48, 0.40, 0.61, 0.58, 0.55, 0.48, 0.55, 0.56, 0.51, 0.60, 0.49, 0.53, 0.53, 0.50, 0.62, 0.48, 0.46, 0.46, 0.46, 0.50, 0.43, 0.54, 0.71, 0.54, 0.55, 0.40, 0.43, 0.50

0.49926829268292683
0.46, 0.58, 0.58, 0.60, 0.34, 0.51, 0.56, 0.58, 0.63, 0.51, 0.41, 0.43, 0.39, 0.46, 0.53, 0.46, 0.43, 0.63, 0.53, 0.58, 0.56, 0.41, 0.51, 0.39, 0.63, 0.39, 0.53, 0.56, 0.56, 0.39, 0.39, 0.51, 0.51, 0.48, 0.56, 0.68, 0.56, 0.39, 0.43, 0.34, 0.53, 0.68, 0.41, 0.53, 0.46, 0.46, 0.58, 0.53, 0.51, 0.48, 0.56, 0.46, 0.51, 0.51, 0.51, 0.51, 0.53, 0.43, 0.48, 0.46, 0.51, 0.51, 0.51, 0.43, 0.65, 0.51, 0.63, 0.65, 0.58, 0.46, 0.34, 0.65, 0.41, 0.41, 0.53, 0.51, 0.53, 0.46, 0.51, 0.48, 0.46, 0.60, 0.39, 0.48, 0.46, 0.39, 0.56, 0.41, 0.39, 0.39, 0.41, 0.43, 0.39, 0.43, 0.73, 0.53, 0.51, 0.36, 0.36, 0.46

0.5230434782608697
0.30, 0.60, 0.69, 0.39, 0.39, 0.65, 0.39, 0.39, 0.52, 0.52, 0.43, 0.56, 0.52, 0.65, 0.65, 0.47, 0.56, 0.43, 0.65, 0.47, 0.52, 0.56, 0.34, 0.43, 0.65, 0.43, 0.56, 0.60, 0.60, 0.52, 0.47, 0.65, 0.39, 0.52, 0.56, 0.65, 0.52, 0.43, 0.43, 0.52, 0.21, 0.73, 0.34, 0.30, 0.43, 0.43, 0.56, 0.56, 0.56, 0.78, 0.39, 0.56, 0.47, 0.56, 0.47, 0.56, 0.69, 0.52, 0.60, 0.56, 0.60, 0.52, 0.56, 0.30, 0.69, 0.39, 0.60, 0.65, 0.60, 0.30, 0.30, 0.65, 0.47, 0.34, 0.60, 0.65, 0.65, 0.43, 0.52, 0.47, 0.56, 0.60, 0.43, 0.43, 0.65, 0.52, 0.52, 0.30, 0.65, 0.39, 0.56, 0.43, 0.56, 0.65, 0.69, 0.65, 0.56, 0.60, 0.34, 0.60

0.5273913043478262
0.56, 0.65, 0.65, 0.56, 0.60, 0.39, 0.65, 0.30, 0.69, 0.60, 0.69, 0.65, 0.73, 0.69, 0.43, 0.56, 0.65, 0.56, 0.47, 0.26, 0.39, 0.60, 0.69, 0.21, 0.39, 0.52, 0.56, 0.65, 0.52, 0.52, 0.47, 0.52, 0.52, 0.60, 0.60, 0.60, 0.43, 0.56, 0.60, 0.43, 0.56, 0.60, 0.69, 0.39, 0.56, 0.39, 0.39, 0.43, 0.47, 0.52, 0.34, 0.56, 0.43, 0.56, 0.39, 0.52, 0.56, 0.47, 0.47, 0.60, 0.34, 0.56, 0.69, 0.69, 0.60, 0.43, 0.73, 0.52, 0.43, 0.65, 0.39, 0.73, 0.34, 0.60, 0.65, 0.52, 0.60, 0.47, 0.52, 0.39, 0.52, 0.56, 0.56, 0.21, 0.39, 0.56, 0.60, 0.52, 0.52, 0.26, 0.60, 0.52, 0.47, 0.39, 0.47, 0.30, 0.52, 0.69, 0.52, 0.60


Running iteration 3 of 10 fold...
[73, 0, 84, 7, 57, 17, 51, 38, 55, 16, 27]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.728395, 0.669865, 0.401494, 0.373350

    accuracy, precision, recall, f_score
max3: 0.707317, 0.565563, 0.428571, 0.380524

    accuracy, precision, recall, f_score
max1: 0.782609, 0.759834, 0.683333, 0.672308


min loss: 0.008, episode: 226000
max accu: 0.728, episode: 240000

45.00 classifiers used
    accuracy, precision, recall, f_score
mv: 0.652174, 0.650104, 0.516667, 0.492727
wv: 0.652174, 0.650104, 0.516667, 0.492727
fs: 0.739130, 0.614493, 0.666667, 0.623889
rl: 0.782609, 0.759834, 0.683333, 0.672308

0.527037037037037
0.48, 0.49, 0.53, 0.60, 0.58, 0.41, 0.46, 0.51, 0.46, 0.49, 0.58, 0.61, 0.56, 0.54, 0.55, 0.48, 0.41, 0.50, 0.45, 0.56, 0.50, 0.65, 0.50, 0.56, 0.49, 0.43, 0.60, 0.35, 0.55, 0.54, 0.55, 0.58, 0.59, 0.49, 0.49, 0.43, 0.62, 0.51, 0.48, 0.56, 0.44, 0.53, 0.53, 0.56, 0.48, 0.55, 0.49, 0.61, 0.60, 0.54, 0.45, 0.54, 0.54, 0.58, 0.43, 0.62, 0.50, 0.62, 0.54, 0.51, 0.30, 0.60, 0.41, 0.45, 0.50, 0.40, 0.58, 0.54, 0.50, 0.58, 0.48, 0.55, 0.43, 0.66, 0.60, 0.51, 0.64, 0.49, 0.44, 0.48, 0.59, 0.49, 0.56, 0.44, 0.51, 0.56, 0.50, 0.55, 0.65, 0.48, 0.50, 0.56, 0.46, 0.59, 0.50, 0.61, 0.56, 0.51, 0.60, 0.45

0.49512195121951225
0.48, 0.58, 0.48, 0.60, 0.56, 0.34, 0.41, 0.41, 0.43, 0.43, 0.63, 0.63, 0.51, 0.53, 0.53, 0.48, 0.41, 0.34, 0.41, 0.48, 0.53, 0.65, 0.46, 0.53, 0.43, 0.43, 0.56, 0.41, 0.53, 0.53, 0.51, 0.58, 0.58, 0.43, 0.46, 0.39, 0.70, 0.39, 0.41, 0.51, 0.48, 0.46, 0.58, 0.56, 0.41, 0.46, 0.43, 0.63, 0.58, 0.46, 0.34, 0.48, 0.48, 0.53, 0.36, 0.65, 0.51, 0.51, 0.58, 0.48, 0.24, 0.60, 0.34, 0.43, 0.41, 0.36, 0.51, 0.56, 0.51, 0.48, 0.41, 0.48, 0.41, 0.60, 0.60, 0.46, 0.58, 0.43, 0.36, 0.46, 0.60, 0.46, 0.53, 0.39, 0.48, 0.46, 0.48, 0.51, 0.65, 0.41, 0.48, 0.60, 0.39, 0.58, 0.46, 0.60, 0.46, 0.51, 0.58, 0.46

0.497391304347826
0.39, 0.39, 0.47, 0.60, 0.39, 0.43, 0.52, 0.34, 0.34, 0.52, 0.47, 0.69, 0.56, 0.39, 0.69, 0.30, 0.26, 0.52, 0.43, 0.43, 0.56, 0.47, 0.47, 0.65, 0.52, 0.56, 0.60, 0.34, 0.56, 0.52, 0.39, 0.60, 0.56, 0.52, 0.47, 0.30, 0.56, 0.43, 0.34, 0.69, 0.47, 0.47, 0.47, 0.52, 0.52, 0.73, 0.39, 0.56, 0.43, 0.39, 0.39, 0.43, 0.60, 0.47, 0.47, 0.78, 0.56, 0.73, 0.60, 0.56, 0.47, 0.39, 0.34, 0.47, 0.47, 0.43, 0.52, 0.56, 0.30, 0.60, 0.56, 0.73, 0.30, 0.60, 0.47, 0.52, 0.56, 0.47, 0.43, 0.43, 0.47, 0.39, 0.56, 0.43, 0.65, 0.56, 0.39, 0.60, 0.56, 0.34, 0.69, 0.39, 0.52, 0.56, 0.30, 0.52, 0.52, 0.43, 0.60, 0.34

0.48608695652173917
0.52, 0.60, 0.17, 0.39, 0.34, 0.39, 0.52, 0.43, 0.60, 0.52, 0.34, 0.43, 0.43, 0.69, 0.43, 0.47, 0.52, 0.30, 0.39, 0.56, 0.43, 0.60, 0.52, 0.69, 0.52, 0.52, 0.47, 0.56, 0.56, 0.47, 0.52, 0.52, 0.43, 0.47, 0.56, 0.60, 0.47, 0.69, 0.56, 0.52, 0.34, 0.43, 0.30, 0.60, 0.65, 0.52, 0.43, 0.39, 0.56, 0.52, 0.56, 0.43, 0.65, 0.52, 0.60, 0.47, 0.52, 0.52, 0.39, 0.47, 0.43, 0.60, 0.39, 0.52, 0.47, 0.52, 0.34, 0.56, 0.34, 0.34, 0.52, 0.47, 0.73, 0.39, 0.26, 0.39, 0.30, 0.39, 0.47, 0.39, 0.34, 0.56, 0.52, 0.47, 0.47, 0.47, 0.34, 0.21, 0.52, 0.56, 0.43, 0.39, 0.43, 0.52, 0.39, 0.65, 0.52, 0.78, 0.56, 0.60

    accuracy, precision, recall, f_score
mv: 0.695652, 0.638199, 0.484215, 0.423968
wv: 0.695652, 0.638199, 0.484215, 0.423968
fs: 0.695652, 0.593513, 0.523369, 0.458764
rl: 0.724638, 0.672119, 0.539153, 0.464713

fs avg size: 17.66667, rl avg size: 40.69565
full test avg accu: 0.49957, test avg accu: 0.50203

training takes 38997.282 sec
