{'dataset': 'credit_card', 'algorithm': 'ptdqn', 'num_clf': 100, 'num_training': 400000, 'learning_rate': 0.1, 'discount_factor': 1.0, 'epsilon': 0.1, 'random_state': 4578, 'portion': 0.5, 'sequential': False}
(30000, 31)
reading data takes 0.440 sec
number of labels: 2

Running iteration 1 of 10 fold...
[97, 0, 12, 9, 14, 78, 42, 4, 98, 44, 69]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.814630, 0.796202, 0.653151, 0.676579

    accuracy, precision, recall, f_score
max3: 0.812407, 0.794297, 0.655823, 0.678766

    accuracy, precision, recall, f_score
max1: 0.822333, 0.808414, 0.664981, 0.691657


min loss: 0.010, episode: 241000
max accu: 0.815, episode: 320000

96.70 classifiers used
    accuracy, precision, recall, f_score
mv: 0.818667, 0.803710, 0.656425, 0.682171
wv: 0.818667, 0.803848, 0.655395, 0.681194
fs: 0.807667, 0.788410, 0.650331, 0.672094
rl: 0.822333, 0.808414, 0.664981, 0.691657

0.7225444444444444
0.72, 0.71, 0.71, 0.72, 0.73, 0.71, 0.72, 0.71, 0.72, 0.72, 0.72, 0.71, 0.72, 0.72, 0.72, 0.72, 0.71, 0.72, 0.72, 0.71, 0.72, 0.72, 0.72, 0.71, 0.72, 0.71, 0.71, 0.71, 0.73, 0.72, 0.72, 0.72, 0.72, 0.71, 0.72, 0.72, 0.71, 0.71, 0.72, 0.72, 0.71, 0.71, 0.72, 0.72, 0.71, 0.72, 0.72, 0.72, 0.72, 0.72, 0.71, 0.72, 0.72, 0.72, 0.72, 0.71, 0.72, 0.72, 0.72, 0.71, 0.72, 0.71, 0.71, 0.71, 0.71, 0.72, 0.71, 0.72, 0.71, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.71, 0.71, 0.71, 0.72, 0.71, 0.73, 0.72, 0.72, 0.72, 0.73, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.71, 0.72, 0.72, 0.72, 0.72, 0.73, 0.72, 0.72

0.7216314814814814
0.72, 0.71, 0.72, 0.72, 0.73, 0.71, 0.72, 0.71, 0.72, 0.73, 0.72, 0.70, 0.72, 0.72, 0.72, 0.72, 0.71, 0.72, 0.72, 0.71, 0.72, 0.73, 0.71, 0.71, 0.72, 0.72, 0.71, 0.72, 0.73, 0.72, 0.73, 0.72, 0.72, 0.71, 0.72, 0.72, 0.72, 0.71, 0.72, 0.72, 0.71, 0.72, 0.71, 0.73, 0.71, 0.71, 0.72, 0.72, 0.72, 0.72, 0.72, 0.71, 0.72, 0.72, 0.73, 0.72, 0.72, 0.72, 0.72, 0.72, 0.71, 0.71, 0.70, 0.71, 0.71, 0.71, 0.71, 0.72, 0.72, 0.72, 0.71, 0.72, 0.71, 0.71, 0.72, 0.71, 0.71, 0.71, 0.71, 0.71, 0.73, 0.71, 0.72, 0.72, 0.72, 0.71, 0.72, 0.72, 0.72, 0.72, 0.71, 0.71, 0.72, 0.72, 0.72, 0.72, 0.71, 0.73, 0.72, 0.72

0.7250633333333333
0.73, 0.73, 0.73, 0.73, 0.73, 0.71, 0.72, 0.72, 0.72, 0.72, 0.71, 0.71, 0.72, 0.71, 0.72, 0.72, 0.73, 0.72, 0.72, 0.73, 0.73, 0.73, 0.71, 0.70, 0.73, 0.73, 0.71, 0.72, 0.72, 0.72, 0.72, 0.71, 0.73, 0.72, 0.71, 0.72, 0.71, 0.73, 0.71, 0.72, 0.72, 0.70, 0.72, 0.73, 0.72, 0.72, 0.72, 0.71, 0.72, 0.73, 0.71, 0.73, 0.72, 0.72, 0.72, 0.71, 0.72, 0.72, 0.74, 0.72, 0.72, 0.72, 0.71, 0.72, 0.72, 0.70, 0.71, 0.72, 0.71, 0.71, 0.72, 0.72, 0.73, 0.73, 0.72, 0.72, 0.73, 0.73, 0.72, 0.70, 0.73, 0.72, 0.72, 0.73, 0.71, 0.72, 0.72, 0.72, 0.71, 0.73, 0.71, 0.72, 0.72, 0.73, 0.72, 0.73, 0.73, 0.73, 0.71, 0.71

0.72406
0.72, 0.71, 0.73, 0.72, 0.72, 0.73, 0.71, 0.72, 0.72, 0.72, 0.71, 0.71, 0.71, 0.71, 0.72, 0.72, 0.72, 0.73, 0.72, 0.73, 0.72, 0.71, 0.73, 0.71, 0.71, 0.71, 0.71, 0.73, 0.73, 0.71, 0.71, 0.71, 0.72, 0.72, 0.71, 0.71, 0.74, 0.73, 0.72, 0.70, 0.70, 0.71, 0.72, 0.71, 0.73, 0.72, 0.71, 0.72, 0.72, 0.72, 0.73, 0.71, 0.72, 0.71, 0.72, 0.72, 0.72, 0.72, 0.71, 0.72, 0.73, 0.71, 0.73, 0.73, 0.72, 0.72, 0.71, 0.71, 0.71, 0.71, 0.72, 0.73, 0.73, 0.72, 0.71, 0.72, 0.72, 0.73, 0.73, 0.73, 0.72, 0.73, 0.71, 0.73, 0.72, 0.72, 0.72, 0.72, 0.72, 0.71, 0.72, 0.73, 0.71, 0.73, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72


Running iteration 2 of 10 fold...
[59, 0, 48, 24, 44, 18, 73, 38, 99, 16, 52, 33, 91, 64]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.815926, 0.798689, 0.658145, 0.682184

    accuracy, precision, recall, f_score
max3: 0.810370, 0.792356, 0.654210, 0.676992

    accuracy, precision, recall, f_score
max1: 0.811000, 0.788456, 0.636806, 0.657881


min loss: 0.007, episode: 238000
max accu: 0.816, episode: 230000

89.74 classifiers used
    accuracy, precision, recall, f_score
mv: 0.808333, 0.784859, 0.632829, 0.653054
wv: 0.809000, 0.785416, 0.631541, 0.652008
fs: 0.800667, 0.777218, 0.632524, 0.650086
rl: 0.811000, 0.788456, 0.636806, 0.657881

0.7218037037037037
0.72, 0.72, 0.71, 0.71, 0.72, 0.71, 0.72, 0.72, 0.71, 0.71, 0.72, 0.73, 0.71, 0.72, 0.72, 0.71, 0.71, 0.71, 0.72, 0.72, 0.72, 0.72, 0.71, 0.71, 0.73, 0.71, 0.72, 0.72, 0.71, 0.71, 0.71, 0.72, 0.71, 0.72, 0.72, 0.72, 0.71, 0.72, 0.72, 0.72, 0.71, 0.71, 0.72, 0.72, 0.72, 0.71, 0.72, 0.72, 0.73, 0.71, 0.72, 0.72, 0.72, 0.72, 0.72, 0.71, 0.71, 0.72, 0.72, 0.73, 0.72, 0.71, 0.72, 0.71, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.71, 0.72, 0.72, 0.71, 0.72, 0.72, 0.72, 0.70, 0.71, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.71, 0.72, 0.71, 0.71, 0.71, 0.72, 0.71, 0.72, 0.72, 0.71, 0.72, 0.71

0.7195240740740743
0.71, 0.72, 0.70, 0.71, 0.71, 0.70, 0.72, 0.71, 0.71, 0.71, 0.72, 0.72, 0.71, 0.72, 0.72, 0.70, 0.72, 0.70, 0.72, 0.71, 0.71, 0.72, 0.71, 0.71, 0.72, 0.71, 0.71, 0.71, 0.70, 0.71, 0.71, 0.71, 0.72, 0.71, 0.71, 0.71, 0.71, 0.72, 0.72, 0.72, 0.71, 0.71, 0.71, 0.72, 0.73, 0.72, 0.72, 0.71, 0.73, 0.71, 0.72, 0.72, 0.72, 0.72, 0.72, 0.71, 0.70, 0.72, 0.71, 0.73, 0.72, 0.71, 0.71, 0.71, 0.73, 0.71, 0.73, 0.72, 0.71, 0.72, 0.72, 0.72, 0.71, 0.71, 0.72, 0.71, 0.72, 0.72, 0.73, 0.70, 0.71, 0.72, 0.71, 0.72, 0.72, 0.71, 0.72, 0.72, 0.71, 0.72, 0.71, 0.71, 0.71, 0.72, 0.71, 0.71, 0.72, 0.71, 0.72, 0.71

0.72158
0.71, 0.72, 0.71, 0.71, 0.73, 0.71, 0.72, 0.71, 0.71, 0.71, 0.72, 0.71, 0.71, 0.73, 0.72, 0.71, 0.72, 0.70, 0.71, 0.71, 0.72, 0.73, 0.71, 0.71, 0.73, 0.70, 0.71, 0.73, 0.71, 0.72, 0.71, 0.73, 0.71, 0.72, 0.70, 0.72, 0.72, 0.71, 0.72, 0.72, 0.70, 0.71, 0.72, 0.72, 0.72, 0.73, 0.72, 0.72, 0.71, 0.70, 0.72, 0.72, 0.71, 0.73, 0.73, 0.72, 0.71, 0.73, 0.72, 0.73, 0.72, 0.72, 0.72, 0.72, 0.72, 0.71, 0.71, 0.71, 0.73, 0.71, 0.71, 0.71, 0.72, 0.72, 0.71, 0.72, 0.71, 0.72, 0.73, 0.71, 0.72, 0.71, 0.73, 0.72, 0.73, 0.72, 0.72, 0.73, 0.71, 0.72, 0.72, 0.71, 0.71, 0.71, 0.72, 0.72, 0.71, 0.71, 0.73, 0.72

0.7214166666666668
0.72, 0.72, 0.72, 0.71, 0.72, 0.71, 0.72, 0.71, 0.73, 0.71, 0.71, 0.71, 0.72, 0.71, 0.71, 0.72, 0.73, 0.72, 0.71, 0.72, 0.71, 0.72, 0.72, 0.70, 0.71, 0.72, 0.72, 0.71, 0.72, 0.71, 0.70, 0.70, 0.71, 0.72, 0.73, 0.72, 0.71, 0.71, 0.72, 0.71, 0.72, 0.72, 0.72, 0.72, 0.71, 0.71, 0.72, 0.72, 0.71, 0.72, 0.72, 0.71, 0.71, 0.71, 0.72, 0.73, 0.72, 0.72, 0.72, 0.72, 0.72, 0.71, 0.73, 0.70, 0.71, 0.72, 0.73, 0.72, 0.72, 0.71, 0.73, 0.71, 0.72, 0.71, 0.72, 0.71, 0.72, 0.71, 0.72, 0.72, 0.72, 0.72, 0.70, 0.71, 0.72, 0.71, 0.72, 0.71, 0.72, 0.72, 0.72, 0.73, 0.72, 0.71, 0.71, 0.72, 0.70, 0.72, 0.72, 0.71


Running iteration 3 of 10 fold...
[8, 0, 14, 23, 90, 42, 43, 38, 13, 69, 24, 49, 46, 25, 34, 94, 31, 12]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.819074, 0.801808, 0.657621, 0.682318

    accuracy, precision, recall, f_score
max3: 0.817963, 0.801308, 0.662018, 0.686480

    accuracy, precision, recall, f_score
max1: 0.812667, 0.795383, 0.652371, 0.676099


min loss: 0.008, episode: 231000
max accu: 0.819, episode: 220000

71.90 classifiers used
    accuracy, precision, recall, f_score
mv: 0.817333, 0.801617, 0.662049, 0.686971
wv: 0.816000, 0.799886, 0.658626, 0.683283
fs: 0.802000, 0.781403, 0.645454, 0.665600
rl: 0.812667, 0.795383, 0.652371, 0.676099

0.723162962962963
0.71, 0.72, 0.72, 0.72, 0.71, 0.72, 0.72, 0.71, 0.73, 0.72, 0.71, 0.71, 0.71, 0.73, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.73, 0.71, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.71, 0.71, 0.72, 0.72, 0.72, 0.71, 0.72, 0.72, 0.71, 0.71, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.71, 0.71, 0.72, 0.71, 0.72, 0.73, 0.72, 0.72, 0.71, 0.71, 0.73, 0.71, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.73, 0.72, 0.72, 0.72, 0.71, 0.72, 0.72, 0.71, 0.71, 0.72, 0.73, 0.71, 0.71, 0.72, 0.71, 0.72, 0.71, 0.72, 0.72, 0.72, 0.72, 0.73, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.71

0.7228537037037038
0.70, 0.72, 0.72, 0.72, 0.71, 0.72, 0.72, 0.71, 0.73, 0.73, 0.71, 0.70, 0.71, 0.73, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.73, 0.73, 0.71, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.73, 0.71, 0.71, 0.72, 0.72, 0.72, 0.72, 0.73, 0.73, 0.71, 0.71, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.71, 0.72, 0.71, 0.71, 0.72, 0.72, 0.72, 0.72, 0.72, 0.71, 0.71, 0.71, 0.72, 0.70, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.73, 0.72, 0.73, 0.72, 0.71, 0.72, 0.70, 0.71, 0.72, 0.72, 0.72, 0.71, 0.72, 0.71, 0.72, 0.70, 0.72, 0.73, 0.71, 0.72, 0.73, 0.72, 0.72, 0.71, 0.72, 0.71, 0.72, 0.72, 0.72, 0.71

0.7223033333333333
0.71, 0.73, 0.72, 0.71, 0.72, 0.73, 0.71, 0.71, 0.72, 0.73, 0.72, 0.72, 0.72, 0.73, 0.73, 0.72, 0.73, 0.71, 0.71, 0.71, 0.72, 0.72, 0.72, 0.71, 0.71, 0.71, 0.72, 0.72, 0.71, 0.71, 0.71, 0.72, 0.72, 0.71, 0.72, 0.71, 0.72, 0.69, 0.72, 0.71, 0.72, 0.71, 0.71, 0.72, 0.72, 0.71, 0.71, 0.71, 0.72, 0.72, 0.70, 0.71, 0.73, 0.70, 0.72, 0.72, 0.72, 0.72, 0.71, 0.71, 0.71, 0.70, 0.72, 0.71, 0.72, 0.73, 0.72, 0.72, 0.72, 0.73, 0.73, 0.72, 0.71, 0.71, 0.71, 0.72, 0.71, 0.72, 0.71, 0.72, 0.71, 0.71, 0.73, 0.71, 0.72, 0.70, 0.73, 0.73, 0.71, 0.73, 0.73, 0.73, 0.73, 0.72, 0.73, 0.73, 0.74, 0.71, 0.72, 0.72

0.7231966666666666
0.72, 0.73, 0.71, 0.72, 0.72, 0.72, 0.72, 0.72, 0.71, 0.71, 0.72, 0.73, 0.73, 0.71, 0.72, 0.71, 0.71, 0.71, 0.72, 0.73, 0.72, 0.71, 0.73, 0.72, 0.72, 0.71, 0.71, 0.72, 0.73, 0.71, 0.72, 0.71, 0.72, 0.73, 0.71, 0.72, 0.71, 0.72, 0.71, 0.73, 0.72, 0.70, 0.72, 0.73, 0.72, 0.71, 0.72, 0.72, 0.71, 0.72, 0.72, 0.70, 0.73, 0.72, 0.71, 0.71, 0.71, 0.72, 0.71, 0.73, 0.72, 0.71, 0.71, 0.72, 0.73, 0.73, 0.72, 0.73, 0.72, 0.71, 0.74, 0.74, 0.72, 0.71, 0.70, 0.72, 0.71, 0.71, 0.72, 0.72, 0.73, 0.73, 0.70, 0.72, 0.72, 0.71, 0.71, 0.71, 0.72, 0.72, 0.72, 0.72, 0.72, 0.73, 0.72, 0.72, 0.73, 0.72, 0.70, 0.72

    accuracy, precision, recall, f_score
mv: 0.814778, 0.796729, 0.650434, 0.674065
wv: 0.814556, 0.796384, 0.648521, 0.672162
fs: 0.803444, 0.782344, 0.642770, 0.662593
rl: 0.815333, 0.797418, 0.651386, 0.675212

fs avg size: 14.33333, rl avg size: 86.11556
full test avg accu: 0.72289, test avg accu: 0.72298

training takes 72997.939 sec
