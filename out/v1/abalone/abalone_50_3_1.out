{'dataset': 'abalone', 'algorithm': 'ptdqn', 'num_clf': 50, 'num_training': 400000, 'learning_rate': 0.1, 'discount_factor': 1.0, 'epsilon': 0.1, 'random_state': 6156, 'portion': 0.5, 'sequential': False}
(4177, 11)
reading data takes 2.860 sec
number of labels: 28

Running iteration 1 of 10 fold...
[30, 0, 47, 2, 33, 42, 36, 20, 35, 29, 23]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.228313, 0.196254, 0.103037, 0.091934

    accuracy, precision, recall, f_score
max3: 0.228967, 0.207592, 0.130646, 0.115848

    accuracy, precision, recall, f_score
max1: 0.263158, 0.216153, 0.166879, 0.132978


min loss: 0.002, episode: 176000
max accu: 0.263, episode: 260000

42.42 classifiers used
    accuracy, precision, recall, f_score
mv: 0.287081, 0.253906, 0.187185, 0.152177
wv: 0.267943, 0.245626, 0.127229, 0.118873
fs: 0.244019, 0.223148, 0.195229, 0.145046
rl: 0.263158, 0.216153, 0.166879, 0.132978

0.18750399148483232
0.18, 0.18, 0.18, 0.19, 0.19, 0.19, 0.19, 0.19, 0.18, 0.20, 0.20, 0.19, 0.19, 0.19, 0.18, 0.20, 0.19, 0.19, 0.18, 0.17, 0.19, 0.20, 0.19, 0.20, 0.18, 0.18, 0.18, 0.19, 0.19, 0.19, 0.21, 0.17, 0.17, 0.18, 0.16, 0.17, 0.19, 0.17, 0.18, 0.15, 0.16, 0.16, 0.18, 0.17, 0.17, 0.17, 0.18, 0.18, 0.19, 0.16

0.1879446219382322
0.19, 0.19, 0.18, 0.19, 0.18, 0.18, 0.18, 0.18, 0.19, 0.20, 0.21, 0.20, 0.18, 0.19, 0.17, 0.20, 0.17, 0.22, 0.18, 0.16, 0.21, 0.18, 0.20, 0.19, 0.17, 0.18, 0.18, 0.19, 0.20, 0.19, 0.23, 0.18, 0.18, 0.17, 0.16, 0.18, 0.19, 0.17, 0.18, 0.14, 0.16, 0.16, 0.19, 0.17, 0.18, 0.18, 0.18, 0.18, 0.18, 0.16

0.19186602870813396
0.17, 0.16, 0.18, 0.17, 0.18, 0.22, 0.21, 0.18, 0.20, 0.19, 0.19, 0.16, 0.17, 0.17, 0.19, 0.19, 0.22, 0.21, 0.18, 0.19, 0.21, 0.21, 0.20, 0.21, 0.16, 0.21, 0.21, 0.19, 0.20, 0.22, 0.17, 0.15, 0.16, 0.20, 0.19, 0.21, 0.16, 0.20, 0.20, 0.16, 0.18, 0.16, 0.16, 0.17, 0.19, 0.20, 0.20, 0.19, 0.19, 0.18

0.20267942583732057
0.24, 0.23, 0.21, 0.17, 0.21, 0.19, 0.21, 0.21, 0.24, 0.20, 0.18, 0.20, 0.22, 0.22, 0.21, 0.21, 0.22, 0.22, 0.18, 0.17, 0.18, 0.19, 0.19, 0.21, 0.20, 0.17, 0.22, 0.19, 0.20, 0.25, 0.18, 0.19, 0.19, 0.21, 0.18, 0.19, 0.17, 0.19, 0.19, 0.22, 0.20, 0.18, 0.18, 0.16, 0.19, 0.21, 0.17, 0.19, 0.16, 0.20


Running iteration 2 of 10 fold...
[15, 0, 17, 40, 31, 32, 43]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.229377, 0.201507, 0.088799, 0.084951

    accuracy, precision, recall, f_score
max3: 0.234292, 0.209140, 0.116004, 0.109974

    accuracy, precision, recall, f_score
max1: 0.234450, 0.198161, 0.120568, 0.106414


min loss: 0.001, episode: 233000
max accu: 0.234, episode: 260000

46.39 classifiers used
    accuracy, precision, recall, f_score
mv: 0.224880, 0.203658, 0.136597, 0.137257
wv: 0.227273, 0.208632, 0.147172, 0.149029
fs: 0.212919, 0.197509, 0.107872, 0.099559
rl: 0.234450, 0.198161, 0.120568, 0.106414

0.1880255455029271
0.19, 0.21, 0.19, 0.18, 0.19, 0.20, 0.19, 0.18, 0.21, 0.20, 0.20, 0.19, 0.20, 0.19, 0.20, 0.21, 0.19, 0.20, 0.17, 0.18, 0.19, 0.18, 0.18, 0.18, 0.19, 0.19, 0.17, 0.19, 0.19, 0.17, 0.18, 0.19, 0.18, 0.17, 0.17, 0.16, 0.18, 0.17, 0.17, 0.18, 0.17, 0.18, 0.18, 0.18, 0.17, 0.18, 0.18, 0.17, 0.18, 0.16

0.18792332268370604
0.20, 0.21, 0.20, 0.19, 0.19, 0.19, 0.18, 0.19, 0.21, 0.21, 0.21, 0.19, 0.19, 0.20, 0.20, 0.21, 0.19, 0.20, 0.17, 0.19, 0.17, 0.19, 0.17, 0.18, 0.19, 0.19, 0.18, 0.19, 0.19, 0.17, 0.17, 0.18, 0.18, 0.17, 0.16, 0.15, 0.19, 0.16, 0.17, 0.17, 0.16, 0.17, 0.18, 0.18, 0.18, 0.18, 0.17, 0.18, 0.16, 0.16

0.18468899521531099
0.19, 0.18, 0.20, 0.19, 0.17, 0.18, 0.20, 0.18, 0.17, 0.16, 0.18, 0.16, 0.20, 0.17, 0.17, 0.15, 0.16, 0.21, 0.19, 0.18, 0.21, 0.19, 0.18, 0.15, 0.16, 0.16, 0.21, 0.18, 0.20, 0.18, 0.19, 0.19, 0.22, 0.19, 0.19, 0.19, 0.16, 0.18, 0.17, 0.20, 0.18, 0.17, 0.16, 0.16, 0.19, 0.16, 0.17, 0.19, 0.16, 0.17

0.18028708133971294
0.18, 0.19, 0.17, 0.17, 0.19, 0.20, 0.20, 0.17, 0.19, 0.20, 0.18, 0.17, 0.17, 0.17, 0.20, 0.18, 0.18, 0.17, 0.18, 0.19, 0.20, 0.16, 0.21, 0.16, 0.18, 0.18, 0.18, 0.16, 0.17, 0.19, 0.17, 0.19, 0.19, 0.19, 0.14, 0.17, 0.17, 0.18, 0.14, 0.16, 0.15, 0.18, 0.16, 0.15, 0.17, 0.17, 0.15, 0.18, 0.15, 0.15


Running iteration 3 of 10 fold...
[14, 0, 29, 26, 31, 28, 38, 33, 32, 5, 24]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.222459, 0.207788, 0.095719, 0.090417

    accuracy, precision, recall, f_score
max3: 0.231097, 0.202107, 0.120658, 0.111894

    accuracy, precision, recall, f_score
max1: 0.248804, 0.208432, 0.153283, 0.143251


min loss: 0.002, episode: 188000
max accu: 0.249, episode: 260000

27.22 classifiers used
    accuracy, precision, recall, f_score
mv: 0.248804, 0.236410, 0.162065, 0.165269
wv: 0.239234, 0.223411, 0.149484, 0.157216
fs: 0.229665, 0.201550, 0.146988, 0.141323
rl: 0.248804, 0.208432, 0.153283, 0.143251

0.18777009047365617
0.20, 0.19, 0.19, 0.20, 0.20, 0.19, 0.20, 0.20, 0.19, 0.18, 0.18, 0.20, 0.19, 0.19, 0.20, 0.19, 0.18, 0.18, 0.19, 0.17, 0.19, 0.17, 0.19, 0.17, 0.17, 0.18, 0.20, 0.19, 0.19, 0.17, 0.18, 0.18, 0.19, 0.19, 0.19, 0.17, 0.20, 0.16, 0.18, 0.17, 0.18, 0.17, 0.18, 0.18, 0.18, 0.16, 0.17, 0.17, 0.18, 0.17

0.19082002129925452
0.20, 0.19, 0.19, 0.20, 0.20, 0.20, 0.19, 0.20, 0.19, 0.18, 0.18, 0.21, 0.19, 0.19, 0.19, 0.20, 0.19, 0.19, 0.20, 0.18, 0.20, 0.17, 0.18, 0.17, 0.16, 0.18, 0.20, 0.19, 0.19, 0.17, 0.18, 0.18, 0.19, 0.21, 0.20, 0.18, 0.19, 0.16, 0.17, 0.17, 0.17, 0.17, 0.17, 0.19, 0.19, 0.16, 0.17, 0.18, 0.19, 0.17

0.19334928229665074
0.20, 0.19, 0.19, 0.21, 0.21, 0.19, 0.20, 0.20, 0.20, 0.17, 0.20, 0.21, 0.20, 0.22, 0.20, 0.20, 0.19, 0.19, 0.18, 0.19, 0.18, 0.18, 0.21, 0.17, 0.19, 0.21, 0.15, 0.18, 0.17, 0.17, 0.17, 0.18, 0.21, 0.19, 0.19, 0.18, 0.18, 0.18, 0.20, 0.17, 0.15, 0.19, 0.20, 0.19, 0.20, 0.18, 0.17, 0.16, 0.19, 0.18

0.19229665071770335
0.21, 0.20, 0.20, 0.20, 0.22, 0.21, 0.22, 0.21, 0.18, 0.20, 0.21, 0.19, 0.20, 0.23, 0.20, 0.20, 0.20, 0.18, 0.19, 0.18, 0.16, 0.15, 0.18, 0.20, 0.17, 0.18, 0.18, 0.19, 0.18, 0.17, 0.15, 0.16, 0.19, 0.20, 0.18, 0.17, 0.22, 0.18, 0.18, 0.19, 0.18, 0.20, 0.17, 0.15, 0.16, 0.17, 0.21, 0.16, 0.18, 0.17

    accuracy, precision, recall, f_score
mv: 0.253589, 0.231325, 0.161949, 0.151568
wv: 0.244817, 0.225889, 0.141295, 0.141706
fs: 0.228868, 0.207402, 0.150030, 0.128643
rl: 0.248804, 0.207582, 0.146910, 0.127548

fs avg size: 9.66667, rl avg size: 38.67305
full test avg accu: 0.19175, test avg accu: 0.18997

training takes 23044.589 sec
