{'dataset': 'abalone', 'algorithm': 'ptdqn', 'num_clf': 100, 'num_training': 400000, 'learning_rate': 0.1, 'discount_factor': 1.0, 'epsilon': 0.1, 'random_state': 510, 'portion': 0.5, 'sequential': False}
(4177, 11)
reading data takes 1.645 sec
number of labels: 28

Running iteration 1 of 10 fold...
[81, 0, 22, 18, 37, 28, 16, 21, 2]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.2672, 0.2476, 0.1405, 0.1379

    accuracy, precision, recall, f_score
max3: 0.2822, 0.2680, 0.1594, 0.1493

    accuracy, precision, recall, f_score
max1: 0.3038, 0.2587, 0.1743, 0.1571


min loss: 0.003, episode: 137000
max accu: 0.304, episode: 380000

22.99 classifiers used
    accuracy, precision, recall, f_score
mv: 0.2751, 0.2303, 0.1367, 0.1172
wv: 0.2751, 0.2616, 0.1295, 0.1125
fs: 0.2656, 0.2192, 0.1462, 0.1239
rl: 0.3038, 0.2587, 0.1743, 0.1571

0.20516746411483255
0.16, 0.28, 0.21, 0.13, 0.19, 0.26, 0.20, 0.13, 0.16, 0.29, 0.20, 0.11, 0.19, 0.27, 0.21, 0.14, 0.21, 0.28, 0.19, 0.10, 0.23, 0.28, 0.22, 0.13, 0.20, 0.29, 0.24, 0.15, 0.22, 0.28, 0.23, 0.13, 0.19, 0.29, 0.20, 0.11, 0.17, 0.27, 0.21, 0.13, 0.17, 0.30, 0.19, 0.13, 0.19, 0.30, 0.22, 0.11, 0.18, 0.28, 0.19, 0.13, 0.23, 0.27, 0.20, 0.10, 0.18, 0.28, 0.19, 0.11, 0.20, 0.28, 0.19, 0.12, 0.18, 0.29, 0.18, 0.10, 0.22, 0.27, 0.19, 0.14, 0.19, 0.28, 0.20, 0.10, 0.20, 0.27, 0.17, 0.11, 0.20, 0.29, 0.22, 0.12, 0.17, 0.27, 0.23, 0.14, 0.21, 0.29, 0.22, 0.14, 0.17, 0.29, 0.19, 0.13, 0.17, 0.28, 0.23, 0.13

0.20550239234449758
0.20, 0.28, 0.21, 0.10, 0.18, 0.28, 0.18, 0.12, 0.20, 0.28, 0.21, 0.14, 0.19, 0.27, 0.20, 0.13, 0.20, 0.27, 0.21, 0.08, 0.20, 0.27, 0.21, 0.13, 0.19, 0.27, 0.22, 0.16, 0.22, 0.27, 0.24, 0.12, 0.20, 0.27, 0.20, 0.13, 0.18, 0.28, 0.23, 0.13, 0.19, 0.26, 0.22, 0.09, 0.19, 0.29, 0.22, 0.13, 0.21, 0.29, 0.19, 0.12, 0.20, 0.29, 0.19, 0.13, 0.19, 0.27, 0.21, 0.12, 0.21, 0.30, 0.22, 0.13, 0.21, 0.28, 0.19, 0.14, 0.20, 0.28, 0.23, 0.12, 0.19, 0.27, 0.23, 0.10, 0.18, 0.28, 0.21, 0.18, 0.21, 0.28, 0.18, 0.12, 0.17, 0.25, 0.20, 0.13, 0.20, 0.29, 0.21, 0.13, 0.19, 0.26, 0.19, 0.09, 0.20, 0.26, 0.22, 0.12


Running iteration 2 of 10 fold...
[65, 0, 34, 73, 48, 96]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.2698, 0.2359, 0.1326, 0.1200

    accuracy, precision, recall, f_score
max3: 0.2897, 0.2575, 0.1600, 0.1378

    accuracy, precision, recall, f_score
max1: 0.2823, 0.2452, 0.1248, 0.1154


min loss: 0.002, episode: 231000
max accu: 0.282, episode: 140000

7.11 classifiers used
    accuracy, precision, recall, f_score
mv: 0.2608, 0.2281, 0.1449, 0.1322
wv: 0.2608, 0.2402, 0.1494, 0.1375
fs: 0.2703, 0.2506, 0.1358, 0.1307
rl: 0.2823, 0.2452, 0.1248, 0.1154

0.19674641148325356
0.19, 0.26, 0.21, 0.10, 0.17, 0.26, 0.18, 0.11, 0.23, 0.27, 0.23, 0.10, 0.15, 0.26, 0.22, 0.09, 0.16, 0.24, 0.23, 0.09, 0.16, 0.26, 0.21, 0.11, 0.17, 0.27, 0.19, 0.14, 0.18, 0.29, 0.22, 0.10, 0.18, 0.28, 0.19, 0.09, 0.19, 0.28, 0.21, 0.13, 0.16, 0.26, 0.19, 0.14, 0.17, 0.27, 0.20, 0.09, 0.16, 0.28, 0.21, 0.11, 0.20, 0.26, 0.22, 0.10, 0.19, 0.27, 0.24, 0.11, 0.21, 0.28, 0.22, 0.10, 0.19, 0.29, 0.21, 0.11, 0.17, 0.26, 0.21, 0.13, 0.18, 0.28, 0.19, 0.14, 0.18, 0.27, 0.20, 0.11, 0.17, 0.28, 0.20, 0.11, 0.17, 0.28, 0.22, 0.11, 0.18, 0.26, 0.21, 0.10, 0.19, 0.28, 0.22, 0.12, 0.17, 0.25, 0.21, 0.11

0.19880382775119615
0.16, 0.28, 0.22, 0.12, 0.18, 0.27, 0.21, 0.14, 0.20, 0.27, 0.21, 0.12, 0.18, 0.28, 0.22, 0.14, 0.19, 0.25, 0.21, 0.13, 0.20, 0.27, 0.25, 0.11, 0.16, 0.26, 0.21, 0.12, 0.21, 0.26, 0.23, 0.09, 0.21, 0.26, 0.22, 0.11, 0.16, 0.29, 0.23, 0.13, 0.16, 0.27, 0.22, 0.15, 0.18, 0.25, 0.20, 0.12, 0.17, 0.26, 0.26, 0.09, 0.20, 0.26, 0.22, 0.11, 0.15, 0.27, 0.22, 0.11, 0.18, 0.27, 0.21, 0.08, 0.17, 0.26, 0.20, 0.09, 0.17, 0.27, 0.22, 0.12, 0.17, 0.25, 0.21, 0.10, 0.17, 0.25, 0.17, 0.10, 0.18, 0.25, 0.19, 0.10, 0.17, 0.28, 0.22, 0.09, 0.19, 0.26, 0.26, 0.13, 0.19, 0.28, 0.22, 0.11, 0.18, 0.30, 0.20, 0.10


Running iteration 3 of 10 fold...
[17, 0, 45, 44]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.2571, 0.2282, 0.1382, 0.1217

    accuracy, precision, recall, f_score
max3: 0.2535, 0.2333, 0.1514, 0.1262

    accuracy, precision, recall, f_score
max1: 0.2703, 0.2233, 0.1304, 0.1132


min loss: 0.004, episode: 116000
max accu: 0.270, episode: 370000

70.50 classifiers used
    accuracy, precision, recall, f_score
mv: 0.2703, 0.2238, 0.1283, 0.1138
wv: 0.2560, 0.2205, 0.1238, 0.1123
fs: 0.2488, 0.2071, 0.1220, 0.1113
rl: 0.2703, 0.2233, 0.1304, 0.1132

0.1875358851674641
0.18, 0.24, 0.20, 0.10, 0.19, 0.22, 0.23, 0.09, 0.17, 0.23, 0.20, 0.11, 0.16, 0.25, 0.21, 0.10, 0.17, 0.24, 0.23, 0.11, 0.17, 0.24, 0.22, 0.10, 0.17, 0.21, 0.21, 0.12, 0.15, 0.23, 0.24, 0.10, 0.18, 0.23, 0.21, 0.11, 0.16, 0.22, 0.20, 0.11, 0.18, 0.23, 0.24, 0.10, 0.16, 0.24, 0.22, 0.10, 0.18, 0.27, 0.20, 0.11, 0.19, 0.24, 0.24, 0.12, 0.18, 0.23, 0.20, 0.11, 0.18, 0.24, 0.21, 0.10, 0.19, 0.25, 0.23, 0.10, 0.19, 0.23, 0.20, 0.11, 0.17, 0.24, 0.22, 0.10, 0.19, 0.25, 0.22, 0.11, 0.19, 0.23, 0.20, 0.11, 0.18, 0.22, 0.19, 0.11, 0.18, 0.24, 0.21, 0.11, 0.17, 0.23, 0.20, 0.11, 0.16, 0.25, 0.20, 0.11

0.18954545454545454
0.19, 0.24, 0.21, 0.11, 0.18, 0.25, 0.21, 0.10, 0.19, 0.24, 0.22, 0.11, 0.18, 0.23, 0.21, 0.10, 0.18, 0.23, 0.21, 0.11, 0.17, 0.25, 0.19, 0.10, 0.18, 0.23, 0.20, 0.09, 0.19, 0.24, 0.22, 0.12, 0.20, 0.24, 0.23, 0.10, 0.18, 0.26, 0.22, 0.09, 0.23, 0.24, 0.22, 0.09, 0.19, 0.22, 0.23, 0.11, 0.17, 0.23, 0.21, 0.09, 0.21, 0.21, 0.21, 0.11, 0.19, 0.24, 0.20, 0.09, 0.18, 0.23, 0.26, 0.08, 0.17, 0.25, 0.20, 0.11, 0.17, 0.24, 0.23, 0.09, 0.16, 0.24, 0.20, 0.10, 0.19, 0.23, 0.22, 0.11, 0.19, 0.24, 0.24, 0.11, 0.19, 0.24, 0.23, 0.10, 0.18, 0.23, 0.22, 0.07, 0.16, 0.23, 0.22, 0.10, 0.18, 0.24, 0.22, 0.11


Running iteration 4 of 10 fold...
[65, 0, 30, 33, 91]
using cpu
using cpu
    accuracy, precision, recall, f_score
max0: 0.2629, 0.2261, 0.1257, 0.1080

    accuracy, precision, recall, f_score
max3: 0.2652, 0.2270, 0.1477, 0.1222

    accuracy, precision, recall, f_score
max1: 0.2871, 0.2489, 0.1809, 0.1541


min loss: 0.003, episode: 218000
max accu: 0.287, episode: 190000

31.93 classifiers used
    accuracy, precision, recall, f_score
mv: 0.2536, 0.2327, 0.1565, 0.1300
wv: 0.2416, 0.2163, 0.1616, 0.1285
fs: 0.2656, 0.2348, 0.1618, 0.1412
rl: 0.2871, 0.2489, 0.1809, 0.1541

0.19186602870813396
0.21, 0.23, 0.22, 0.11, 0.19, 0.24, 0.23, 0.11, 0.16, 0.23, 0.23, 0.11, 0.20, 0.24, 0.21, 0.11, 0.18, 0.24, 0.21, 0.09, 0.17, 0.24, 0.20, 0.10, 0.18, 0.23, 0.25, 0.11, 0.20, 0.25, 0.22, 0.11, 0.17, 0.28, 0.21, 0.12, 0.15, 0.22, 0.22, 0.09, 0.16, 0.24, 0.22, 0.09, 0.18, 0.24, 0.20, 0.09, 0.19, 0.27, 0.26, 0.09, 0.16, 0.23, 0.22, 0.07, 0.16, 0.23, 0.23, 0.10, 0.22, 0.25, 0.22, 0.09, 0.22, 0.25, 0.26, 0.10, 0.19, 0.23, 0.20, 0.12, 0.19, 0.23, 0.23, 0.10, 0.21, 0.23, 0.21, 0.09, 0.16, 0.26, 0.24, 0.09, 0.18, 0.23, 0.22, 0.11, 0.21, 0.24, 0.26, 0.09, 0.18, 0.24, 0.23, 0.08, 0.18, 0.25, 0.23, 0.09

0.1905023923444976
0.17, 0.24, 0.22, 0.12, 0.17, 0.23, 0.19, 0.10, 0.21, 0.25, 0.23, 0.09, 0.20, 0.25, 0.23, 0.05, 0.19, 0.23, 0.22, 0.09, 0.19, 0.25, 0.21, 0.08, 0.13, 0.24, 0.22, 0.11, 0.18, 0.25, 0.24, 0.12, 0.16, 0.25, 0.22, 0.10, 0.16, 0.25, 0.23, 0.11, 0.19, 0.23, 0.22, 0.09, 0.15, 0.24, 0.20, 0.10, 0.20, 0.24, 0.21, 0.13, 0.15, 0.24, 0.24, 0.09, 0.20, 0.27, 0.23, 0.09, 0.19, 0.26, 0.21, 0.08, 0.16, 0.23, 0.27, 0.10, 0.20, 0.25, 0.20, 0.09, 0.17, 0.23, 0.23, 0.10, 0.18, 0.23, 0.25, 0.11, 0.19, 0.23, 0.22, 0.11, 0.18, 0.26, 0.24, 0.09, 0.19, 0.26, 0.22, 0.10, 0.18, 0.27, 0.22, 0.09, 0.17, 0.24, 0.20, 0.09

    accuracy, precision, recall, f_score
mv: 0.2650, 0.2287, 0.1416, 0.1233
wv: 0.2584, 0.2347, 0.1411, 0.1227
fs: 0.2626, 0.2279, 0.1415, 0.1268
rl: 0.2859, 0.2440, 0.1526, 0.1350

fs avg size: 6.00000, rl avg size: 33.13278
full test avg accu: 0.19609, test avg accu: 0.19533

training takes 45803.038 sec
